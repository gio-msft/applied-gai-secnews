üõ°Ô∏è **From Prompt Injections to Protocol Exploits: Threats in LLM-Powered AI
  Agents Workflows** [source](http://arxiv.org/pdf/2506.23260v1.pdf) #security 

 Sophisticated input, model, and protocol-based attacks can easily breach LLM-powered agent systems due to high vulnerability rates and fragmented defenses, demonstrating a pressing need for holistic, multi-layered security frameworks.
 - Multiple attack techniques, including adaptive prompt injections and protocol-level exploits, achieve exceptionally high success rates‚Äîoften over 90%‚Äîagainst LLM-powered AI agent workflows, indicating current defenses are inadequate at both the input and protocol layers.
 - Backdoor and data poisoning attacks, such as composite backdoor and retrieval poisoning, can stealthily compromise agents and knowledge bases with near-perfect activation rates (up to 100%) while maintaining standard operational performance, underscoring the difficulty of detecting silent, persistent threats.
 - Protocol and system-level vulnerabilities‚Äîranging from Model Context Protocol (MCP) and Agent-to-Agent (A2A) exploits to side-channel information leaks and federated learning attacks‚Äîexpand the attack surface of AI agent ecosystems, highlighting an urgent need for robust, end-to-end security measures and ongoing research into cryptographic provenance, trust management, and scalable anomaly detection.

<br>

üåç **Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt
  and Alignment-Based Approach** [source](http://arxiv.org/pdf/2507.00601v2.pdf) #general 

 Efficient prompt and alignment strategies greatly enhance large language model performance, stability, and practicality in low-resource languages and tasks without sacrificing general capabilities.
 - A unified approach combining knowledge alignment losses and soft prompt tuning achieves over 10 percentage point improvement in F1 and Exact Match scores on low-resource cross-lingual tasks like MLQA, XQuAD, and PAWS-X compared to baseline models.
 - The proposed adaptation mechanism for large language models demonstrates the highest stability score (0.89) in low-resource settings, effectively reducing parameter fluctuations and training instability relative to mainstream models such as LLaMA-2 and BLOOMZ.
 - Introducing moderate levels of synthetic pseudo-data augmentation significantly boosts performance on low-resource tasks‚Äîwith F1 rising from 73.2 to 80.1 and accuracy from 83.0 to 87.2‚Äîwhile excessive augmentation can lead to diminishing returns due to distributional drift.

<br>

üõ°Ô∏è **A Different Approach to AI Safety: Proceedings from the Columbia
  Convening on Openness in Artificial Intelligence and AI Safety** [source](http://arxiv.org/pdf/2506.22183v1.pdf) #security 

 Openness in AI development can increase safety through transparency and pluralistic oversight, but urgent infrastructure, benchmarking, and participatory gaps remain that must be addressed with community-driven, context-aware solutions.
 - Adoption of open-source and open-weight AI models is rapidly accelerating, with a reported 880% growth in generative AI repositories over two years and 46% of Fortune 500 leaders expressing strong preference for open models, highlighting openness as a mainstream industry direction.
 - Significant gaps persist in current AI safety practices, including a lack of robust multimodal and multilingual benchmarks, limited defenses against prompt-injection and compositional attacks in agentic systems, and insufficient participatory mechanisms for communities most affected by AI harms.
 - Key actionable priorities identified include expanding participatory approaches to AI safety, developing future-proof and adaptable content safety filters, establishing system-wide safety infrastructure, enhancing safeguards for agentic AI, and broadening harm taxonomies to capture overlooked risks.

<br>

üßë‚Äçüíª **Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among
  Large Language Models** [source](http://arxiv.org/pdf/2506.22957v1.pdf) #security 

 This paper uncovers both the collaborative promise and the nuanced safety risks of LLMs' ability to recognize and adapt to their interlocutors' identities in multi-agent environments.
 - Large language models (LLMs) show high accuracy (up to 90% or more) in identifying the family of other LLMs, especially for their own model family, with significantly lower but nontrivial accuracy for prominent or widely-used families such as GPT.
 - Revealing the identity of an interacting model in multi-agent settings consistently improves cooperative task performance (by up to 10% for weaker models), but also enables models to strategically adapt outputs for specific evaluators, thus increasing risks of reward hacking and biased evaluations.
 - LLMs that are strong at preference alignment with known judges also tend to be more susceptible to identity-aware jailbreak attempts, indicating a correlation between strategic adaptation and increased vulnerability to adversarial manipulation.

<br>

üõ°Ô∏è **Advancing Jailbreak Strategies: A Hybrid Approach to Exploiting LLM
  Vulnerabilities and Bypassing Modern Defenses** [source](http://arxiv.org/pdf/2506.21972v1.pdf) #security 

 Hybrid adversarial attacks that fuse token- and prompt-level techniques can reliably bypass leading LLM defenses and expose critical safety gaps, especially on fine-tuned models.
 - Hybrid jailbreak attacks that combine token-level and prompt-level strategies achieved a 91.6% attack success rate on Llama-3 models, significantly surpassing single-method attacks which peaked at 58.4%.
 - State-of-the-art defenses like JBShield and Gradient Cuff‚Äîeffective against standalone attacks‚Äîwere bypassed by hybrid approaches on Vicuna-7B, raising attack success rates from near 0% up to 58%, highlighting substantial vulnerabilities in current safety stacks.
 - Transferability and efficiency of these hybrid methods were demonstrated as they maintained over 80% success rate even when evaluated under strict adversarial judges, revealing that model fine-tuning can introduce new jailbreak vulnerabilities.

<br>

ü¶æ **VERA: Variational Inference Framework for Jailbreaking Large Language
  Models** [source](http://arxiv.org/pdf/2506.22666v1.pdf) #security 

 A variational inference approach enables efficient, diverse, and highly transferable black-box jailbreak attacks on LLMs that surpass existing methods and uncover substantial vulnerabilities even in the presence of advanced defenses.
 - The VERA framework achieves state-of-the-art attack success rates on large language models in black-box settings, with ASR reaching up to 70% on Vicuna-7B, 64.8% on Baichuan-2-7B, and 72% on Orca-2-7B, outperforming both black-box and white-box baselines on most models.
 - VERA-generated adversarial prompts are significantly more diverse and independent of initial templates compared to prior methods, with BLEU scores indicating much lower similarity both to themselves and to original templates, ensuring broader vulnerability coverage and future-proofing against patched prompts.
 - Prompts crafted by VERA show high transferability and robustness, maintaining over 58% attack success against perplexity-based defenses and outperforming competitors under circuit breaker defenses, highlighting the resilience and generalizability of its adversarial techniques across models and alignment methods.

<br>

‚úÇÔ∏è **SafePTR: Token-Level Jailbreak Defense in Multimodal LLMs via
  Prune-then-Restore Mechanism** [source](http://arxiv.org/pdf/2507.01513v1.pdf) #security 

 Mitigating multimodal jailbreaks requires targeting a tiny fraction of critical tokens‚ÄîSafePTR surgically removes these at vulnerable layers, delivering state-of-the-art safety with no utility loss or extra compute.
 - Pruning less than 1% of semantically misaligned multimodal tokens in early-middle layers reduces multimodal jailbreak attack success rates by over 95% across leading vision-language models, reaching near-zero levels in prohibited content categories.
 - SafePTR, a training-free defense framework, outperforms prior methods by maintaining high model utility on benign multimodal tasks (e.g., 1538.1 on MME benchmark) while eliminating computational overhead and training requirements.
 - Unlike safety prompts or retraining-based defenses‚Äîwhich can be over-defensive and inefficient‚Äîselective token pruning followed by targeted restoration enables robust, interpretable, and lightweight mitigation of both text- and vision-driven jailbreaks.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **MetaCipher: A General and Extensible Reinforcement Learning Framework
  for Obfuscation-Based Jailbreak Attacks on Black-Box LLMs** [source](http://arxiv.org/pdf/2506.22557v1.pdf) #security 

 MetaCipher reveals that adaptive cipher obfuscation can robustly and efficiently defeat LLM safety filters, extending to both text and image models.
 - An adaptive reinforcement learning framework using a diverse pool of 21 ciphers enables automated obfuscation-based jailbreaks, achieving over 92% attack success rate (ASR) on non-reasoning LLMs and over 74% ASR on leading reasoning-capable LLMs within 10 queries.
 - By encrypting all malicious keywords in a prompt and dynamically selecting ciphers, the system exploits model reasoning skills while circumventing conventional safety guardrails that rely on keyword detection or context filtering.
 - The framework‚Äôs extensibility to multimodal tasks is demonstrated by successfully bypassing text-to-image content filters, achieving success rates above 50% for violence, gore, and illegal activity prompts on real-world image generation services.

<br>

üõ°Ô∏è **Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large
  Language Models** [source](http://arxiv.org/pdf/2506.23576v1.pdf) #security 

 While multi-agent setups strengthen LLM defences against jailbreaks, they introduce trade-offs in misclassification rates and are highly sensitive to attack strategies and evaluation ambiguities.
 - Multi-agent defence systems reduce the success rate of jailbreaking attacks on large language models‚Äîcutting attack success from 55.74% to as low as 7.95%‚Äîbut the optimal number of agents depends heavily on the type of attack.
 - Increasing the number of agents generally decreases false negatives (missed unsafe responses) but correspondingly increases false positives (unethical content misclassified as safe), which can undermine overall security.
 - Automated evaluators and multi-agent architectures struggle most with layered prompts containing ethically ambiguous or contextually mixed content, often leading to misclassification, especially in edge cases.

<br>

üåê **Leveraging the Potential of Prompt Engineering for Hate Speech Detection
  in Low-Resource Languages** [source](http://arxiv.org/pdf/2506.23930v1.pdf) #general 

 Metaphor prompting enables LLMs to detect hate speech in low-resource languages with record accuracy and minimized environmental impact by bypassing model safety constraints.
 - Metaphor prompting, which replaces sensitive terms like 'hate' with neutral metaphors, boosted Llama2-7B's hate speech detection F1 score in Bengali to 95.89%, exceeding conventional methods and surpassing state-of-the-art baselines.
 - Role-relevant prompts (e.g., 'cyber crusader', 'misogynist') improved LLM-driven hate speech detection in contextually rich datasets up to 92.38% F1, highlighting the impact of context-aligned prompt strategies.
 - Metaphor prompting not only improved classification accuracy across low- and high-resource languages (Bengali, Hindi, English, German) but also consistently reduced computational time and environmental impact (lower CO2, electricity), supporting more sustainable NLP deployment.

<br>

üß† **Reasoning as an Adaptive Defense for Safety** [source](http://arxiv.org/pdf/2507.00971v1.pdf) #security 

 Adaptive reasoning during response generation makes language models much harder to jailbreak while preserving their ability to help with harmless tasks.
 - Models trained with the TARS (Training Adaptive Reasoners for Safety) approach achieved a superior safety‚Äìrefusal trade-off compared to both non-reasoning and other reasoning-based models, balancing the ability to refuse harmful queries while remaining helpful on harmless prompts.
 - TARS-trained models demonstrate adaptive computational behavior, allocating significantly more reasoning tokens‚Äîup to twice as many‚Äîto ambiguous or complex queries, thereby improving their robustness to both white-box and black-box safety attacks.
 - Internal representation analysis reveals that TARS-trained models more effectively separate harmful from harmless prompts in embedding space, leading to a 24% relative increase in defense success rate under adversarial attacks compared to standard RL or supervised approaches.

<br>

ü¶∏‚Äç‚ôÇÔ∏è **Decoding Memes: Benchmarking Narrative Role Classification across
  Multilingual and Multimodal Models** [source](http://arxiv.org/pdf/2506.23122v1.pdf) #general 

 Role detection in memes remains highly challenging; even top-tier multimodal models struggle with cultural nuance and subtle victim portrayals, but prompt engineering offers consistent, if modest, improvements.
 - Multimodal models like Qwen2.5-VL-7B-Instruct achieved the best macro-average F1 scores (up to 0.35) for narrative role classification in memes, outperforming text-only and smaller code-mixed baselines, but still demonstrated inconsistent generalization across languages and cultural settings.
 - Despite advancements, all evaluated models exhibited significant difficulty reliably identifying the 'Victim' role in memes, with F1 scores for this class consistently lagging behind 'Hero' and 'Villain', especially in code-mixed and culturally diverse content.
 - Hybrid prompt engineering strategies‚Äîcombining role definitions and structural instructions‚Äîincreased macro F1 scores by up to 11 percentage points over naive prompting for multimodal models, particularly enhancing prediction in the 'Hero' and 'Villain' categories.

<br>

ü¶æ **Smaller = Weaker? Benchmarking Robustness of Quantized LLMs in Code
  Generation** [source](http://arxiv.org/pdf/2506.22776v1.pdf) #general 

 Contrary to traditional assumptions, compressing LLMs through quantization enhances their robustness to adversarial and noisy conditions, especially in code generation tasks.
 - Quantized large language models (LLMs) exhibited greater robustness to adversarial attacks than their full-precision counterparts in 51.59% of experiments, with especially pronounced gains in smaller models (1‚Äì3B parameters).
 - Noise perturbation experiments revealed that quantized LLMs consistently withstood higher levels of both Gaussian and Uniform noise, particularly at moderate to high intensities (‚â•3e-3), compared to original models.
 - 8-bit quantization provided an optimal trade-off, resulting in only a 0.36% average reduction in clean performance while delivering substantial improvements in resilience to both input- and architecture-level perturbations.

<br>

üõ°Ô∏è **Tuning without Peeking: Provable Privacy and Generalization Bounds for
  LLM Post-Training** [source](http://arxiv.org/pdf/2507.01752v1.pdf) #security 

 Secure comparison-based black-box post-training (BBoxER) enables privacy-preserving and robust LLM adaptation with formal generalization guarantees and demonstrated real-world improvements.
 - Post-training large language models using comparison-based black-box optimization (BBoxER) achieves provable generalization bounds and guarantees perfect differential privacy (Œµ=0, Œ¥=0), regardless of model size.
 - BBoxER demonstrates robustness against data poisoning and extraction attacks by compressing user feedback signals to a minimal information bottleneck, leading to practical immunity even when adversarial feedback is present in less than ‚àön/b of user inputs per iteration.
 - Empirical evaluations on contemporary LLMs (Llama3.1-8B, Qwen2.5-3B) show statistically significant accuracy improvements on math and reasoning benchmarks (e.g., +5.6% GSM8K-test, +4.5% OOD transfer) without overfitting, even under tight computational and data constraints.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **QLPro: Automated Code Vulnerability Discovery via LLM and Static Code
  Analysis Integration** [source](http://arxiv.org/pdf/2506.23644v1.pdf) #security 

 Automated integration of large language models with static code analysis tools dramatically improves both accuracy and coverage in vulnerability detection for complex, real-world software projects.
 - QLPro achieved a 90.90% CodeQL syntactic correctness rate in vulnerability scanning rule generation when using a collaborative approach with two Claude-3.7-thinking models, significantly surpassing previous methods.
 - Automatic rules generated by QLPro detected 41 out of 62 known vulnerabilities (66.1%) in large-scale open-source Java projects, compared to only 24 vulnerabilities (38.7%) identified by the official CodeQL rule repository.
 - QLPro autonomously discovered 6 previously unknown (0-day) vulnerabilities, 2 of which have been confirmed by project maintainers, demonstrating its capacity to uncover critical security issues missed by established tools.

<br>

üõ°Ô∏è **Guiding AI to Fix Its Own Flaws: An Empirical Study on LLM-Driven Secure
  Code Generation** [source](http://arxiv.org/pdf/2506.23034v1.pdf) #security 

 Actionable, high-quality security hints and fine-grained feedback can guide LLMs to generate safer code or repair flaws, but both model capability and hint quality are critical for success.
 - Large language models (LLMs) generate vulnerable code at rates ranging from 9.8% to 42.1%, with similar vulnerability patterns present in both proprietary and open-weight models.
 - Incorporating contextualized, self-generated vulnerability hints into prompts reduces vulnerable code output in advanced LLMs, but irrelevant or imprecise hints can increase vulnerabilities.
 - Detailed, explained feedback for post-hoc code repair (rather than raw detection outputs) enables more powerful LLMs to fix vulnerabilities more effectively, while less capable models struggle to utilize such feedback.

<br>

üß† **Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care** [source](http://arxiv.org/pdf/2507.01282v1.pdf) #general 

 Actionable, interpretable hybrid AI that integrates expert knowledge and clinician input is critical for meaningful clinical adoption and impact in dementia care, surpassing the limitations of prediction-only black-box models.
 - Black-box AI models in dementia care, including large language models, show high accuracy in research but consistently fail to improve real-world diagnostic accuracy, clinician trust, or workflow integration due to lack of interpretability and actionable recommendations.
 - Hybrid AI systems, which combine machine learning with domain expert rules and clinician-in-the-loop feedback, significantly increase interpretability, alignment with clinical guidelines, and adaptability‚Äîoffering contextualized, transparent output and actionable next steps for patient care.
 - Sustained clinical adoption of AI requires tools to provide not only accurate predictions but also clear explanations, context-specific plans, uncertainty alerts, and continuous support for knowledge updating, with rigorous evaluation focusing on clinician understanding, workflow fit, and patient outcomes.

<br>

üõ°Ô∏è **ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks** [source](http://arxiv.org/pdf/2507.01321v1.pdf) #security 

 Backdoor attacks in in-context learning can be robustly mitigated using strategically selected clean demonstrations, making large language models more secure without retraining.
 - Introducing additional clean, high-confidence, and semantically similar examples to poisoned in-context learning (ICL) demonstrations reduced backdoor attack success rates by 29.14% on average, outperforming baseline defenses by nearly 10 times.
 - The probability of successful ICL backdoor attacks is governed by the 'concept preference ratio' between task and attack latent concepts, and increasing this ratio via demonstrations with higher similarity and confidence strongly decreases vulnerability.
 - The ICLShield defense method maintained effectiveness across eleven open-source and two closed-source large language models, preserving original task accuracy while significantly reducing attack vectors even in black-box (closed) settings.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **Graph Representation-based Model Poisoning on Federated LLMs in
  CyberEdge Networks** [source](http://arxiv.org/pdf/2507.01694v1.pdf) #security 

 Graph-based poisoning attacks can stealthily corrupt federated large language models by blending in with benign updates, highlighting the need for defenses beyond traditional statistical detection.
 - Graph representation-based model poisoning attacks can evade state-of-the-art distance and similarity-based defenses in federated large language models, achieving a 47% attack success rate while preserving high model accuracy (over 83%).
 - Existing federated learning defenses, which rely heavily on statistical anomaly detection, fail to identify structurally sophisticated malicious updates that mimic higher-order relationships among benign clients‚Äô gradients.
 - The emergence of graph neural network (GNN)-powered poisoning methods demonstrates that attackers can bypass current security measures and compromise critical decision-making processes in edge-deployed AI, especially in safety-critical domains like healthcare and autonomous vehicles.

<br>

üõ°Ô∏è **Enhancing Android Malware Detection with Retrieval-Augmented Generation** [source](http://arxiv.org/pdf/2506.22750v1.pdf) #security 

 AgenticRAG-based malware detection significantly surpasses other LLM fusion methods, especially when paired with cybersecurity-specialized transformers like CySecBERT, setting a new benchmark for Android malware classification accuracy.
 - The integration of Retrieval-Augmented Generation (AgenticRAG) with transformer-based models achieved a malware detection accuracy of 92.89% and a recall of 96.69%, outperforming Gemini Fusion and traditional LLM fusion approaches by a significant margin.
 - Domain-adapted language models, specifically CySecBERT, demonstrated superior recall and F1-score (recall: 96.69%, F1: 92.86%) in detecting Android malware, highlighting the advantage of fine-tuning on cybersecurity-specific corpora for classifying complex and obfuscated threats.
 - Gemini 2.0 Flash Lite proved to be the most effective fusion model when compared to LLaMA2 and Mistral, but even its best accuracy of 91.36% and recall of 90.50% was outperformed by the AgenticRAG approach, validating the critical role of structured retrieval and agentic planning in security classification pipelines.

<br>

üõ†Ô∏è **More Vulnerable than You Think: On the Stability of Tool-Integrated LLM
  Agents** [source](http://arxiv.org/pdf/2506.21967v1.pdf) #security 

 Tool-integrated LLM agents are far more unstable and attack-prone throughout the tool invocation process than previously recognized, especially regarding parameter errors and response attacks.
 - Open-source tool-integrated language model agents exhibit a 7‚Äì15% higher performance drop than proprietary models when faced with incomplete tool documentation, especially missing parameter descriptions.
 - Parameter-related hallucinations, such as incorrect or missing parameter values, cause task completion rates to decrease by more than 12%, a significantly larger impact than tool selection errors, and larger model size does not mitigate this vulnerability.
 - LLM agents are highly susceptible to response attacks, with some information leakage attacks succeeding more than 90% of the time, and larger models‚Äîwhile more robust to obvious attacks‚Äîbecome more vulnerable to covert, user-like adversarial prompts.

<br>

üß† **Impact of Fine-Tuning Methods on Memorization in Large Language Models** [source](http://arxiv.org/pdf/2507.00258v1.pdf) #security 

 Prompt-based fine-tuning safeguards privacy by minimizing data memorization in large language models, even as models grow, while parameter-based tweaks quickly escalate privacy risks.
 - Prompt-based fine-tuning methods for large language models achieve competitive task performance while reducing memorization of training data, as indicated by low AUC scores (near 0.5), compared to parameter-based approaches which exhibit high vulnerability to membership inference attacks (AUC > 0.8 in some cases).
 - Increasing model size significantly amplifies memorization risks in parameter-based fine-tuning methods (AUC rising from ~0.60 in small models to ~0.99 in largest models), while prompt-based methods maintain consistently low and stable memorization across scales.
 - Configuring parameter-efficient adapters (LoRA) in projection layers dramatically raises memorization, suggesting that placement within transformer blocks critically impacts privacy risks, and structured downstream tasks (like WebNLG) also elicit somewhat higher memorization in prompt-tuned models.

<br>

üõ°Ô∏è **A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents** [source](http://arxiv.org/pdf/2506.23844v1.pdf) #security 

 Agentic AI systems empowered by large models pose novel and intensifying security risks that demand integrated, risk-aware architectures‚Äîlike R2A2‚Äîto internalize safety as a core capability.
 - As large language model-based agents progress from simple tools to fully autonomous systems, their attack surface and vulnerability to security threats such as memory poisoning, tool misuse, reward hacking, and emergent misalignment increase dramatically and become structurally entangled across perception, cognition, memory, and actuation.
 - Existing defense strategies for LLM-based agents‚Äîsuch as input sanitization, memory lifecycle control, constrained decision-making, and post-hoc reflection‚Äîare typically siloed, lacking the cross-layer integration necessary to counter persistent, temporally extended, and cross-module risks in autonomous environments.
 - The introduction of the Reflective Risk-Aware Agent Architecture (R2A2), grounded in Constrained Markov Decision Processes, enables agents to proactively simulate risk trajectories, enforce modular safety contracts, and jointly optimize for reward and risk, providing a blueprint for scalable, principled safety in next-generation autonomous AI agents.

<br>

üé£ **In-context learning for the classification of manipulation techniques in
  phishing emails** [source](http://arxiv.org/pdf/2506.22515v1.pdf) #security 

 In-context learning with LLMs enables accurate, scalable, and nuanced identification of psychological manipulation tactics in phishing emails, revealing key attacker strategies and improving phishing detection capabilities.
 - Large language models utilizing in-context learning achieved a weighted average accuracy of 0.76 for fine-grained classification of psychological manipulation techniques in phishing emails.
 - Certain manipulation strategies‚Äînamely Request For Minor Favor (78%), Baiting (72%), and Curiosity Appeal (71%)‚Äîwere identified as the most prevalent across real-world phishing emails.
 - GPT-4o-mini outperformed eight other leading language models on this classification task, highlighting the influence of model selection in detection efficacy.

<br>

üõ°Ô∏è **On the Surprising Efficacy of LLMs for Penetration-Testing** [source](http://arxiv.org/pdf/2507.00829v1.pdf) #security 

 LLMs have swiftly emerged as powerful‚Äîyet unpredictable‚Äîtools for penetration testing, advancing both cyber defense and offense while raising urgent questions around safety, reliability, and ethics.
 - Large Language Models (LLMs) have demonstrated the ability to autonomously identify and exploit vulnerabilities in systems, with recent studies showing performance on par with human penetration testers in controlled scenarios.
 - Industry and adversarial use of LLMs is rising rapidly, with practical deployments accelerating vulnerability discovery but also enabling threat actors to automate hacking, malware generation, and influence operations.
 - Despite their surprising effectiveness in offensive security, LLM-driven approaches face critical challenges including inconsistent reliability, significant ecological and economic costs, unresolved safety and accountability issues, and ethical concerns regarding dual-use technology.

<br>

ü§ñ **Self-Guided Process Reward Optimization with Masked Step Advantage for
  Process Reinforcement Learning** [source](http://arxiv.org/pdf/2507.01551v1.pdf) #general 

 Eliminating auxiliary process reward models, this framework enables faster, more accurate, and more robust process-level RL for LLMs with greater efficiency and scalability.
 - Self-Guided Process Reward Optimization (SPRO) improves test accuracy by 17.5% over outcome-supervised RL methods and outperforms the previous state-of-the-art process RL method by 8.3%.
 - SPRO increases training efficiency by a factor of 3.4 compared to outcome-supervised baselines and by 6.7 compared to process-supervised baselines, requiring only 15%‚Äì29% of the GPU hours for equivalent performance.
 - The framework maintains higher policy entropy throughout training while reducing average response length by approximately one-third, leading to more concise outputs, enhanced exploration, and effective mitigation of reward hacking.

<br>

üéØ **Activation Reward Models for Few-Shot Model Alignment** [source](http://arxiv.org/pdf/2507.01368v1.pdf) #security 

 Activation Reward Models redefine adaptive model alignment, achieving state-of-the-art reward robustness against exploitation with minimal supervision and no retraining.
 - Activation Reward Models demonstrate up to 78.37% accuracy in mitigating reward hacking in language models, surpassing all tested baselines and even outperforming GPT-4o on critical safety-related benchmarks.
 - This few-shot activation steering method enables rapid, interpretable adaptation to new human preferences for large language and multimodal models, without requiring any additional model fine-tuning or weight updates.
 - Performance improvements from Activation Reward Models are robust even with smaller models and scale with the number of steering examples, showing strong sample efficiency and versatility across language-only and multimodal domains.

<br>

üöÅ **Trusted Routing for Blockchain-Enabled Low-Altitude Intelligent Networks** [source](http://arxiv.org/pdf/2506.22745v1.pdf) #general 

 Integrating blockchain-based zero-trust management and advanced reinforcement learning dramatically improves the speed, security, and efficiency of UAV routing in dynamic low-altitude networks.
 - Implementing a blockchain-enabled zero-trust architecture for low-altitude intelligent networks reduced average end-to-end data transmission delay by 22.38% compared to traditional routing benchmarks.
 - The proposed SHERB-MADDQN-based adaptive routing algorithm achieved up to 24.09% lower E2E delay and required fewer transmission hops under high network demand, enhancing both timeliness and reliability.
 - Simulation results demonstrated improved convergence speed, stability, and training efficiency for routing decisions when embedding observed state information into the reinforcement learning reward structure.

<br>

üõ°Ô∏è **CP-Guard: A Unified, Probability-Agnostic, and Adaptive Framework for
  Malicious Agent Detection and Defense in Multi-Agent Embodied Perception
  Systems** [source](http://arxiv.org/pdf/2506.22890v1.pdf) #security 

 A new framework robustly detects and neutralizes malicious agents in collaborative perception systems, maintaining high perception accuracy with notably improved sampling and verification efficiency.
 - CP-Guard restores collaborative perception system performance nearly to the all-benign upper bound even under strong adversarial attacks, achieving a mIoU of 39.3+ compared to drops below 22 without defense.
 - The PASAC method reduces verification overhead by over 30% compared to prior consensus-based defenses and requires no prior knowledge of the attack ratio, leading to consistently fewer and more stable malicious agent checks.
 - CP-Guard consistently outperforms adversarially trained and baseline consensus defenses in object detection, achieving AP@0.5 scores above 80 against various attacks‚Äîsurpassing the next-best method by up to 6.5 points.

<br>

üõ°Ô∏è **General Autonomous Cybersecurity Defense: Learning Robust Policies for
  Dynamic Topologies and Diverse Attackers** [source](http://arxiv.org/pdf/2506.22706v1.pdf) #cyber 

 This work shows how combining graph-based deep learning with optimal transport enables autonomous cyber defense agents to robustly adapt to unpredictable and diverse network environments, outperforming traditional reinforcement learning baselines.
 - A graph neural network-based agent, enhanced with optimal transport techniques, demonstrated the ability to generalize its defensive policies across up to 128 distinct and dynamic network topologies while sustaining reward performance comparable to specialized state-of-the-art models.
 - In experiments involving sudden network topology changes during cyberattack simulations, this approach maintained consistent defense effectiveness, whereas conventional agents experienced significant drops in performance after such changes.
 - The use of optimal transport mappings to create a structured, continuous latent space for graph encoding significantly outperformed baseline graph neural network models, yielding up to a 58% improvement in cumulative reward scores on multi-topology tasks.

<br>

üîí **SafeMobile: Chain-level Jailbreak Detection and Automated Evaluation for
  Multimodal Mobile Agents** [source](http://arxiv.org/pdf/2507.00841v1.pdf) #security 

 A chain-level defense and evaluation framework dramatically strengthens safety for multimodal mobile agents, cutting jailbreak attacks by 78% and enabling scalable, automated security assessment.
 - SafeMobile raises the average agent safety score (G-Score) from 31.04 to 83.94 and slashes jailbreak success rates by 78.4%, reducing them from 86.1% to 8.4%, while maintaining normal task completion rates within 5.6% of baseline in high-risk mobile scenarios.
 - The defense system demonstrates strong generalizability and robustness, achieving similarly low jailbreak rates (average 16.7% G-ASR) across diverse vision-language models, multi-agent frameworks, and numerous real-world apps and risk scenarios.
 - Automated safety evaluation via GPTJudge achieves at least 0.85 consistency with human experts on over 300 tasks, streamlining and standardizing risk assessment while eliminating the need for costly manual reviews.

<br>

ü™§ **PNAct: Crafting Backdoor Attacks in Safe Reinforcement Learning** [source](http://arxiv.org/pdf/2507.00485v1.pdf) #security 

 Safe RL agents can be surreptitiously compromised with backdoors that cause dangerous actions on command, evading current detection techniques.
 - The PNAct framework exposes that Safe Reinforcement Learning agents can be covertly manipulated through backdoor attacks, resulting in deliberate safety constraint violations only when specific trigger conditions are met.
 - Experimental results show a distinct increase in safety violation costs‚Äîup to two-fold‚Äîduring triggered episodes, while maintaining normal cumulative rewards and low costs in untriggered episodes, demonstrating both the effectiveness and stealthiness of the attack.
 - This attack methodology generalizes to various environments without scenario-specific modifications, and current defense methods targeting reward-based backdoors fail to detect or mitigate these safety-cost-focused threats.

<br>

ü¶æ **Fragile, Robust, and Antifragile: A Perspective from Parameter Responses
  in Reinforcement Learning Under Stress** [source](http://arxiv.org/pdf/2506.23036v1.pdf) #general 

 Targeted synaptic filtering exposes and leverages antifragile network parameters, enabling reinforcement learning agents to not only resist but improve under stress, advancing the design of adaptive and resilient RL systems.
 - Low-pass synaptic filtering consistently reveals antifragile parameters within reinforcement learning policies that not only withstand internal and external stress but can actually improve performance when specific high-magnitude network parameters are suppressed.
 - High-pass filtering identifies policy parameters that are fragile, as their removal leads to significant and rapid performance degradation, especially under potent adversarial attacks like FGSM, which can reduce agent returns to near zero at moderate perturbation strengths (œµ ‚â• 0.5).
 - Antifragility and robustness are environment-dependent, with certain environments (such as HalfCheetah) exhibiting greater resilience to both internal and adversarial perturbations, maintaining moderate performance even as stress levels increase‚Äîsuggesting that targeted filtering can directly enhance adaptability in complex systems.

<br>

üõ°Ô∏è **Curriculum-Guided Antifragile Reinforcement Learning for Secure UAV
  Deconfliction under Observation-Space Attacks** [source](http://arxiv.org/pdf/2506.21129v1.pdf) #security 

 A curriculum-guided antifragile RL approach enables UAVs to safely adapt and maintain performance under severe adversarial sensor attacks, outperforming existing robust and meta-learning methods.
 - The antifragile reinforcement learning framework for UAV deconfliction achieved up to 15% higher cumulative reward and over 30% fewer conflict events compared to standard and robust RL baselines when subjected to strong observation-space attacks such as PGD and GPS spoofing.
 - Antifragile policies maintained bounded divergence in temporal-difference error distributions as adversarial strength increased, while non-adaptive (robust or standard) agents exhibited monotonic increases in catastrophic forgetting and degraded performance under escalating attacks.
 - In benchmarking against existing robust RL, adversarial meta learning, and state-adversarial MDP approaches, the antifragile framework consistently demonstrated superior safety (lower conflict rates and more conflict-free trajectories) and resilience, even in environments with multiple dynamic 3D obstacles and persistent sensor spoofing.

<br>

