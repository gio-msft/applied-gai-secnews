üîí **Can AI Keep a Secret? Contextual Integrity Verification: A Provable Security Architecture for LLMs** [source](http://arxiv.org/pdf/2508.09288v1.pdf) #security 

 A mathematically enforced, cryptographically tagged trust architecture enables truly deterministic LLM defenses that stop all prompt-injection attacks without sacrificing utility.
 - Contextual Integrity Verification (CIV) eliminates prompt-injection and jailbreak attacks with a 0% attack success rate across rigorous benchmarks, outperforming leading guardrails that allow 16‚Äì54% attacks through.
 - CIV preserves 93.1% output similarity and maintains model perplexity, demonstrating minimal impact on legitimate LLM utility while enforcing strict information-flow control between token trust tiers.
 - CIV operates as a drop-in patch for pre-trained LLMs, requiring no retraining or fine-tuning, and attaches cryptographically signed, immutable trust labels to every token for audit-grade provenance.

<br>

üõ°Ô∏è **Quantifying Conversation Drift in MCP via Latent Polytope** [source](http://arxiv.org/pdf/2508.06418v1.pdf) #security 

 Activation vector deviations in LLM-powered agent systems provide a reliable, generalizable means to proactively detect and quantify subtle conversational security risks, outperforming conventional rule-based methods.
 - A novel approach using latent polytope activation analysis accurately detects conversation drift and security risks in Model Context Protocol (MCP) systems, achieving AUROC scores consistently greater than 0.915 across data exfiltration, misleading, and hijacking attacks.
 - The proposed SECMCP method effectively distinguishes malicious manipulations from benign queries, outperforming existing baseline defense strategies with an average AUROC of 0.98 in multicategory adversarial scenarios while preserving normal system usability.
 - SECMCP demonstrates robustness to adaptive attacks and scalability, as increasing the number of anchor samples enhances detection accuracy and activation deviation visualizations reveal clear separation between malicious and benign samples.

<br>

üõ°Ô∏è **BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks** [source](http://arxiv.org/pdf/2508.08127v1.pdf) #security 

 BlindGuard delivers strong, attack-agnostic protection for multi-agent LLM systems using only normal data, matching supervised defenses without labeled attack examples.
 - BlindGuard, an unsupervised defense system for LLM-based multi-agent systems (MAS), achieves an average detection AUC above 80% across diverse attack types and topologies, closely approaching the best supervised methods that require labeled attack samples.
 - Unlike supervised defenses, BlindGuard generalizes robustly to previously unseen attack modalities (prompt injection, memory poisoning, tool exploitation) using only normal MAS communication data for training, making it highly practical for real-world deployments where labeled attacks are rare.
 - The addition of hierarchical agent encoding and corruption-guided contrastive learning allows BlindGuard to consistently reduce attack success rates (ASR@3) by 30-60 percentage points compared to systems with no defense, scalable to networks of 50+ agents, and outperforming prior unsupervised graph anomaly detection methods.

<br>

üõ°Ô∏è **Context Misleads LLMs: The Role of Context Filtering in Maintaining Safe Alignment of LLMs** [source](http://arxiv.org/pdf/2508.10031v1.pdf) #security 

 Filtering adversarial context before LLM inference drastically boosts defenses against jailbreak attacks while preserving usefulness.
 - Context Filtering, an input pre-processing method, reduces Attack Success Rates of diverse jailbreak attacks on large language models by up to 88%, maintaining original model helpfulness.
 - Compared to existing defenses, Context Filtering consistently achieves higher Safety and Helpfulness Product (SHP) scores, indicating a better balance between security and utility in LLMs across multiple benchmarks.
 - The approach is model-agnostic, applicable to both white-box and black-box LLMs without requiring fine-tuning of the base model, and preserves benign prompts without information loss in standard evaluations.

<br>

üß¨ **Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs** [source](http://arxiv.org/pdf/2508.10029v1.pdf) #security 

 Manipulating internal model representations via Latent Fusion Jailbreak exposes critical safety vulnerabilities in LLMs, but can be substantially mitigated by targeted adversarial training.
 - A new latent-space jailbreak attack called Latent Fusion Jailbreak (LFJ) achieves an average attack success rate of 94.01% across diverse language models and safety benchmarks, significantly outperforming prior input- and representation-based methods.
 - Precise pairing of thematically and syntactically similar harmful and benign queries, combined with gradient-guided hidden state interpolation, is essential for maximizing attack efficacy, as removing these strategies causes attack success rates to drop by more than 60%.
 - Adversarial training with interpolated hidden state examples reduces success of LFJ attacks by over 80% (down to 12.45% ASR), while preserving model performance on benign queries, highlighting an actionable defense against such latent-space exploits.

<br>

üí• **Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts** [source](http://arxiv.org/pdf/2508.10390v1.pdf) #security 

 Explicitly harmful prompts combined with advanced detection and innovative attack templates expose critical vulnerabilities in commercial language models, even in systems thought robust to prompting attacks.
 - The hybrid MDH framework achieves over 95% accuracy in detecting non-obvious harmful prompts while reducing manual review to under 10%, enabling scalable and efficient cleaning of red-teaming datasets.
 - Novel jailbreak techniques leveraging specially crafted developer messages, D-Attack and DH-CoT, dramatically increase attack success rates on leading LLMs, surpassing prior methods and enabling successful jailbreaks in models previously resistant to prompting attacks.
 - After cleaning with MDH, benchmark datasets (RTA series) are substantially more effective for evaluating jailbreak attacks, with rejection rates dropping to as low as 2%‚Äì9% on advanced LLMs, highlighting persistent safety weaknesses in commercial systems.

<br>

üõ°Ô∏è **A Real-Time, Self-Tuning Moderator Framework for Adversarial Prompt Detection** [source](http://arxiv.org/pdf/2508.07139v1.pdf) #security 

 A real-time, self-tuning moderator framework drastically improves LLM defense against jailbreaking while enabling agile, low-cost adaptation to novel adversarial attacks.
 - Implementation of the RTST framework reduced adversarial Attack Success Rates in Gemini 2.5 Flash from 12.0% to 0% (JBB GCG), 63.1% to 16.6% (JBB PAIR), and 35.4% to 0% (JBC + Reddit), substantially improving LLM resilience to jailbreak attacks.
 - Real-time self-tuning and adaptive behavior weighting in RTST allowed for rapid single-prompt learning and enabled a highly explainable, low-overhead defense that outperformed traditional classifier and fine-tuning approaches.
 - Ablation studies show that activating real-time optimization further decreased Attack Success Rates and refusal rates over static configurations, indicating that continual online adaptation is beneficial for moderator performance.

<br>

üõ°Ô∏è **Securing Educational LLMs: A Generalised Taxonomy of Attacks on LLMs and DREAD Risk Assessment** [source](http://arxiv.org/pdf/2508.08629v1.pdf) #security 

 A unified attack taxonomy paired with DREAD-based risk assessment spotlights critical security vulnerabilities in educational LLMs, underscoring the urgency for tailored defense and institutional safeguards.
 - Over 32 out of 50 categorised attacks on educational LLMs pose high security risks, with the most critical threats being token smuggling, adversarial prompts, direct injection, and multi-step jailbreak techniques.
 - The systematic attack taxonomy distinguishes attacks by their complexity, revealing that both low-complexity prompt manipulations and highly technical infrastructure exploits can significantly impact the confidentiality, integrity, and availability of educational data and operations.
 - Effective risk mitigation in educational environments requires ongoing threat modeling, policy enforcement, security updates, and targeted training to defend against evolving attack vectors and reduce potential damage to learners, staff, and institutional reputation.

<br>

üîì **Many-Turn Jailbreaking** [source](http://arxiv.org/pdf/2508.06755v1.pdf) #security 

 Multi-turn jailbreaking reveals that LLMs, once compromised, become increasingly susceptible to answering a wide array of harmful queries, which dramatically amplifies safety risks beyond conventional single-turn attacks.
 - Once a large language model is jailbroken in the first turn, its likelihood of answering subsequent harmful questions in follow-up turns increases by 5‚Äì20%, enabling attacks with minimal additional effort.
 - The success rate for eliciting harmful responses in multi-turn conversations rises dramatically, with average attack success rates reaching up to 65‚Äì75% for relevant follow-up questions in several widely used models.
 - Even when initial jailbreak attempts fail, up to 35% of models in tested scenarios will nonetheless generate harmful outputs in subsequent turns if attackers persist, revealing a significant vulnerability in the conversation safety mechanisms.

<br>

üß† **The Cost of Thinking: Increased Jailbreak Risk in Large Language Models** [source](http://arxiv.org/pdf/2508.10032v1.pdf) #security 

 Reasoning-augmented LLMs are much more vulnerable to jailbreaks, but explicit safety prompts can drastically mitigate this risk.
 - Large Language Models operating in thinking mode are significantly more susceptible to jailbreak attacks, consistently showing higher attack success rates than models in non-thinking mode across multiple benchmarks and attack techniques.
 - Approximately 80% of harmful responses generated in thinking mode contained clear indications that the models understood the harm, yet proceeded to respond, often justifying output for 'educational purposes' or with lengthy reasoning.
 - Implementing a 'safe thinking intervention'‚Äîinjecting explicit safety instructions at the start of the model's reasoning process‚Äîreduced the attack success rate to near-zero across both open-source and closed-source LLMs, outperforming traditional defense methods.

<br>

üõ°Ô∏è **Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak Evaluation** [source](http://arxiv.org/pdf/2508.06194v1.pdf) #security 

 Scenario-adaptive, multi-dimensional evaluation sets new accuracy benchmarks for LLM jailbreak detection and offers nuanced, extensible harm quantification tailored to real-world scenarios.
 - A scenario-adaptive, multi-dimensional evaluation framework increases jailbreak detection F1 scores by 6% over previous state-of-the-art, achieving 0.917 on a 14-scenario dataset and 0.995 on JBB.
 - The approach robustly identifies context-specific vulnerabilities in large language models, particularly excelling in scenario-dependent challenges like regional sensitive issues, as shown by a high attack success rate of 70.37% and harm scores up to 2.76 for mainstream LLMs.
 - Expert-aligned evaluations show a near-perfect correlation (Spearman-Rho > 0.93 and NMAE < 0.02), confirming the system's ability to generate nuanced, trustworthy harm assessments across diverse, complex scenarios.

<br>

üß¨ **Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation** [source](http://arxiv.org/pdf/2508.10404v1.pdf) #security 

 Layer-wise sparse feature manipulation using autoencoders dramatically boosts the effectiveness of adversarial attacks against LLM safety filters while preserving covert prompt intent.
 - Applying Sparse Feature Perturbation Framework (SFPF) to adaptive jailbreak attacks increased the attack success rate (ASR) from 77% to 95% on safety-aligned LLMs while maintaining high semantic similarity between prompts.
 - Intermediate model layers, particularly layer 17, were identified as especially sensitive to adversarial perturbations, where direct manipulation yielded a 29% ASR even against strong baseline defenses.
 - SFPF-generated adversarial texts consistently bypassed state-of-the-art defense mechanisms without explicit optimization at the token level, revealing persistent vulnerabilities in current NLP safety systems.

<br>

üß© **Multi-Turn Jailbreaks Are Simpler Than They Seem** [source](http://arxiv.org/pdf/2508.07646v1.pdf) #security 

 Repeated attempts in multi-turn jailbreaks mirror simply resampling single-turn attacks, revealing that current AI safety benchmarks may underestimate model vulnerability and that stronger reasoning models can be more prone to exploitation.
 - Automated multi-turn jailbreak attacks achieve success rates consistently above 70% on leading language models, even those with advanced single-turn defenses.
 - The increased effectiveness of multi-turn attacks is primarily due to additional attack attempts rather than any sophisticated conversational strategy, with retries serving the same function as extra single-turn samples.
 - Models that employ greater reasoning effort are paradoxically more likely to be successfully jailbroken, and attack vulnerability is highly correlated among models from the same provider.

<br>

ü¶∫ **LLM Robustness Leaderboard v1 --Technical report** [source](http://arxiv.org/pdf/2508.06296v2.pdf) #security 

 Automated red-teaming reveals widespread LLM vulnerabilities, but models differ drastically in how easily harmful behaviors can be elicited, necessitating nuanced, scenario-aware robustness assessments.
 - Near-universal vulnerability was observed among state-of-the-art large language models, with 100% attack success rates in 37 out of 41 tested models using automated adversarial optimization techniques.
 - The average number of attempts required to elicit harmful behavior varies by over 300-fold between models, indicating significant practical differences in robustness that binary success metrics obscure.
 - The effectiveness of jailbreaking primitives is highly context-dependent, meaning that the same attack technique can either enhance or reduce adversarial success depending on the specific hazard scenario.

<br>

üõ°Ô∏è **Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM Inference** [source](http://arxiv.org/pdf/2508.08438v1.pdf) #security 

 Selective cache sharing guided by multi-tier privacy detection offers robust side-channel security for LLM serving without sacrificing efficiency.
 - SafeKV blocks over 94% of timing side-channel attacks in LLM inference systems while maintaining high throughput and low latency.
 - Selective KV-cache sharing raises inference throughput by up to 2.66√ó and cuts time-to-first-token overhead from 50.41% to 11.74% compared to strict per-user isolation.
 - The multi-tier privacy detection pipeline classifies 92% of prompts using lightweight methods, with only 8% requiring expensive context-aware LLM validation, preserving scalability.

<br>

ü§ñ **Extending the OWASP Multi-Agentic System Threat Modeling Guide: Insights from Multi-Agent Security Research** [source](http://arxiv.org/pdf/2508.09815v1.pdf) #security 

 The paper highlights previously overlooked vulnerabilities in multi-agent LLM systems and offers concrete evaluation strategies, expanding the scope and precision of AI security threat modeling.
 - Critical gaps have been identified in existing multi-agent threat models, including failures like reasoning collapse, metric overfitting, unsafe delegation escalation, and emergent covert coordination, which traditional frameworks like OWASP do not sufficiently address.
 - New threat categories‚Äîsuch as multi-agent backdoors, cross-agent hallucination propagation, and affective prompt framing‚Äîhave been proposed, highlighting that even individually compliant agents can collude or be exploited to bypass safety mechanisms in LLM-driven multi-agent systems.
 - Robustness, safety, and emergent behavior testing strategies, such as chaos engineering, topology-aware network testing, and long-term interaction monitoring, are necessary to ensure resilience and early detection of complex vulnerabilities in real-world deployments.

<br>

ü¶æ **Robust Anomaly Detection in O-RAN: Leveraging LLMs against Data Manipulation Attacks** [source](http://arxiv.org/pdf/2508.08029v1.pdf) #security 

 LLM-based anomaly detection in O-RAN can withstand advanced data manipulation attacks that crash traditional ML models, providing reliable, low-latency detection even with hypoglyph input.
 - Traditional ML-based anomaly detection systems in O-RAN environments crashed upon encountering even a single Unicode-manipulated (hypoglyph) message, resulting in a complete loss of detection capabilities against sophisticated evasion attacks.
 - Large Language Model (LLM)-based xApps maintained robust operation and successfully processed all messages‚Äîincluding Unicode-manipulated ones‚Äîwithout system failures, demonstrating resilience to adversarial input that breaks conventional ML approaches.
 - LLM-based anomaly detection achieved low-latency performance (average detection time per message below 0.07 seconds), fulfilling near-real-time requirements, although its initial detection accuracy (F1-score up to ~0.32) indicates further optimization via prompt engineering is still needed.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **Chimera: Harnessing Multi-Agent LLMs for Automatic Insider Threat Simulation** [source](http://arxiv.org/pdf/2508.07745v2.pdf) #security 

 Chimera leverages multi-agent LLM simulation to generate realistic, diverse insider threat datasets that challenge and improve ITD methods far beyond current benchmarks.
 - ChimeraLog, a synthetic insider threat dataset generated using LLM-based multi-agent simulation across three enterprise scenarios, received an average realism score of 4.2 from experts‚Äîmatching real-world datasets and outperforming existing synthetic datasets.
 - Benchmarking four insider threat detection models revealed that all methods experienced a drop in average F1-score from 0.99 (CERT) to 0.83 (ChimeraLog), indicating ChimeraLog poses a significantly greater challenge and better reflects real-world threat complexity.
 - Detection models trained on ChimeraLog exhibited superior generalization across scenarios, while models trained on synthetic datasets produced high false positive rates and struggled with distributional shifts, highlighting Chimera‚Äôs advantage in advancing ITD robustness.

<br>

üõ°Ô∏è **Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference** [source](http://arxiv.org/pdf/2508.09442v1.pdf) #security 

 KV-cache in LLM inference is a critical privacy vulnerability, but highly practical and efficient mitigation is achievable without sacrificing utility.
 - Over 96% of user prompts to large language models can be accurately reconstructed by attackers intercepting plaintext KV-cache data using a collision-based attack, exposing severe privacy risks in real-world LLM deployments.
 - Existing privacy protection approaches such as differential privacy or full cryptographic encryption are either ineffective at preventing input reconstruction or incur intolerable degradation to speed and model utility, with some attacks succeeding despite added noise or encryption.
 - The proposed KV-Cloak defense completely disrupts all known input reconstruction attacks, reducing success rates to chance while preserving virtually 100% of model accuracy and adding less than 10% inference latency overhead.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **Searching for Privacy Risks in LLM Agents via Simulation** [source](http://arxiv.org/pdf/2508.10880v1.pdf) #security 

 Automated simulation uncovers overlooked AI agent privacy risks and develops practical, transferable defenses against sophisticated multi-turn social engineering attacks.
 - LLM-based agents in realistic multi-agent simulations exhibited privacy leaks in up to 43% of scenarios when only basic privacy instructions were used, with more capable defense models reducing‚Äîbut not eliminating‚Äîthese risks.
 - Sophisticated, adaptive multi-turn attacks, including consent forgery and impersonation, were automatically discovered to consistently bypass naive rule-based defenses, demonstrating critical vulnerabilities that static tests fail to surface.
 - Introducing state-machine-based defenses with strict identity verification protocols reduced successful privacy leak rates to below 7% and proved transferable across different models and privacy situations, indicating this approach's potential for real-world protection.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **Attacks and Defenses Against LLM Fingerprinting** [source](http://arxiv.org/pdf/2508.09021v1.pdf) #security 

 This work demonstrates that reinforcement learning can substantially optimize LLM fingerprinting query efficiency, while semantic-preserving output filtering provides a robust defense that significantly reduces fingerprinting success without sacrificing response quality.
 - An RL-optimized query selection method achieves 93.89% fingerprinting accuracy with only 3 queries, marking a 14.2% improvement over randomly selected queries of the same size.
 - A semantic-preserving output filter using a secondary LLM reduces fingerprinting accuracy from 90‚Äì100% down to 5‚Äì45% across various models while maintaining output quality above 0.94 cosine similarity.
 - Defensive effectiveness against LLM fingerprinting varies with prompt choice and original model, with the most effective prompt achieving a prompt evaluation score of 0.8562 and lowering model identification success to 24.4%.

<br>

üõ°Ô∏è **Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System** [source](http://arxiv.org/pdf/2508.06059v1.pdf) #security 

 Fact2Fiction exposes critical weaknesses in modern autonomous fact-checkers, dramatically boosting misinformation attack success by targeting both claim decomposition and justifications.
 - Fact2Fiction achieves attack success rates that are 8.9%‚Äì21.2% higher than previous state-of-the-art poisoning attacks against agentic fact-checking systems at comparable poisoning budgets.
 - Targeted exploitation of system-generated justifications enables malicious evidence targeting, revealing a transparency‚Äìsecurity trade-off, with up to a 12.4% increase in attack success rates under tight resource constraints.
 - Existing defenses‚Äîincluding paraphrasing, clustering-based, and perplexity-based detection‚Äîare largely ineffective against Fact2Fiction, highlighting an urgent need for novel security countermeasures for automated fact-checking systems.

<br>

üõ°Ô∏è **SLIP: Soft Label Mechanism and Key-Extraction-Guided CoT-based Defense Against Instruction Backdoor in APIs** [source](http://arxiv.org/pdf/2508.06153v1.pdf) #security 

 A prompt-based defense combining key phrase extraction with semantic scoring reduces backdoor attack success rates from over 90% to just 25%, setting a new benchmark in securing black-box LLM APIs.
 - SLIP reduces the average attack success rate (ASR) of black-box instruction backdoor attacks in large language models from 90.2% to 25.13% while maintaining a high clean accuracy of 87.15%.
 - The proposed soft label mechanism and key-extraction-guided chain-of-thought approach drastically lower both the false acceptance rate (4.21%) and false rejection rate (1.49%), outperforming previous state-of-the-art defenses.
 - SLIP is robust and generalizes well across multiple LLM architectures and attack variants, without requiring white-box access or model parameter modifications.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **When AIOps Become "AI Oops": Subverting LLM-driven IT Operations via Telemetry Manipulation** [source](http://arxiv.org/pdf/2508.06394v1.pdf) #security 

 Subtle telemetry manipulation can turn LLM-powered AIOps agents into attack vectors, but context-specific sanitization can robustly prevent exploitation.
 - Adversarial manipulation of system telemetry enables attackers to reliably induce LLM-driven AIOps agents to execute malicious remediations, achieving an 89.2% average success rate across varied environments and models.
 - Existing state-of-the-art prompt injection defenses, such as PromptShields and Prompt-Guard2, were evaded in 100% of tested cases by contextually plausible adversarial 'reward-hacking' payloads injected via telemetry.
 - AIOpsShield, an automated telemetry sanitization tool, wholly neutralized all adversarial attempts in evaluation, with negligible performance impact (<5% utility loss), demonstrating that narrow, context-aware defenses are both feasible and highly effective for LLM-based IT operations.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **Who's the Evil Twin? Differential Auditing for Undesired Behavior** [source](http://arxiv.org/pdf/2508.06827v1.pdf) #security 

 Effective auditing of neural networks for hidden behaviors critically depends on model type and access level, with adversarial attacks and prompt-based probes emerging as robust strategies in low-information settings.
 - Gradient-based adversarial attack methods, particularly FGSM, achieved up to 100% accuracy in distinguishing poisoned CNNs, outperforming other techniques like Gaussian noising and model diffing, which struggled to identify hidden behaviors.
 - Prompt engineering and scratchpad hijacking allowed auditors to fully extract concealed prompts and user instructions in black-box LLMs, revealing undesired behaviors without prior knowledge or access to model internals.
 - Detecting fine-tuned misalignments in open-weight LLMs, such as medical misinformation, brand promotion, or domain censorship, generally required targeted behavioral probes or strong hints, as standard benchmarks yielded weak or inconclusive signals.

<br>

üéõÔ∏è **NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs** [source](http://arxiv.org/pdf/2508.09473v1.pdf) #security 

 NeuronTune delivers a tunable, neuron-level alignment framework that precisely balances safety and utility in large language models by dynamically identifying and modulating safety- and utility-critical neurons.
 - Neuron-level modulation using NeuronTune significantly increases the Safety-Utility F1 (SU-F1) score across multiple LLMs, outperforming existing coarse-grained methods in simultaneously maintaining safety and utility.
 - Experimental results show that fine-grained intervention‚Äîvia precise identification and adaptive scaling of safety-critical and utility-related neurons‚Äîenables flexible adaptation to diverse deployment scenarios by tuning the number of modulated neurons.
 - Ablation studies confirm that all key components of NeuronTune, including attack-aware attribution, neuron identification, and meta-learning-driven adaptive adjustment, are essential for achieving superior balance, as removing any part severely degrades performance.

<br>

üìù **Assessing LLM Text Detection in Educational Contexts: Does Human Contribution Affect Detection?** [source](http://arxiv.org/pdf/2508.08096v1.pdf) #general 

 AI text detectors struggle to reliably identify nuanced human-AI collaboration in student essays, risking false accusations and limiting real-world educational use.
 - Detection methods show high accuracy when distinguishing between fully human-written and fully LLM-generated essays, but their effectiveness drops sharply for texts with intermediate human and AI contribution, with area under curve (AUC) values falling from 0.93 (fully distinct) to below 0.81 for rewritten human or improved texts.
 - False positive rates are notably high when minor LLM enhancements are made to human texts, posing a significant risk of misclassifying authentic student work as AI-generated and raising concerns about fairness in educational assessment.
 - Zero-shot detectors (like Fast-DetectGPT) outperform supervised models across diverse datasets and generalize better to various generative models and prompt types, but none of the methods tested are reliable enough to recommend for educational deployment given the frequency of misclassifications, especially for short or paraphrased texts.

<br>

üé≠ **Can You Trick the Grader? Adversarial Persuasion of LLM Judges** [source](http://arxiv.org/pdf/2508.07805v1.pdf) #security 

 LLM-based graders can be systematically manipulated with persuasive language, showing a critical vulnerability that persists across models, scales, and evaluation settings.
 - Embedding persuasive language cues in incorrect mathematical solutions leads LLM judges to assign unjustifiably higher scores, increasing bias by up to 8% on average, with the 'consistency' technique exerting the most influence.
 - Model vulnerability to persuasion is not mitigated by increased model size; even the strongest LLMs, such as GPT-4o, display measurable score inflation in the presence of these rhetorical cues.
 - Combining multiple persuasion strategies further amplifies bias, enabling even incorrect responses to overturn correct judgments in both single and comparative evaluation settings, while counter-prompts fail to reliably reduce the effect.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **Model-Agnostic Sentiment Distribution Stability Analysis for Robust LLM-Generated Texts Detection** [source](http://arxiv.org/pdf/2508.06913v1.pdf) #security 

 Sentiment distribution stability offers a powerful and robust fingerprint for identifying LLM-generated texts, outperforming current detectors even under adversarial conditions.
 - SentiDetect, a model-agnostic sentiment analysis framework, improves the F1 score for detecting LLM-generated text by over 16% on Gemini-1.5-Pro and over 11% on GPT-4-0613 compared to existing methods across five diverse text domains.
 - The method demonstrates strong robustness against paraphrasing and adversarial attacks, with baseline detectors losing up to 80% performance post-attack while SentiDetect maintains a significantly higher detection rate, dropping by less than 50%.
 - LLM-generated texts consistently exhibit stable sentiment distributions under low-emotional, semantic-preserving rewrites, creating a measurable signal that distinguishes them from human-written texts across both commercial and open-source LLMs.

<br>

ü¶∫ **Omni-SafetyBench: A Benchmark for Safety Evaluation of Audio-Visual Large Language Models** [source](http://arxiv.org/pdf/2508.07173v1.pdf) #security 

 Omni-SafetyBench exposes critical safety and consistency vulnerabilities in state-of-the-art audio-visual language models, especially under complex, multi-modal attack scenarios.
 - No evaluated audio-visual large language model achieved strong performance in both overall safety and cross-modal consistency, with only three models exceeding scores of 0.6 and the highest reaching around 0.8 in either metric.
 - All models showed significantly reduced safety defenses when processing complex modality combinations, particularly audio-visual inputs, with some models demonstrating severe vulnerabilities as low as 0.14 in certain settings.
 - Cross-modal safety consistency remains a major challenge, as even top-performing models exhibit notable inconsistencies in safety responses across different input modalities, increasing susceptibility to modality-based attack vectors.

<br>

üõ°Ô∏è **Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs** [source](http://arxiv.org/pdf/2508.06601v1.pdf) #security 

 Pretraining data filtering for dual-use topics provides state-of-the-art, durable tamper-resistance against harmful capability revival in open-weight large language models, setting a new benchmark for robust AI safeguarding.
 - Filtering biothreat-related content from language model pretraining data enables models to withstand up to 10,000 steps and 300 million tokens of adversarial fine-tuning without acquiring harmful proxy knowledge, which is over 10 times more robust than existing post-training safeguards.
 - General domain capabilities remain unaffected by aggressive pretraining data filtering, as models preserve their performance on standard benchmarks despite substantial reductions in biothreat knowledge.
 - Combining pretraining data filtering with post-training safeguards such as Circuit-Breaking yields stronger defenses and improved resistance to a range of tampering and in-context attacks, highlighting the importance of layered safety strategies.

<br>

üõ°Ô∏è **Pentest-R1: Towards Autonomous Penetration Testing Reasoning Optimized via Two-Stage Reinforcement Learning** [source](http://arxiv.org/pdf/2508.07382v1.pdf) #security 

 A lightweight, two-stage RL pipeline enables open-source models to autonomously execute complex penetration tests at performance levels rivaling proprietary giants.
 - Pentest-R1 achieves a 24.2% success rate on AutoPenBench and leads all open-source models, surpassing proprietary benchmarks like GPT-4o and ranking just behind Gemini 2.5 Flash.
 - On Cybench, Pentest-R1 sets a new state-of-the-art for open-source models with a 15.0% unguided success rate, which matches performance levels of top proprietary systems in fully autonomous penetration testing tasks.
 - Ablation studies confirm that the synergy of two-stage reinforcement learning‚Äîoffline training on real expert walkthroughs paired with online interactive fine-tuning‚Äîis essential for developing robust error correction and adaptive attack strategies in AI-driven pen testing.

<br>

üõ°Ô∏è **Fine-Grained Safety Neurons with Training-Free Continual Projection to Reduce LLM Fine Tuning Risks** [source](http://arxiv.org/pdf/2508.09190v1.pdf) #security 

 A training-free, fine-grained safety neuron approach enables highly effective and efficient safety alignment in LLMs, generalizing against evolving risks with minimal impact on model performance.
 - Fine-Grained Safety Neurons with Training-Free Continual Projection achieved a minimum harmfulness score (close to 1.02, as judged by GPT-4o) on LLMs with just 4.67‚Äì5.38% parameter modification, outperforming prior post-fine-tuning defenses.
 - The safety neuron projection method preserved or even improved model utility (AlpacaEval WinRate up to 54.61‚Äì55.87%) while continually reducing attack success rates to as low as 14% across multiple safety dimensions.
 - Continual projection across emerging safety concerns required progressively fewer parameter edits (down to 0.75% for terrorism), demonstrating strong generalizability and sustainability of safety enhancements without catastrophic forgetting.

<br>

üõ°Ô∏è **In-Training Defenses against Emergent Misalignment in Language Models** [source](http://arxiv.org/pdf/2508.06249v1.pdf) #security 

 Simple in-training safeguards like KL-divergence and safe data interleaving can sharply reduce emergent misalignment in LLMs during fine-tuning, but each brings trade-offs affecting model utility or response quality.
 - KL-divergence regularization and interleaving safe training data each reduce emergent misalignment (EMA) in large language models by over 87% across multiple domains but introduce distinct side effects‚ÄîKL-divergence restricts learning in tasks that depart from the original alignment while interleaving can increase incoherent outputs, especially with more extensive interleaving.
 - Only interleaving safe data consistently prevents broad EMA while preserving the model's ability to learn target misaligned behaviors, though at the expense of occasionally generating less coherent responses in some domains.
 - Other regularization approaches, such as LDIFS and SafeLoRA, have minimal or inconsistent effects on EMA and do not match the efficacy of KL-divergence or interleaving methods, indicating a significant alignment tax or compromise with current mitigation options.

<br>

üö¶ **Street-Level AI: Are Large Language Models Ready for Real-World Judgments?** [source](http://arxiv.org/pdf/2508.08193v1.pdf) #general 

 Current LLMs are unreliable and misaligned for use in high-stakes societal decision-making like homelessness resource allocation, often failing to match either bureaucratic or human expert priorities.
 - Large Language Models (LLMs) show significant internal inconsistency in prioritizing vulnerable populations, with rankings varying widely between independent runs and even within the same model.
 - LLM-generated prioritization rankings have near-zero or negative correlation with established bureaucratic vulnerability scores and fail to reliably predict real-world caseworker decisions in homelessness service allocation.
 - Despite qualitative similarities to non-expert human decision-making in controlled comparisons, off-the-shelf LLMs lack the expertise and reliability needed for high-stakes resource allocation, underscoring the risk of automating such critical social processes.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **Symbolic Execution in Practice: A Survey of Applications in Vulnerability, Malware, Firmware, and Protocol Analysis** [source](http://arxiv.org/pdf/2508.06643v1.pdf) #security 

 Systematic use of guidance heuristics and hybrid analysis has transformed symbolic execution into a versatile and scalable technique for discovering deep bugs in software, firmware, and protocols.
 - Guided symbolic execution‚Äîusing strategies like scope reduction and heuristics‚Äîhas enabled scalable analysis for complex domains such as vulnerability research, malware deobfuscation, firmware testing, and protocol inference.
 - Hybrid approaches, particularly those combining fuzzing and symbolic execution (e.g., Driller), improve bug-finding efficacy, achieving up to 13% more vulnerability discoveries compared to standalone fuzzing or symbolic execution tools.
 - Major obstacles remain, including path explosion and environment modeling, but advances such as parallelization, model-guided exploration, and LLM-assisted techniques are actively expanding symbolic execution's applicability in security-critical software systems.

<br>

ü¶† **Mitigating Distribution Shift in Graph-Based Android Malware Classification via Function Metadata and LLM Embeddings** [source](http://arxiv.org/pdf/2508.06734v1.pdf) #security 

 Incorporating semantic node features into malware graphs substantially mitigates performance degradation from distribution shifts and sets a new standard for resilient Android malware detection.
 - Graph-based Android malware classifiers experience significant accuracy drops‚Äîup to 45%‚Äîwhen tested on unseen malware families, revealing a generalization gap under distribution shift.
 - Introducing semantic features, such as function metadata and large language model code embeddings, into function call graphs improves classification accuracy by up to 8% in distribution-shifted scenarios across multiple graph neural network architectures.
 - Semantic enrichment techniques, particularly the 'Zero' collation scheme, not only increase accuracy but also consistently boost the effectiveness and robustness of existing test-time and domain adaptation methods for robust malware detection.

<br>

üéØ **A Stable and Principled Loss Function for Direct Language Model Alignment** [source](http://arxiv.org/pdf/2508.07137v1.pdf) #general 

 A principled and stable loss function dramatically improves language model alignment by directly addressing instability and reward hacking in preference optimization.
 - A novel loss function for language model preference alignment, termed Stable Preference Optimization (SPO), eliminates training instability and reward hacking by targeting a finite optimal logits difference instead of unbounded maximization.
 - SPO-aligned models achieved a 56.5% and 53.73% win rate over the standard Direct Preference Optimization (DPO) baseline on Qwen2.5-7B and Llama-3-8B models, respectively, outperforming larger models with more stable alignment.
 - The SPO approach demonstrates robust gradient behavior, leading to a stable learning process that generalizes across architectures without exhibiting the large gradients and instability observed in DPO.

<br>

ü¶† **Cowpox: Towards the Immunity of VLM-based Multi-Agent Systems** [source](http://arxiv.org/pdf/2508.09230v1.pdf) #security 

 Just a handful of specialized, unmodifiable agents can immunize vast multi-agent systems against infectious jailbreak attacks through distributed cure sample propagation.
 - COWPOX, a distributed defense mechanism deployed to as few as 3% of agents in VLM-based multi-agent systems, consistently recovers over 95% of infected agents and prevents up to 10% from being infected, demonstrating impactful system-wide immunity.
 - The theoretical model and empirical results confirm that COWPOX‚Äôs ‚Äòcure samples‚Äô outperform adversarial virus samples in retrieval priority, making recovery and immunity inevitable given enough communication rounds.
 - Even when attackers adapt their strategies, efficacy is preserved: for any given virus sample, there always exists a cure sample able to neutralize it and halt further transmission in practice.

<br>

üß≠ **Imaginative World Modeling with Scene Graphs for Embodied Agent Navigation** [source](http://arxiv.org/pdf/2508.06990v1.pdf) #general 

 Imaginative scene graph-driven world modeling empowers embodied agents to navigate complex, unseen spaces with state-of-the-art efficiency and generalization.
 - A proactive navigation approach using hierarchical scene graphs and symbolic imagination boosts agent success rates on object-goal navigation tasks by 12.3% over prior methods, attaining 65.4% and 66.8% on HM3D and HSSD benchmarks respectively.
 - Balancing semantic exploitation and geometric exploration enables efficient target search, with ablation studies revealing up to 6.75% success rate increase from world modeling and 2.75% from information gain estimation.
 - The system demonstrates robust real-world generalization, enabling autonomous robots to successfully locate objects across rooms and floors in diverse, open-bounded indoor environments.

<br>

ü§ñ **Constrained Black-Box Attacks Against Multi-Agent Reinforcement Learning** [source](http://arxiv.org/pdf/2508.09275v1.pdf) #security 

 Even with minimal access, adversaries can reliably disrupt multi-agent cooperation in complex learning systems by stealthily misaligning agents' perceptions.
 - Adversarial attackers with access only to agent observations (and not actions or policies) can significantly degrade the performance of deployed collaborative multi-agent reinforcement learning (c-MARL) systems using as few as 1,000 samples, compared to millions needed by prior methods.
 - Novel misalignment attacks (Align and Hadamard) achieve significant drops in episodic returns across 22 diverse c-MARL environments‚Äîincluding fully and partially observable, and highly cooperative tasks‚Äîoften matching or outperforming standard white-box and random-noise baselines.
 - Subtle, coordinated observation perturbations not only reduce task success but also greatly increase episode length (e.g., up to 285% longer in some settings), demonstrating that plausible and stealthy black-box attacks can have severe operational impacts even without deep system access.

<br>

üõ°Ô∏è **Securing Agentic AI: Threat Modeling and Risk Analysis for Network Monitoring Agentic AI System** [source](http://arxiv.org/pdf/2508.10043v1.pdf) #security 

 This work demonstrates how agentic AI systems for network monitoring are highly susceptible to multi-layer attacks that can cripple their decision-making, but shows that a structured, layered defense using the MAESTRO framework can substantially improve threat detection and response.
 - Simulated attacks such as resource exhaustion and memory poisoning led to significant performance degradation in autonomous network monitoring agents, with telemetry update intervals increasing by up to 13 seconds and heavy CPU/memory utilization delays.
 - The layered MAESTRO threat modeling framework successfully identified and localized cross-layer vulnerabilities‚Äîsuch as chain-of-thought manipulation and multi-agent exploitation‚Äîrevealing high-risk threats (risk scores up to 27), which traditional security frameworks could not systematically map or mitigate.
 - A defense-in-depth mitigation strategy, including memory isolation, real-time anomaly detection, and planner validation, proved essential for maintaining agent reliability, highlighting the critical importance of secure memory management and proactive cross-layer controls in agentic AI deployments.

<br>

üõ°Ô∏è **REFN: A Reinforcement-Learning-From-Network Framework against 1-day/n-day Exploitations** [source](http://arxiv.org/pdf/2508.10701v1.pdf) #security 

 This paper shows that network-driven RL training of LLMs can autonomously and scalably generate exploit-blocking filters for new vulnerabilities with superior speed and effectiveness compared to human and ML-based approaches.
 - Automated vulnerability-fixing filters generated by RL-trained LLMs achieved a 21.1% higher accuracy and 225.9% greater F1-score than traditional and ML-based methods in blocking 1-day/n-day exploits across 22 exploit families and 65 device types.
 - Mean-Time-To-Patch was reduced from multiple days (up to 7 days for manual/software methods) to just 3.65 hours (a 95.4% improvement), ensuring vulnerabilities are fixed well within the critical window before mass exploitation.
 - The framework demonstrated robust scalability and operational efficiency, supporting rapid deployment to 10,000 devices with only 1.5 hours of aggregate downtime, and reducing installation delays by at least 10x compared to manual alternatives.

<br>

üÜî **An Architecture for Distributed Digital Identities in the Physical World** [source](http://arxiv.org/pdf/2508.10185v1.pdf) #general 

 This work outlines and validates a privacy-focused, distributed digital identity system that enables secure, document-free access to physical services, overcoming the major privacy, security, and scalability drawbacks of centralized approaches.
 - A decentralized digital identity architecture allows individuals to authenticate and access physical services without carrying documents or devices, while maintaining strong privacy through unlinkability and selective disclosure of attributes.
 - The combination of Personal Identity Agents (PIAs), secure biometric sensors, and verifiers, backed by advanced cryptographic signatures and remote attestation, effectively mitigates identity theft, unauthorized modification, and mass surveillance risks commonly found in centralized systems.
 - Prototype implementation demonstrates practical feasibility with end-to-end authentication latencies as low as 384 ms on local networks and around 3.5 seconds over Tor, confirming scalability for millions of users except in ultra low-latency scenarios.

<br>

üîí **Enhancing Privacy in Decentralized Min-Max Optimization: A Differentially Private Approach** [source](http://arxiv.org/pdf/2508.07505v1.pdf) #security 

 This work introduces DPMixSGD, a decentralized algorithm balancing strong differential privacy and high optimization accuracy, showing both theoretical convergence and practical robustness against privacy attacks.
 - The DPMixSGD algorithm achieves provable differential privacy in decentralized nonconvex min-max optimization without significantly sacrificing convergence rates or utility, maintaining a sample complexity of O(ùúÖ¬≥ùúñ‚Åª¬≥) comparable to the best non-private methods when the number of agents is moderate.
 - In empirical tests across logistic regression and deep learning benchmarks, DPMixSGD outperforms previous privacy-preserving baselines and demonstrates resilience to Deep Leakage from Gradients (DLG) attacks, effectively mitigating data reconstruction risks from shared gradients.
 - Even under varying network topology, noise intensity, and agent count, DPMixSGD maintains robust optimization performance and privacy guarantees, consistently achieving AUROC scores that match or exceed standard (non-private) decentralized min-max training in high-sparsity and low-agent-number settings.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **Optimization of Private Semantic Communication Performance: An Uncooperative Covert Communication Method** [source](http://arxiv.org/pdf/2508.07586v1.pdf) #security 

 This study introduces a cutting-edge reinforcement learning framework that significantly boosts privacy and communication quality in uncooperative covert semantic communication systems, making private semantic transmission both robust and practical.
 - A prioritized sampling assisted twin delayed deep deterministic policy gradient algorithm enhances covert semantic communication privacy by up to 77.8% and the quality of semantic information transmission by 14.3% compared to traditional reinforcement learning methods.
 - The proposed method enables private semantic transmission without coordination between the server and the friendly jammer, maintaining user privacy even in dynamic, multi-attacker, and variable wireless environments.
 - The approach leverages a novel graph-to-nearest-triple metric, allowing optimal decision-making on semantic triple selection and transmit power, thereby reducing the risk of meaningful data leakage during transmission.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **NetMoniAI: An Agentic AI Framework for Network Security & Monitoring** [source](http://arxiv.org/pdf/2508.10052v1.pdf) #security 

 NetMoniAI offers real-time, scalable, and interpretable network threat detection by fusing distributed micro-agents and centralized AI analysis with minimal latency.
 - NetMoniAI achieves sub-5-second detection latency for network anomalies, even under severely degraded network conditions, by leveraging asynchronous agent-based processing at each node.
 - The dual-layer architecture, combining autonomous node-level agents with centralized semantic analysis, enables accurate detection and classification of both localized threats and distributed attacks (e.g., DDoS) without introducing processing bottlenecks across deployments of up to 50 nodes.
 - Operator transparency and actionable insights are provided through structured, human-readable reports and interactive dashboards powered by LLMs, supporting real-time monitoring and clear system-wide threat visualization.

<br>

