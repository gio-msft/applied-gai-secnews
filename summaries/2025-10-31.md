ü´Ç **Fortytwo: Swarm Inference with Peer-Ranked Consensus** [source](http://arxiv.org/pdf/2510.24801v1.pdf) #security 

 Pairwise, peer-ranked consensus delivers superhuman accuracy and robust security for decentralized AI, showcasing resilience against adversarial attacks and democratizing high-quality inference.
 - Peer-ranked swarm inference with pairwise comparison and reputation weighting achieved an 85.9% accuracy rate on GPQA Diamond, outperforming majority voting by 17.21 percentage points and rivaling frontier AI models across benchmarks.
 - The protocol demonstrated exceptional adversarial robustness, with only a 0.12% accuracy drop under extraneous or prompt-injected noise, compared to an average 6.2% degradation for state-of-the-art single-model baselines.
 - A compute stake mechanism requiring demonstration of capability, not capital, effectively mitigates Sybil attacks and fosters an evolving meritocracy, enabling secure and open participation without sacrificing inference quality or scalability.

<br>

üéØ **QueryIPI: Query-agnostic Indirect Prompt Injection on Coding Agents** [source](http://arxiv.org/pdf/2510.23675v1.pdf) #security 

 A new prompt injection attack, QueryIPI, exploits coding agent vulnerabilities using internal prompt leaks, enabling stealthy, query-agnostic compromises with high real-world success rates.
 - An automated query-agnostic attack method, QueryIPI, consistently achieved up to 87% attack success against simulated coding agents with only 8 training queries, vastly surpassing traditional prompt injection rates.
 - Real-world testing showed QueryIPI could covertly compromise live coding agents with a 50% average success rate, more than 25 times higher than comparable baseline methods.
 - The attack leveraged agent internal prompt knowledge, enabling practical exploitation and undetectable payloads that bypassed state-of-the-art statistical detection systems such as PPL and Window PPL.

<br>

üîä **ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for
  Audio-Language Models** [source](http://arxiv.org/pdf/2510.26096v1.pdf) #security 

 A novel defense activates inherent safety pathways in audio-language models using lightweight acoustic cues, sharply reducing jailbreak vulnerability while maintaining model usability.
 - A targeted acoustic perturbation strategy reduces the average success rate of audio-language model-specific jailbreak attacks from 41.6% to just 14.6% across four advanced models.
 - Defensive acoustic signals applied selectively to sensitive Mel-frequency bins enable strong generalization to both seen and unseen attacks, achieving an average success rate as low as 1.9% on state-of-the-art adversarial threats.
 - The proposed safeguard preserves benign task performance, introducing only minimal degradation (e.g., less than 2% increase in word error rate), and improves or matches utility compared to existing state-of-the-art defenses.

<br>

üõ°Ô∏è **Jailbreak Mimicry: Automated Discovery of Narrative-Based Jailbreaks for
  Large Language Models** [source](http://arxiv.org/pdf/2510.22085v1.pdf) #security 

 Automated narrative reframing exposes systematic weaknesses in AI safety, enabling scalable jailbreaks especially in technical domains and revealing the need for context-driven defenses.
 - Automated narrative-based jailbreak generation achieved an 81.0% attack success rate, representing a 54√ó improvement over direct prompting and a significant escalation in vulnerability discovery for large language models.
 - Technical domains, including cybersecurity (93.1% success rate) and fraud (87.9%), are particularly susceptible to contextual reframing attacks, while models like Gemini 2.5 Flash demonstrate notably enhanced resistance (overall 33.0% ASR) compared to GPT-4 and Llama 3.
 - Current safety alignment in language models is reliably bypassed by sophisticated narrative reframings that shift model objectives from compliance to task completion, highlighting the urgent need for context-aware and multi-layered defense mechanisms.

<br>

üß® **Adjacent Words, Divergent Intents: Jailbreaking Large Language Models
  via Task Concurrency** [source](http://arxiv.org/pdf/2510.21189v1.pdf) #security 

 Task concurrency exposes a severe vulnerability: combining harmful and benign intents at the word level dramatically increases jailbreak success and guardrail evasion in large language models.
 - Interleaving harmful and benign tasks at the word level enables large language models to process concurrent prompts with high utility, consistently achieving comparable performance to sequential processing across various benchmarks.
 - When harmful content is embedded within a concurrent task alongside benign content, guardrail-based moderation systems are up to 50% less likely to detect and filter harmful responses, significantly reducing the effectiveness of existing safeguards.
 - The proposed JAIL-CON attack framework achieves an average attack success rate of 0.95 and outperforms prior jailbreak methods by up to 2.4x in bypassing both standard and advanced guardrails across six leading LLMs.

<br>

üï≥Ô∏è **The Trojan Example: Jailbreaking LLMs through Template Filling and
  Unsafety Reasoning** [source](http://arxiv.org/pdf/2510.21190v1.pdf) #security 

 Reframing unsafe requests as template-filling tasks and using unsafety reasoning enables highly effective, efficient, and transferable jailbreaking of state-of-the-art language models.
 - Template-filling jailbreak prompts with unsafety reasoning achieve a 97‚Äì100% attack success rate against leading LLM APIs like GPT-4o, Gemini, and DeepSeek, outperforming previous black-box methods by over 130%.
 - Obfuscating harmful instruction terms via placeholders and lightweight encryption (e.g., Caesar cipher, Base64) drastically lowers model refusal rates and facilitates high cross-model transferability of jailbreak prompts.
 - The approach enables fast convergence‚Äîmost models are reliably jailbroken using five or fewer prompt iterations, keeping practical costs extremely low (e.g., under $0.02 per attempt on commercial APIs).

<br>

üß† **Uncovering the Persuasive Fingerprint of LLMs in Jailbreaking Attacks** [source](http://arxiv.org/pdf/2510.21983v1.pdf) #security 

 Harnessing social persuasion strategies enables stealthy, effective jailbreaks of large language models, revealing model-specific vulnerabilities and the need for psychologically robust safeguards.
 - Persuasive prompts based on human influence principles increased jailbreak attack success on large language models by 56% to 97% compared to original harmful queries.
 - Different language models display unique susceptibility profiles, with the Scarcity and Social Proof principles generally the most effective and Reciprocity usually the least.
 - Persuasion-aware jailbreak prompts maintain high attack success rates while being more human-readable and less detectable by perplexity-based defenses than many current adversarial prompt strategies.

<br>

üõ°Ô∏è **Sentra-Guard: A Multilingual Human-AI Framework for Real-Time Defense
  Against Adversarial LLM Jailbreaks** [source](http://arxiv.org/pdf/2510.22628v1.pdf) #security 

 A hybrid, multilingual LLM defense system delivers near-perfect real-time jailbreak detection with rapid adaptation and broad language support, setting a new benchmark for prompt security.
 - Sentra-Guard achieves a 99.996% detection rate and an attack success rate of only 0.004%, surpassing major baseline defenses against LLM jailbreaks across 24,145 adversarial prompts.
 - Multilingual capability enables robust detection and normalization of adversarial prompts in over 100 languages, consistently yielding detection rates above 96% and false positive rates below 2.1% across diverse LLMs and linguistic scenarios.
 - Real-time HITL feedback reduces adaptation lag by over 90%, incrementally improving recall by 4.2% and lowering false alarms by 11% without retraining, supporting scalable, low-latency deployment at 47 ms per inference.

<br>

üîì **RECAP: Reproducing Copyrighted Data from LLMs Training with an Agentic
  Pipeline** [source](http://arxiv.org/pdf/2510.25941v1.pdf) #security 

 RECAP unlocks strong evidence of LLM memorization‚Äîincluding copyrighted material‚Äîusing a feedback-driven pipeline, achieving far higher extraction accuracy and overcoming model safeguards.
 - RECAP achieves an average 78% improvement in extracting verbatim copyrighted content from large language models compared to prior extraction methods, as measured by ROUGE-L scores.
 - The iterative feedback loop and jailbreaking module in RECAP reliably overcome alignment-based refusals, raising extraction success rates by over 42% beyond single-step prompting approaches.
 - RECAP demonstrates negligible contamination or false positives when tested on non-training data, confirming its robustness and reliability for identifying actual memorized content.

<br>

üõ°Ô∏è **LLM-Powered Detection of Price Manipulation in DeFi** [source](http://arxiv.org/pdf/2510.21272v1.pdf) #security 

 A new LLM-guided static analysis pipeline detects DeFi price manipulation vulnerabilities with state-of-the-art accuracy, low cost, and unprecedented speed, filling critical security gaps in smart contract auditing.
 - Price manipulation attacks account for 17.3% of major DeFi exploits and have caused losses exceeding $165.8 million, with single incidents such as BonqDAO resulting in $88 million in damages.
 - The hybrid framework, PMDetector, achieves up to 88% precision and 90% recall using LLM-powered reasoning, outperforming both traditional static analysis and existing LLM-based approaches for detecting price vulnerabilities in smart contracts.
 - Auditing a DeFi contract for price manipulation vulnerabilities using PMDetector costs as little as $0.03 and is completed in about 4 seconds, providing a vastly more efficient and scalable alternative to manual audits, which typically range from $5,000‚Äì$15,000.

<br>

üõ°Ô∏è **Secure Retrieval-Augmented Generation against Poisoning Attacks** [source](http://arxiv.org/pdf/2510.25025v1.pdf) #security 

 RAGuard sets a new standard for defending retrieval-augmented generation systems against knowledge poisoning, combining high accuracy, resilience to advanced attacks, and low computational cost.
 - RAGuard achieves detection accuracy rates above 92% and nearly 100% output accuracy across multiple large datasets and poisoning attack types, outperforming baseline and advanced defense methods by substantial margins.
 - The framework maintains low false positive rates (mostly under 6%) and false negative rates, ensuring effective discrimination between benign and poisoned texts with minimal impact on legitimate knowledge retrieval and model responses.
 - RAGuard remains highly robust even against strong adaptive attacks designed to evade its detection, and introduces negligible computational overhead compared to other methods, making it practical for real-world deployments in retrieval-augmented generation systems.

<br>

üõ°Ô∏è **FLAMES: Fine-tuning LLMs to Synthesize Invariants for Smart Contract
  Security** [source](http://arxiv.org/pdf/2510.21401v1.pdf) #security 

 Domain-adapted LLMs can autonomously harden smart contracts with executable runtime guards, achieving high compilability, semantic accuracy, and real exploit mitigation without manual intervention.
 - 96.7% of smart contracts hardened with domain-adapted LLM-synthesized invariants are compilable, greatly surpassing the 62.6% rate of standard code models, demonstrating robust deployability.
 - On challenging real-world smart contract invariants, 44.5% of LLM-generated security guards are exactly or semantically equivalent to human-written defenses, doubling the accuracy compared to baselines.
 - LLM-generated invariants autonomously prevent 20.4% of high-impact exploits in tested smart contracts while preserving intended functionality and have successfully reverted a major real-world attack incident.

<br>

üõ°Ô∏è **Securing AI Agent Execution** [source](http://arxiv.org/pdf/2510.21236v2.pdf) #security 

 A zero-modification, containerized access control framework efficiently hardens AI agent servers against systemic attacks by automatically generating enforceable, least-privilege policies with near-perfect coverage.
 - Automated access control manifests for AI agent servers can be accurately generated from source code with 80.9% accuracy and 100% recall, ensuring nearly all necessary permissions are identified with minimal manual revision.
 - Policy enforcement through containerization restricts MCP servers to declared permissions, effectively blocking all environment-targeted attacks‚Äîsuch as data exfiltration and unauthorized resource access‚Äîwithout requiring changes to existing agent workflows.
 - Enforcing least-privilege isolation with the proposed system incurs negligible performance overhead, adding on average less than 1 ms per operation and only a few hundred milliseconds to server startup, making it practical for real-world deployments.

<br>

üõ°Ô∏è **SIRAJ: Diverse and Efficient Red-Teaming for LLM Agents via Distilled
  Structured Reasoning** [source](http://arxiv.org/pdf/2510.26037v1.pdf) #security 

 SIRAJ introduces dynamic, diverse, and cost-efficient red-teaming for LLM agents via structured reasoning, achieving state-of-the-art attack coverage and efficiency‚Äîeven with smaller models.
 - By conditioning on previously generated test cases, the SIRAJ framework achieves a 2‚Äì2.5x increase in coverage for risk outcomes and tool-calling trajectory diversity when red-teaming LLM agents.
 - Structured reasoning distillation enables an 8B red-teaming model to surpass a 671B baseline, with 100% improvement in attack success rate while using significantly fewer computational resources.
 - Iterative adversarial refinement leveraging agent execution feedback increases attack success rates by up to 10% and enables effective generalization to agents with unseen toolsets or risk types.

<br>

üïµÔ∏è **SBASH: a Framework for Designing and Evaluating RAG vs. Prompt-Tuned LLM
  Honeypots** [source](http://arxiv.org/pdf/2510.21459v1.pdf) #security 

 SBASH shows that RAG improves the accuracy of untuned local LLM honeypots, but system prompt tuning offers a faster and comparably accurate alternative with lower costs.
 - Retrieval Augmented Generation (RAG) significantly boosts response accuracy in untuned lightweight local LLM honeypots, with improvements up to 19% compared to non-RAG approaches as measured by similarity metrics.
 - System prompt tuning enables non-RAG LLM honeypots to reach comparable or superior accuracy to RAG-enhanced untuned models, while reducing response latency and operational cost.
 - Human evaluators rated the realism of the SBASH RAG-enhanced honeypot at an average of 4 out of 5, indicating good attacker engagement potential despite lag and limited shell interactivity.

<br>

‚öñÔ∏è **Law in Silico: Simulating Legal Society with LLM-Based Agents** [source](http://arxiv.org/pdf/2510.24442v1.pdf) #general 

 This work demonstrates that LLM-driven agent simulations can realistically model both societal crime patterns and the evolution of legal systems, offering actionable insights into how law design, enforcement, and accessibility shape societal welfare.
 - Simulated macro-level crime rates using LLM-based agents closely match real-world statistics across diverse countries, with discrepancies in developing countries likely reflecting underreporting in official records rather than simulation error.
 - Introducing harsher perceived legal punishments in the simulation consistently led to lower crime rates, indicating that agent decision-making within the framework is highly sensitive to changes in legal deterrence.
 - Micro-level experiments demonstrated that a transparent, efficiently enforced, and corruption-free legal system boosts welfare and rights protection for vulnerable individuals, whereas legal corruption or high litigation costs result in increased exploitation and lower well-being.

<br>

üêû **Beyond Imprecise Distance Metrics: LLM-Predicted Target Call Stacks for
  Directed Greybox Fuzzing** [source](http://arxiv.org/pdf/2510.23101v1.pdf) #security 

 Predicting vulnerability-triggering call stacks with LLMs revolutionizes directed fuzzing, unlocking far superior speed, accuracy, and bug-finding potential compared to traditional static analysis methods.
 - Compared to state-of-the-art fuzzers, LLM-guided call stack prioritization triggers vulnerabilities between 1.86√ó and 3.09√ó faster, with optimal performance on nearly half of tested cases.
 - The approach discovered 10 new vulnerabilities and 2 incomplete fixes in widely used software (GNU Linker/Binutils), with all assigned CVE IDs, demonstrating strong real-world impact.
 - Replacing imprecise static analysis distance metrics with LLM-predicted call stacks dramatically increases the efficiency and stability of directed greybox fuzzing, stably reproducing more vulnerabilities than all baselines.

<br>

üîç **Uncovering Gaps Between RFC Updates and TCP/IP Implementations:
  LLM-Facilitated Differential Checks on Intermediate Representations** [source](http://arxiv.org/pdf/2510.24408v1.pdf) #security 

 Using LLM-driven differential and knowledge graph analysis, this work automates the scalable detection of RFC-to-code inconsistencies in TCP/IP stacks, uncovering vulnerabilities previously missed by manual review.
 - Automated analysis leveraging large language models and differential checks exposed 15 substantial inconsistencies between TCP/IP protocol implementations and RFC specifications across 7 major OS versions, with vulnerabilities including risk areas such as TCP sequence number prediction, replay attacks, and TCP RST spoofing.
 - The proposed framework achieved high detection precision‚Äîup to 91.1% accuracy and 0.857 F1 score‚Äîwhile simultaneously reducing computational and manual verification costs compared to standard and vanilla LLM-based or full-coverage code review approaches.
 - Critical gaps‚Äîsuch as failure to periodically reseed secret keys in ISN generation and incomplete implementation of TCP authentication‚Äîpersist even in modern kernel versions, underscoring that protocol drift remains a real-world security concern despite frequent RFC updates.

<br>

üõ°Ô∏è **Network Intrusion Detection: Evolution from Conventional Approaches to
  LLM Collaboration and Emerging Risks** [source](http://arxiv.org/pdf/2510.23313v1.pdf) #security 

 LLMs have transformed both the defensive and offensive landscape of network intrusion detection, bringing new detection capabilities‚Äîand risks‚Äîthat outpace traditional AI and rule-based systems.
 - Large Language Models (LLMs) have significantly enhanced network intrusion detection systems by enabling adaptive threat detection, deeper behavioral analysis, and more effective classification of both known and novel attacks compared to traditional methods.
 - Despite their advantages, LLM-based intrusion detection faces notable challenges such as high computational costs, data representation mismatches with network traffic, vulnerabilities to adversarial attacks, and difficulties in explainability and real-time deployment.
 - LLMs are now exploited offensively to automate penetration testing, generate sophisticated attack traffic, and craft evasive malware and phishing campaigns, which raises new security concerns and necessitates hybrid detection strategies and continuous adaptation.

<br>

üõ†Ô∏è **TDFlow: Agentic Workflows for Test Driven Software Engineering** [source](http://arxiv.org/pdf/2510.23761v1.pdf) #general 

 TDFlow demonstrates that modular, test-driven agentic LLM workflows can surpass existing automated software repair systems and reach human-level performance, provided high-quality tests are available.
 - TDFlow achieved an 88.8% pass rate on SWE-Bench Lite with human-written tests, outperforming the next best system by 27.8%.
 - Human-level performance in test resolution was demonstrated with a 94.3% success rate on SWE-Bench Verified, indicating large language models (LLMs) are already highly effective at test-driven development when provided with high-quality tests.
 - Test hacking was extremely rare, with only 7 instances out of 800 runs, and the primary limitation to full autonomy lies in generating accurate and meaningful reproduction tests rather than issue-solving or debugging.

<br>

‚öñÔ∏è **A Neuro-Symbolic Multi-Agent Approach to Legal-Cybersecurity Knowledge
  Integration** [source](http://arxiv.org/pdf/2510.23443v1.pdf) #cyber 

 Connecting legal mandates to technical cybersecurity knowledge with transparent, explainable AI agents shows promising accuracy‚Äîbut wide coverage and multilingual robustness need further development.
 - A hybrid neuro-symbolic multi-agent framework effectively links legal obligations to technical cybersecurity documentation, achieving 84% classification accuracy on EU legal texts across 10 languages.
 - The retrieval pipeline demonstrates reliable top-ranked precision (MRR 0.552, P@1 0.494), but broader coverage is limited, highlighting the need for improved recall and generalization in cross-domain tasks.
 - Cross-lingual and domain adaptation challenges persist, with strong performance on English legal-cyber queries but moderate results on non-English inputs and hybrid datasets, indicating further research is needed for robust multilingual integration.

<br>

üõí **Magentic Marketplace: An Open-Source Environment for Studying Agentic
  Markets** [source](http://arxiv.org/pdf/2510.25779v1.pdf) #general 

 This paper unveils how open-source agentic marketplaces expose severe behavioral biases and market vulnerabilities that reshape competition, efficiency, and the design of autonomous AI-driven economies.
 - Agentic marketplaces powered by large language models achieve near-optimal consumer welfare under ideal search conditions, with frontier models coming within 93-99% of the theoretical best outcomes.
 - All tested models display a pronounced first-proposal bias, with businesses responding earliest receiving up to 30 times the selection rate compared to later responders, shifting competition from quality to speed.
 - Increasing the number of available search options paradoxically diminishes welfare, with consumer satisfaction declining by up to 65% as consideration sets grow, due to limited agent exploration and cognitive overload.

<br>

üõ°Ô∏è **Who Grants the Agent Power? Defending Against Instruction Injection via
  Task-Centric Access Control** [source](http://arxiv.org/pdf/2510.26212v1.pdf) #security 

 Dynamically-scoped, task-based permissions protect AI agents from instruction injection attacks embedded in seemingly benign user content.
 - Task-centric access control significantly reduces instruction injection risks in autonomous AI agents by granting permissions only for the specific, user-authorized task and revoking them immediately upon completion.
 - Experimental results show that the proposed framework successfully blocks unauthorized email forwarding‚Äîeven when prompted by malicious content‚Äîwhile permitting completion of legitimate registration tasks.
 - Traditional static permission models create over-privileged environments, highlighting the urgent need to shift toward dynamic, intent-driven security paradigms for AI-powered mobile automation.

<br>

üõ°Ô∏è **MCPGuard : Automatically Detecting Vulnerabilities in MCP Servers** [source](http://arxiv.org/pdf/2510.23673v1.pdf) #security 

 Security flaws in MCP servers dramatically expand AI system attack surfaces, but automated, layered scanning tools offer actionable and effective protections against both code-level exploits and semantic prompt-based hijacking.
 - Over 68% of examined MCP servers were vulnerable to at least one high-impact security flaw, such as command injection or prompt poisoning, potentially exposing sensitive user data and enabling remote code execution.
 - The absence of trusted registries and weak isolation in MCP-enabled marketplaces allows attackers to hijack agent workflows through supply chain attacks, leading to credential theft and unauthorized tool manipulation in real-world deployments.
 - Automatic vulnerability detection tools, such as MCPGuard, combining static and neural analysis, achieve a 91% detection rate for novel MCP-specific threats in pre-deployment audits, enabling proactive mitigation of emerging attack vectors.

<br>

üö• **Scalable Supervising Software Agents with Patch Reasoner** [source](http://arxiv.org/pdf/2510.22775v1.pdf) #general 

 Model-driven reasoning verification unlocks scalable and practical supervision for coding agents, outperforming test-based methods in both accuracy and efficiency.
 - A reasoning-based patch verifier enables efficient, test-free supervision of software engineering agents, achieving 72.2% verification accuracy‚Äîsurpassing advanced proprietary models like OpenAI o3.
 - The use of group-wise patch evaluation improves verification consistency, delivers dense and stable rewards for reinforcement learning, and mitigates reward hacking risks common to binary outcome models.
 - Mini-SE, an agent trained solely with reasoning-based rewards, attained a Pass@1 rate of 26.2% (a 10% boost over baseline) and reached 32.8% through scalable patch selection, while verifying patches 50 times faster than conventional sandbox testing.

<br>

