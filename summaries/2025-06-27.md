üõ°Ô∏è **Enhancing Security in LLM Applications: A Performance Evaluation of
  Early Detection Systems** [source](http://arxiv.org/pdf/2506.19109v1.pdf) #security 

 Early prompt leak detection systems show limitations; optimizing detection policies and enhancing canary word implementation are critical for robust LLM application security.
 - Transformer-based scanners detected over 99% of prompt leak attacks but had notable false positive rates, while Rebuff's hybrid approach achieved a high balance of recall (98.1%) and low false positives (3.4%).
 - Both Vigil and Rebuff‚Äôs default canary word checks failed to detect prompt leaks unless enhanced with precise instructions, dramatically improving detection rates when modified.
 - Even advanced LLMs such as GPT-4o and Claude-3-5-sonnet remain vulnerable to sophisticated prompt leak attacks, showing attack success rates of up to 19% for certain combinations, highlighting that LLM-hardening alone is insufficient.

<br>

üõ°Ô∏è **Semantic-Aware Parsing for Security Logs** [source](http://arxiv.org/pdf/2506.17512v1.pdf) #security 

 Automated log parsers with semantic understanding, powered by LLMs, facilitate accurate, efficient security analytics and outperform previous AI and manual methods in both accuracy and query capability.
 - Queries over logs parsed with Matryoshka achieve an average precision of 0.96 and recall of 0.95, significantly outperforming naive substring matching and prior AI-based log parsing frameworks.
 - Matryoshka's syntactic parser, leveraging regular expressions and semantic clustering, provides a parser group similarity of 0.97 and template similarity of 0.91 on large benchmarks, surpassing the best previous methods.
 - Automated semantic field naming and mapping to standardized security schemas (OCSF) is feasible, although mapping accuracy varies by log complexity, highlighting the need for further improvement in schema alignment.

<br>

üö® **Multi-turn Jailbreaking via Global Refinement and Active Fabrication** [source](http://arxiv.org/pdf/2506.17881v1.pdf) #security 

 A sophisticated multi-turn attack strategy can reliably and efficiently circumvent safety protections in leading language models by disguising harm through dialogue refinement.
 - The proposed multi-turn jailbreaking technique achieved up to 95% attack success rate (ASR) on state-of-the-art large language models, significantly surpassing both single-turn and prior multi-turn baselines.
 - This method effectively bypassed existing AI safety defenses, with safety interventions like OpenAI‚Äôs moderation system only reducing attack success by 20‚Äì26%.
 - Analysis showed that the approach shifts the internal representation of harmful queries closer to harmless ones, making detection by current filters more challenging and highlighting a key vulnerability.

<br>

üö® **Security Assessment of DeepSeek and GPT Series Models against Jailbreak
  Attacks** [source](http://arxiv.org/pdf/2506.18543v1.pdf) #security 

 Open-source DeepSeek models offer selective resistance to certain attacks but are overall less robust than GPT-4 Turbo, showing heightened vulnerability to prompt-based and human-engineered jailbreak methods.
 - DeepSeek models, built with a Mixture-of-Experts (MoE) architecture, are significantly more susceptible than GPT-4 to prompt-based and human-engineered jailbreak attacks, with attack success rates (ASR) frequently exceeding 40% for common strategies.
 - GPT-4 Turbo consistently demonstrates superior and more stable safety alignment across diverse harmful behavior categories, achieving some of the lowest ASRs (as low as 2.6% in human-jailbreak scenarios) among all tested models.
 - While DeepSeek's MoE sparsity offers localized robustness to optimization-based attacks (e.g., TAP-T success rates under 1%), it leads to inconsistent refusal behavior and leaves high vulnerabilities in domains such as misinformation, cybercrime, and contextual harms.

<br>

ü¶† **MIST: Jailbreaking Black-box Large Language Models via Iterative
  Semantic Tuning** [source](http://arxiv.org/pdf/2506.16792v1.pdf) #security 

 Iterative semantic prompt tuning can reliably and efficiently jailbreak state-of-the-art large language models in black-box settings, highlighting persistent vulnerabilities even in restricted-access scenarios.
 - Iterative Semantic Tuning via the MIST framework enables black-box jailbreaks of large language models with success rates up to 86% (GPT-4-turbo) and 88% (Vicuna), outperforming or matching prior state-of-the-art attacks across both open- and closed-source models.
 - MIST's order-determining optimization (MIST-ODO) reduces average successful jailbreak query counts to as low as 18.9 (Vicuna) and 24.8 (GPT-4-turbo), demonstrating markedly higher efficiency than previous black-box methods.
 - Jailbreak prompts crafted with MIST-ODO exhibit notably higher transferability (e.g., 44.19% success rate transferring from GPT-4-turbo to GPT-4o) and greater robustness against baseline defenses, though advanced safeguards like backtranslation can still mitigate attack success to some extent.

<br>

üîç **PrivacyXray: Detecting Privacy Breaches in LLMs through Semantic
  Consistency and Probability Certainty** [source](http://arxiv.org/pdf/2506.19563v1.pdf) #security 

 Internal semantic coherence and probability certainty metrics enable highly accurate, robust, and generalizable inference-time privacy breach detection in large language models, surpassing previous approaches without needing access to real private validation data.
 - Detection of privacy breaches in large language models can achieve an average accuracy of 92.69%, with a 20.06% improvement over current state-of-the-art methods, by leveraging internal model states for semantic consistency and probability certainty.
 - Correct private outputs from language models show higher intra-layer and inter-layer semantic similarity, as well as greater probabilistic certainty, distinguishing them from incorrect or non-memorized responses in both white-box and black-box evaluation scenarios.
 - The proposed detection framework demonstrates strong generalization and transferability, maintaining high accuracy even on unseen data types, different model architectures, low-data regimes, and real-world privacy benchmarks such as the DecodingTrust dataset.

<br>

üß© **From Concepts to Components: Concept-Agnostic Attention Module Discovery
  in Transformers** [source](http://arxiv.org/pdf/2506.17052v1.pdf) #general 

 A tiny, easily controlled subset of attention heads drives complex behaviors in large transformers, revealing new levers for precise model control and interpretability across language and vision.
 - A sparse set of only 3‚Äì10 attention heads in transformer models are disproportionately responsible for encoding specific, user-defined concepts, regardless of the model or concept complexity.
 - Scalar manipulation of these identified 'concept modules' can dramatically affect model behavior, including a 72.7% increase in LLM jailbreak success rates by suppressing safety, and a 1.6% improvement in mathematical reasoning performance by amplifying reasoning modules.
 - This concept-agnostic pipeline generalizes to both language and vision transformers, enabling selective deletion of a target class (e.g., reducing image recognition accuracy for a specific label to 0%) by intervening in just 0.1% of model weights.

<br>

üß¨ **Command-V: Pasting LLM Behaviors via Activation Profiles** [source](http://arxiv.org/pdf/2506.19140v1.pdf) #general 

 Efficient activation-profile transfer enables rapid, data-free editing of LLM behaviors‚Äîboth beneficial and risky‚Äîwith minimal compute.
 - Transferring behaviors between large language models via activation profile converters achieves comparable or superior task performance to direct finetuning, while reducing compute cost by orders of magnitude and requiring no further access to training data.
 - Ported adapters enable impressive cross-model transferability of critical behaviors‚Äîincluding safety refusal, jailbreaking, and chain-of-thought reasoning‚Äîwith attack success rates on harmful prompts increasing from low single digits to as high as 80% in some target models after transfer.
 - The activation-profile-based method allows rapid, backpropagation-free behavior editing that executes on CPUs in seconds, making advanced model editing accessible on resource-limited devices, but with noted risk of occasional output degradation or overrefusal depending on task and model pairing.

<br>

üõ°Ô∏è **KnowML: Improving Generalization of ML-NIDS with Attack Knowledge Graphs** [source](http://arxiv.org/pdf/2506.19802v1.pdf) #security 

 Embedding systematically extracted attack knowledge into ML-NIDS fundamentally overcomes prior generalization failures, enabling robust and accurate detection of both known and evolving attack variants.
 - Knowledge-guided input via attack knowledge graphs enables machine learning-based network intrusion detection systems (ML-NIDS) to achieve up to 99% F1-Score and maintain a false positive rate below 0.1% when detecting diverse and realistic attack variants, whereas baseline systems often fail completely (F1-Score as low as 0%).
 - Traditional ML-NIDS relying on limited, designer-driven assumptions exhibit severe generalization failures‚Äîmissing distributed, non-throughput, and out-of-dimension attacks that deviate from predefined patterns, with several baselines achieving 0% detection on these variants.
 - Automated extraction and integration of attack strategies from over 7,000 open-source repositories using large language models (LLMs) vastly reduces manual labor and enables the construction of comprehensive knowledge graphs‚Äîimproving detection coverage at a marginal compute cost (completing in 26 hours what would otherwise take 157 workdays manually).

<br>

üß† **Large Language Model Unlearning for Source Code** [source](http://arxiv.org/pdf/2506.17125v1.pdf) #security 

 A new probabilistic redistribution framework allows LLMs to forget problematic code without sacrificing core code generation abilities, overcoming previous methods' severe utility drop.
 - Existing large language model unlearning methods significantly degrade code generation utility, with model performance reductions exceeding 60% even as they succeed in forgetting targeted code, rendering LLMs practically unusable for software engineering tasks.
 - The newly proposed PROD approach achieves up to a 124% average improvement in balancing forget quality and model utility across critical code unlearning tasks compared to the best baseline, with robust performance demonstrated across diverse LLM series and adversarial attack scenarios.
 - PROD enables targeted forgetting of copyrighted, insecure, and deprecated code data while maintaining high perceptual quality and user experience, consistently winning over 70% of human and AI evaluation comparisons against existing unlearning methods.

<br>

üõ°Ô∏è **SV-LLM: An Agentic Approach for SoC Security Verification using Large
  Language Models** [source](http://arxiv.org/pdf/2506.20415v1.pdf) #security 

 SV-LLM leverages a collaborative multi-agent LLM approach to automate and greatly enhance accuracy and reliability in end-to-end hardware security verification.
 - A specialized multi-agent LLM framework for SoC security verification achieved 84.8% bug detection accuracy, outperforming non-fine-tuned models by over 42 percentage points, and approaching closed-source industry models.
 - The agentic pipeline enabled automated generation and validation of security testbenches, raising bug validation rates up to 89%, a significant improvement over baseline LLM prompting methods that scored as low as 18%.
 - The system autonomously covers all critical security verification dimensions‚Äîincluding asset identification, threat modeling, property generation, vulnerability detection, and testbench synthesis‚Äîoutperforming both traditional and existing LLM-based approaches in breadth and automation.

<br>

üõ°Ô∏è **SAVANT: Vulnerability Detection in Application Dependencies through
  Semantic-Guided Reachability Analysis** [source](http://arxiv.org/pdf/2506.17798v1.pdf) #security 

 A novel LLM-powered system dramatically increases precision and recall of real-world dependency vulnerability detection in complex Java codebases, outperforming traditional tools and uncovering actionable software supply chain risks.
 - Over 80% of Java applications inherit at least one known vulnerability from third-party libraries, with supply-chain attacks increasing by 1300% since 2020.
 - The SAVANT system achieves 83.8% precision, 73.8% recall, and a 78.5% F1-score for detecting API-level vulnerabilities in real-world Java projects, outperforming existing software composition analysis tools by up to 182% in recall.
 - Optimal vulnerability detection occurs when code is segmented into blocks of 2,000‚Äì2,500 tokens, with large language models such as GPT-4O and Gemini Flash delivering F1-scores up to 0.87 when paired with OpenAI embeddings.

<br>

ü¶ô **Smart-LLaMA-DPO: Reinforced Large Language Model for Explainable Smart
  Contract Vulnerability Detection** [source](http://arxiv.org/pdf/2506.18245v1.pdf) #security 

 A preference-optimized large language model significantly advances smart contract vulnerability detection and delivers expert-level, explainable results across both standard and hard-to-audit cases.
 - Smart-LLaMA-DPO achieved an average improvement of 10.43% in F1 score and 7.87% in accuracy over state-of-the-art baselines for detecting key smart contract vulnerabilities, including reentrancy, timestamp dependency, integer overflow/underflow, and delegatecall.
 - For machine-unauditable vulnerabilities‚Äîissues traditionally difficult to detect automatically‚ÄîSmart-LLaMA-DPO attained an 83.4% F1 score and 90.7% accuracy, outperforming the best prior method (iAudit) by 25.98% in F1 and 18.25% in accuracy.
 - Explanations produced by Smart-LLaMA-DPO received high ratings in both automated and human evaluations, with over 80% of responses rated positively (scores of 3 or 4 out of 4) for correctness, thoroughness, and clarity, indicating superior explainability compared to existing methods.

<br>

üîí **FORGE: An LLM-driven Framework for Large-Scale Smart Contract
  Vulnerability Dataset Construction** [source](http://arxiv.org/pdf/2506.18795v1.pdf) #security 

 This work demonstrates fully automated, high-precision construction of large-scale smart contract vulnerability datasets, exposes major gaps in current security tooling, and highlights the critical need to realign research with real-world risk profiles.
 - An LLM-driven framework successfully built a comprehensive smart contract vulnerability dataset from 6,454 audit reports, resulting in 81,390 Solidity files and 27,497 vulnerability findings across 296 CWE categories‚Äîfar outscaling previous efforts.
 - The automated extraction and classification achieved high accuracy, with an overall precision of 95.6%, a Macro-F1 score of 86.1%, and classification consistency with human experts marked by a Krippendorff's alpha of 0.87.
 - Benchmarking 13 existing smart contract security tools on the new dataset revealed significant detection limitations, with the highest F1 score at only 18.59%, and also uncovered a stark misalignment between academic research focus and the most severe real-world vulnerabilities.

<br>

üì° **Generative AI for Vulnerability Detection in 6G Wireless Networks:
  Advances, Case Study, and Future Directions** [source](http://arxiv.org/pdf/2506.20488v1.pdf) #security 

 Generative AI is poised to transform vulnerability detection for 6G wireless networks through adaptive and context-aware analysis, but faces challenges in scalability, data authenticity, and robust, privacy-preserving implementation.
 - Generative AI models‚Äîincluding VAEs, GANs, LLMs, and diffusion models‚Äîenable adaptive, multimodal, and context-aware detection of vulnerabilities across software, protocol, cloud, and hardware layers in 6G wireless networks, overcoming the limitations of traditional signature- and rule-based methods.
 - In benchmark experiments, fine-tuned domain-specific large language models outperformed both traditional and general-purpose counterparts for code vulnerability detection (e.g., achieving up to 65.98 F1 score on Devign), but suffered performance drops on highly imbalanced real-world datasets, highlighting the challenge of data scarcity and bias.
 - To ensure practical deployment and trustworthy results, future directions must focus on developing lightweight models, generating high-authenticity content, ensuring adversarial robustness, integrating external threat intelligence, and preserving privacy in generative-AI-powered wireless network security frameworks.

<br>

ü™≤ **Tracing Errors, Constructing Fixes: Repository-Level Memory Error Repair
  via Typestate-Guided Context Retrieval** [source](http://arxiv.org/pdf/2506.18394v1.pdf) #general 

 Typestate-guided context retrieval enables LLMs to repair complex memory errors in C programs at scale, vastly outperforming both traditional and LLM-based baselines in accuracy, reliability, and efficiency.
 - LTFix, a typestate-guided and context retrieval-augmented LLM framework, successfully repaired 37 out of 49 real-world memory errors in large open-source C codebases, achieving 14.5 times more correct repairs than SAVER and 2.36 times more than ProveNFix, with no new errors introduced.
 - Compared to the leading LLM-based tool SWE-agent, LTFix repaired 94% more memory errors while using 41 times fewer tokens, demonstrating both superior accuracy and efficiency in repository-level automated memory error repair.
 - Selective typestate-guided context retrieval was shown to be key to LTFix's effectiveness, boosting correct patch generation by up to 214% and reducing harmful patches by over 80% relative to LLM approaches using file-level or function-level context alone.

<br>

üõ°Ô∏è **FuncVul: An Effective Function Level Vulnerability Detection Model using
  LLM and Code Chunk** [source](http://arxiv.org/pdf/2506.19453v1.pdf) #security 

 Pinpointing vulnerabilities using small code chunks rather than entire functions yields vastly superior detection rates and enables fine-grained remediation.
 - FuncVul achieves state-of-the-art function-level vulnerability detection accuracy ranging from 87% to 92%, outperforming existing deep learning and transformer-based models across six benchmark datasets.
 - The code chunk approach, particularly using a 3-line extension around code changes, delivers a dramatic 53.9% improvement in accuracy and 42% increase in F1-score over analyzing entire functions, enabling more precise localization of vulnerabilities.
 - FuncVul demonstrates strong generalization, correctly detecting 81.95% of previously unseen CVEs and new project vulnerabilities, and can reliably identify multiple vulnerabilities within a single function.

<br>

üõ°Ô∏è **Leaner Training, Lower Leakage: Revisiting Memorization in LLM
  Fine-Tuning with LoRA** [source](http://arxiv.org/pdf/2506.20856v1.pdf) #security 

 LoRA fine-tuning enables competitive LLM adaptation with minimal privacy risk, sharply reducing memorization compared to full fine-tuning while remaining robust across model sizes, data duplication, and tuning parameters.
 - LoRA fine-tuning achieves near-zero plagiarism-based memorization, maintaining similarity scores consistently below extraction thresholds, even with increasing model size and data duplication, while preserving strong task performance.
 - Full fine-tuning steadily increases direct memorization and data extractability as both model size and data redundancy rise, whereas LoRA and head-only fine-tuning remain largely unaffected under the same conditions.
 - Moderate LoRA hyperparameters, such as balanced rank and scaling factor with moderate dropout, further mitigate memorization risks without compromising utility, with no observed extractable memorization even in high-memorization tasks like dialogue.

<br>

üß© **JsDeObsBench: Measuring and Benchmarking LLMs for JavaScript
  Deobfuscation** [source](http://arxiv.org/pdf/2506.20170v1.pdf) #security 

 LLMs show marked progress in JavaScript code and malware deobfuscation, surpassing traditional tools in simplification and readability but remain limited by execution reliability and handling complex or safety-sensitive cases.
 - Large Language Models (LLMs) such as GPT-4o and Codestral can deobfuscate JavaScript programs with superior code simplification and readability, achieving up to 16% greater simplification over traditional deobfuscation tools.
 - Despite generating syntactically correct code in over 97% of predictions, LLMs still face significant challenges with execution correctness, exhibiting a 37.4% average failure rate compared to just 16.25% for established baselines.
 - LLMs outperform rule-based deobfuscators in making obfuscated malware code more analyzable for humans, but their success rates decline notably with increasing obfuscation complexity and can be impaired by context length or safety filters.

<br>

‚ö° **ReDit: Reward Dithering for Improved LLM Policy Optimization** [source](http://arxiv.org/pdf/2506.18631v2.pdf) #general 

 Reward Dithering adds simple noise to discrete rewards, dramatically speeding up and stabilizing policy optimization in LLMs.
 - Introducing controlled random noise (Reward Dithering) to discrete reward signals in large language model (LLM) reinforcement learning accelerates convergence, enabling models to reach strong performance in only 10% of the training steps required by standard methods.
 - Across diverse tasks and LLM architectures, the ReDit method boosts final test accuracy by 1.7‚Äì4.5 percentage points compared to conventional group relative policy optimization (GRPO), while ensuring greater training stability by mitigating both vanishing and exploding gradients.
 - The efficacy of Reward Dithering is highly dependent on selecting an optimal noise variance: moderate perturbations yield the best results, while too little or excessive noise diminishes performance, emphasizing the importance of tailored variance scheduling for practical deployment.

<br>

üéØ **Inference-Time Reward Hacking in Large Language Models** [source](http://arxiv.org/pdf/2506.19248v1.pdf) #security 

 Inference-time reward hacking is inevitable but can be reliably controlled by principled hedging, exemplified by new alignment algorithms that balance reward optimization and model fidelity.
 - Reward hacking at inference time‚Äîwhereby models over-optimize imperfect proxy rewards‚Äîinevitably emerges in optimization schemes such as Best-of-n sampling, causing true performance to first rise then decline as optimization strength increases.
 - Introducing the Best-of-Poisson (BoP) method enables efficient inference-time alignment, achieving a near-optimal balance between maximizing reward and minimizing distortion, with a KL-divergence gap of less than 0.007 compared to the theoretical optimum.
 - Applying the HedgeTune hedging algorithm to tune inference-time parameters significantly mitigates reward hacking, optimizing for true reward over proxy scores and outperforming conventional greedy selection with negligible computational overhead.

<br>

