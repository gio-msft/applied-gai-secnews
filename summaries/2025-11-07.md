üï∏Ô∏è **Inter-Agent Trust Models: A Comparative Study of Brief, Claim, Proof, Stake, Reputation and Constraint in Agentic Web Protocol Design-A2A, AP2, ERC-8004, and Beyond** [source](http://arxiv.org/pdf/2511.03434v1.pdf) #security 

 No single trust mechanism can secure large-scale AI agent networks‚Äîrobust hybrid models and risk-adaptive tiers are essential for resilience against modern LLM vulnerabilities.
 - Hybrid trust models‚Äîcombining proof, stake, brief credentials, and reputation‚Äîsignificantly improve security, reliability, and scalability over single-mechanism approaches in open multi-agent AI ecosystems.
 - Reputation and self-claimed identity are highly vulnerable to LLM-specific weaknesses such as prompt injection, deception, and hallucination, making them insufficient as standalone trust mechanisms.
 - Tiered, task-specific trust protocols anchored in cryptographic proof and economic stake are essential for gating high-impact agent actions, while technical constraints and dynamic credentialing reduce systemic risk and support auditability.

<br>

üõ°Ô∏è **DRIP: Defending Prompt Injection via De-instruction Training and Residual Fusion Model Architecture** [source](http://arxiv.org/pdf/2511.00447v1.pdf) #security 

 Semantic-aware training with DRIP makes LLMs much more resilient against prompt injection attacks without sacrificing output quality.
 - The proposed DRIP method improves semantic role separation between instructions and data by 12‚Äì49% compared to prior defenses, as measured on SEP benchmarks using LLaMA-8B and Mistral-7B models.
 - DRIP reduces the success rate of prompt injection attacks by up to 66%, outperforming state-of-the-art training-time defenses including StruQ, SecAlign, ISE, and PFT in both heuristic and adaptive attack scenarios.
 - Unlike many previous approaches, DRIP maintains instruction-following utility on standard benchmarks such as AlpacaEval and IFEval, matching or exceeding the performance of undefended models.

<br>

üõ°Ô∏è **"Give a Positive Review Only": An Early Investigation Into In-Paper Prompt Injection Attacks and Defenses for AI Reviewers** [source](http://arxiv.org/pdf/2511.01287v1.pdf) #security 

 Invisible prompt injections in papers reliably manipulate AI review scores, and current defenses are easily evaded by adaptive attacks.
 - Malicious prompt injections embedded in scientific papers can systematically bias AI reviewers, increasing average review scores by up to 2.80 points, frequently approaching maximum values.
 - These attacks are robust to variations in injection position, paper length, and initial human-assigned ratings, and maintain effectiveness across multiple frontier AI models, with notable cross-model transferability.
 - Detection-based defenses can identify and partially mitigate basic attacks with over 99% detection accuracy, but remain vulnerable, as adaptive adversarial strategies reduce detection to 24% and still inflate scores by an average of 1.05 points.

<br>

üõ°Ô∏è **Prompt Injection as an Emerging Threat: Evaluating the Resilience of Large Language Models** [source](http://arxiv.org/pdf/2511.01634v1.pdf) #security 

 This work introduces a reproducible framework for quantitatively measuring LLM resilience to prompt injection, revealing that strong alignment and safety training, rather than model size, are critical for defending against adversarial instruction attacks.
 - GPT-4 achieved the highest overall resilience to prompt injection attacks, with a Unified Resilience Score (URS) of 0.871 and a Safety Compliance Coefficient (SCC) of 96.4%, outperforming both GPT-4o and open-source models.
 - Open-source models such as LLaMA-3 8B Instruct and Flan-T5-Large exhibited significantly greater performance degradation and lower safety compliance, especially on reasoning and code generation tasks, highlighting persistent vulnerabilities.
 - Alignment strength and safety tuning were shown to be more influential than model size in mitigating adversarial control, indicating that robust training procedures substantially enhance both task fidelity and refusal of unsafe instructions.

<br>

üõ°Ô∏è **Large Language Models for Cyber Security** [source](http://arxiv.org/pdf/2511.04508v1.pdf) #cyber 

 LLMs revolutionize cybersecurity by delivering high-accuracy threat detection and robust, adaptive defenses‚Äîtransforming encrypted prompt handling and enabling scalable, intelligent protection against sophisticated attacks.
 - Encrypted prompts combined with large language models (LLMs) effectively mitigate prompt injection attacks, enhancing security against unauthorized actions in cybersecurity workflows.
 - LLM-powered intrusion detection systems demonstrate up to 98% classification accuracy‚Äîsignificantly outperforming traditional rule-based and signature-based IDS which struggle to adapt to novel threats.
 - A modular four-layer architecture for integrating LLMs‚Äîdata processing, model integration, application, and continuous learning‚Äîensures scalability, context-awareness, and rapid adaptability for evolving cyber threat landscapes.

<br>

üõ°Ô∏è **LM-Fix: Lightweight Bit-Flip Detection and Rapid Recovery Framework for Language Models** [source](http://arxiv.org/pdf/2511.02866v1.pdf) #security 

 LM-Fix delivers real-time, lightweight detection and rapid recovery from bit-flip attacks in language models, drastically improving reliability and security with minimal cost.
 - Over 94% of single-bit flips and nearly 100% of multi-bit flips in language model parameters are accurately detected using LM-Fix, with minimal computational overhead (‚âà1%‚Äì7.7% at optimal test vector length).
 - The recovery mechanism achieves more than 100√ó speedup versus full-model reloads by restoring only corrupted parameters via small redundancy buffers, requiring under 5% additional memory.
 - Silent Safe Bit-Flips, accounting for the few undetected events, are shown to have negligible impact on model performance and output quality, confirming the reliability of LM-Fix‚Äôs selective detection.

<br>

üîì **AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models** [source](http://arxiv.org/pdf/2511.02376v1.pdf) #security 

 Adaptive multi-turn adversarial prompting exposes major LLM safety gaps, achieving near-total jailbreaks and highlighting the urgent need for more robust, context-aware defenses.
 - Multi-turn automated adversarial attacks achieve a 95% jailbreak success rate on Llama-3.1-8B within six turns, outperforming single-turn attacks by 24%.
 - Inclusion of curated prompt exemplars, adaptive pattern learning, and dynamic temperature management each boost attack success rates by 7-17%, revealing critical weaknesses in current LLM safety strategies.
 - Commercial and open-source large language models remain persistently vulnerable, with AutoAdv achieving up to 99% attack success rate on Qwen3-235B, and even resilient models like GPT-4o-mini reaching 86% with multi-turn adaptive attacks.

<br>

üêù **Let the Bees Find the Weak Spots: A Path Planning Perspective on Multi-Turn Jailbreak Attacks against LLMs** [source](http://arxiv.org/pdf/2511.03271v1.pdf) #security 

 Dynamic path-planning with swarm intelligence uncovers LLM weak spots rapidly and efficiently, demonstrating superior attack success and reduced resource demands compared to existing jailbreak methods.
 - An enhanced Artificial Bee Colony algorithm enables multi-turn jailbreak attacks to achieve over 90% attack success rates on five major language models, peaking at 98% on GPT-3.5-Turbo.
 - The approach reduces overhead by requiring only 26 queries on average for a successful attack, outperforming baseline methods that require up to 50 queries.
 - Attack effectiveness remains consistently high across varied harm categories, with attack success rates above 85% for all categories and above 95% for challenging types like malware and hacking, revealing model weaknesses in specific scenarios.

<br>

üõ°Ô∏è **AdversariaLLM: A Unified and Modular Toolbox for LLM Robustness Research** [source](http://arxiv.org/pdf/2511.04316v1.pdf) #security 

 A unified toolbox fixes critical bugs and standardizes adversarial robustness evaluations, dramatically improving reproducibility and comparability in LLM safety research.
 - Correct implementation of tokenization filters leads to up to 28% higher attack success rates compared to previous baselines, affecting results in 94% of attack runs.
 - Resource-aware tracking of attacks enables more meaningful and fair comparisons across algorithms by automatically separating query, compute, and sampling budgets.
 - The framework achieves 2.12√ó more accurate batched generation results compared to default methods, increasing consistency in reproducible LLM evaluations.

<br>

üß® **Exploiting Latent Space Discontinuities for Building Universal LLM Jailbreaks and Data Extraction Attacks** [source](http://arxiv.org/pdf/2511.00346v1.pdf) #security 

 Demonstrates that latent space discontinuities represent a potent, systemic attack vector capable of universal jailbreaks and sensitive data extraction in ostensibly aligned LLMs and generative AI systems.
 - A novel attack exploiting latent space discontinuities in large language models achieved broad jailbreak and data extraction success across seven state-of-the-art LLMs and one diffusion-based image generator, indicating that these systems are fundamentally vulnerable beyond existing interface-level defenses.
 - Under black-box testing, the attack generated policy-violating and technically detailed content for highly restricted malicious intents‚Äîsuch as weapon and toxin production‚Äîon nearly all tested LLMs, often succeeding within five prompt reformulations.
 - Adversarial prompting with non-semantic or rare tokens caused a diffusion-based generative image model to produce synthetic portraits visually traceable to real individuals, with 91.6% of generated samples matched by public face recognition tools, implicating training data privacy concerns.

<br>

üß® **An Automated Framework for Strategy Discovery, Retrieval, and Evolution in LLM Jailbreak Attacks** [source](http://arxiv.org/pdf/2511.02356v1.pdf) #security 

 A continuous-learning jailbreak framework dramatically raises security risks by autonomously discovering and evolving attack strategies that widely defeat LLM safety measures.
 - A fully automated jailbreak framework can achieve an average attack success rate of 82.7% against mainstream LLMs, outperforming existing black-box attack methods by over 20 percentage points.
 - This framework requires just 2.3 queries on average per successful attack, demonstrating a significant improvement in attack efficiency compared to previous approaches.
 - The strategies learned are highly transferable across datasets and attacker models, indicating the framework reveals fundamental vulnerabilities in LLM defenses rather than overfitting to specific prompts.

<br>

üõ°Ô∏è **Prevalence of Security and Privacy Risk-Inducing Usage of AI-based Conversational Agents** [source](http://arxiv.org/pdf/2510.27275v1.pdf) #security 

 Significant numbers of users exhibit behaviors that pose security and privacy risks when interacting with AI chatbots, with real-world prevalence of risky actions such as uploading untrusted files, jailbreaking, and (occasionally) sharing sensitive information‚Äîoften without full awareness of potential consequences.
 - Over one third of regular AI conversational agent users (approximately 35‚Äì40%) upload non-self-created, potentially insecure content and a notable proportion (16‚Äì24%) grant these agents access to other programs, exposing themselves and organizations to security risks including prompt injections and code execution.
 - About 28% of regular users intentionally attempt to bypass chatbot restrictions ('jailbreaking'), motivated equally by curiosity, entertainment, and the pursuit of restricted information, with such actions not strongly predicted by demographics, experience, or tech-savviness.
 - While most users claim not to share or redact sensitive data, a small but non-negligible subset does input information like passwords and bank details, and the majority remain unaware that their data can be used for model training or that opt-out options exist, underscoring an urgent need for vendor transparency and user education.

<br>

üåê **Do Methods to Jailbreak and Defend LLMs Generalize Across Languages?** [source](http://arxiv.org/pdf/2511.00689v2.pdf) #security 

 Safety alignment and defenses for LLMs do not generalize uniformly across languages, revealing significant multilingual vulnerabilities and the need for language-aware safety benchmarks.
 - Unsafe response rates for language models can vary by more than 50% across different languages, with high-resource languages being safer for standard queries but more vulnerable to adversarial jailbreaking attacks.
 - Simple prompt-based and filtering-based defenses show effectiveness in reducing unsafe outputs, but their robustness is highly dependent on both the underlying language model and the specific language used.
 - A lightweight classifier trained on multilingual response embeddings can detect unsafe outputs with F1-scores up to 0.89 for some attack scenarios, but generalization performance across languages and attack types remains inconsistent.

<br>

üñºÔ∏è **Reimagining Safety Alignment with An Image** [source](http://arxiv.org/pdf/2511.00509v1.pdf) #security 

 A single optimized image prompt can recalibrate multimodal AI model safety, slashing over-refusal and boosting harmful content rejection for agile, deployable alignment.
 - Optimizing a visual prompt, known as Magic Image, significantly reduces over-refusal rates in multimodal large language models, lowering benign query rejection from ~15% to around 2% while not compromising safety against harmful prompts.
 - Magic Image enhances model defense against jailbreak attacks, improving refusal rates for harmful requests by up to 20 percentage points compared to traditional baselines, and achieves a higher safety-efficiency balance score across multiple MLLM architectures and datasets.
 - The effectiveness of Magic Image is robust to different initial image types and performs well even with limited training data, showcasing strong adaptability and transferability without impacting semantic integrity of benign responses.

<br>

üõ°Ô∏è **Death by a Thousand Prompts: Open Model Vulnerability Analysis** [source](http://arxiv.org/pdf/2511.03247v1.pdf) #security 

 Multi-turn conversations drastically increase the vulnerability of open LLMs to adversarial attacks, exposing persistent systemic weaknesses that simple safeguards and single-turn tests routinely miss.
 - Multi-turn adversarial attacks against open-weight large language models exhibit a 2x to 10x increase in success rates compared to single-turn attacks, with some models like Mistral Large-2 experiencing multi-turn attack success rates as high as 92.78%.
 - Models prioritizing capability and leaving safety alignment to deployers (such as Meta‚Äôs Llama and Alibaba‚Äôs Qwen) showed the largest security gaps, while models explicitly emphasizing alignment and safety (such as Google Gemma-3-1B-IT) demonstrated greater resistance to multi-turn exploits.
 - High-risk threat vectors including manipulation, misinformation, and malicious code generation remain prevalent across all tested models, while the top 15 subthreats display alarmingly high exploit success rates, highlighting the need for layered and context-aware security measures.

<br>

üö® **Align to Misalign: Automatic LLM Jailbreak with Meta-Optimized LLM Judges** [source](http://arxiv.org/pdf/2511.01375v1.pdf) #security 

 Automatically co-optimizing jailbreak prompts and evaluation rubrics dramatically amplifies the ability to break safety safeguards in state-of-the-art large language models.
 - The AMIS framework achieved state-of-the-art attack success rates, reaching 88.0% on Claude-3.5-Haiku and 100.0% on Claude-4-Sonnet, surpassing previous jailbreak baselines by an average of more than 70.5 percentage points.
 - Joint optimization of both jailbreak prompts and judge scoring templates, rather than relying on fixed or binary feedback, consistently improves attack effectiveness and creates more generalizable adversarial inputs against various LLMs.
 - Ablation and transferability analyses demonstrate that dense, dataset-level scoring rubrics and prompt inheritance are critical components for maximizing attack success and transferring optimized prompts between different models.

<br>

üß† **Consistency Training Helps Stop Sycophancy and Jailbreaks** [source](http://arxiv.org/pdf/2510.27062v1.pdf) #security 

 Teaching models to respond consistently across adversarial and augmented prompts is a simple, self-supervised way to reduce both sycophancy and jailbreak vulnerabilities with minimal downsides.
 - Bias-augmented Consistency Training (BCT) reduced jailbreak attack success rates from 67.8% to just 2.9% on Gemini 2.5 Flash, while maintaining strong model capabilities.
 - Both BCT and Activation Consistency Training (ACT) suppress sycophancy effectively, increasing the rate of resisting user-led factual errors by up to 25 percentage points across model scales without degrading general knowledge performance.
 - Consistency training with fresh, model-generated data outperforms fine-tuning on static or stale datasets, helping prevent both specification and capability staleness in practical model alignment pipelines.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **Friend or Foe: How LLMs' Safety Mind Gets Fooled by Intent Shift Attack** [source](http://arxiv.org/pdf/2511.00556v1.pdf) #security 

 Subtle, human-readable edits to harmful prompts can bypass LLM safety systems with overwhelming effectiveness, exposing a fundamental challenge for intent inference defenses.
 - Minimal linguistic modifications in queries, such as shifting perspective or tense, result in over a 70% increase in attack success rates against large language models‚Äô safety mechanisms, compared to direct harmful prompts.
 - Fine-tuning models exclusively on benign, intent-shifted data makes them nearly 100% vulnerable to adversarial attacks, demonstrating that superficial query reframing completely undermines robust safety alignment.
 - Existing training-free and training-based defense strategies are inconsistent and largely ineffective against intent-shifted attacks, revealing a critical gap in current LLM safety mechanisms that misinterpret malicious requests as benign information-seeking.

<br>

üí£ **Diffusion LLMs are Natural Adversaries for any LLM** [source](http://arxiv.org/pdf/2511.00203v1.pdf) #security 

 Diffusion LLMs enable efficient, high-success jailbreak attacks against state-of-the-art models, including robust and proprietary LLMs, with low-cost generation and evasive outputs.
 - Adversarial prompts generated by pretrained Diffusion LLMs successfully bypass the safety mechanisms of both open-source and robust proprietary models, achieving attack success rates up to 100% in transfer scenarios.
 - Sample-efficient conditional generation enables attack rates as high as 91‚Äì99% on models hardened by adversarial training, outperforming prior methods with significantly lower computational cost.
 - Prompts produced via diffusion-based inference exhibit low perplexity under target models, making them more natural and difficult to detect using standard likelihood-based defenses.

<br>

üõ°Ô∏è **Rescuing the Unpoisoned: Efficient Defense against Knowledge Corruption Attacks on RAG Systems** [source](http://arxiv.org/pdf/2511.01268v1.pdf) #security 

 RAGDEFENDER delivers resource-efficient, high-accuracy protection against knowledge corruption attacks in RAG systems without incurring model retraining or inference costs.
 - RAGDEFENDER reduces adversarial attack success rates on Retrieval-Augmented Generation (RAG) systems from as high as 0.89 down to 0.02, significantly outperforming previous defenses at similar conditions.
 - The proposed defense mechanism operates without additional model training or inference, yielding 12.36x faster processing speeds and no added GPU memory footprint relative to existing solutions.
 - Robustness remained high even under adaptive and multi-clustered attacks, with RAGDEFENDER maintaining low attack success rates (<0.05) and preserving an average detection rate for adversarial passages above 94%.

<br>

üö® **Characterizing Selective Refusal Bias in Large Language Models** [source](http://arxiv.org/pdf/2510.27087v1.pdf) #security 

 Selective refusal bias in LLM safety guardrails not only perpetuates inequity but also introduces exploitable vulnerabilities for toxic content generation.
 - Large language models systematically refuse to generate toxic content more frequently when the targeted group is historically marginalized, with statistically significant disparities observed across gender, religion, nationality, and sexual orientation attributes.
 - When prompts targeting intersectional groups combine a marginalized and a majority group, the refusal rate typically aligns with the refusal rate of the marginalized group, illustrating the persistence and compounding effect of selective bias.
 - A two-step indirect attacking strategy can bypass guardrails, achieving an average 89.5% attack success rate, enabling the generation of harmful content about previously protected groups by exploiting loopholes in refusal bias.

<br>

üõ°Ô∏è **Black-Box Guardrail Reverse-engineering Attack** [source](http://arxiv.org/pdf/2511.04215v1.pdf) #security 

 Adversarial agents can cheaply and reliably reverse-engineer moderation guardrails in black-box LLMs, posing serious risks to AI safety and intellectual property.
 - Guardrail reverse-engineering via black-box attacks achieves rule matching rates above 92% on commercial LLMs, demonstrating near-complete extraction of moderation policies.
 - The guardrail extraction process is highly cost-effective, converging in a few hundred iterations and requiring less than $85 in API queries per attack target.
 - Surrogate guardrails trained with this method preserve both harmlessness (F1 > 0.81, AUC > 0.85) and transferability, maintaining high predictive power across different attack types and LLMs.

<br>

üõ°Ô∏è **Specification-Guided Vulnerability Detection with Large Language Models** [source](http://arxiv.org/pdf/2511.04014v1.pdf) #security 

 Explicit security specifications mined from historical vulnerabilities enable LLMs to reason about safe code behaviors, yielding state-of-the-art vulnerability detection and real-world vulnerability discovery.
 - Specification-guided vulnerability detection with large language models achieves a 45.0% F1-score on the PrimeVul benchmark, a 32.7% improvement over the best prior method.
 - The approach boosts recall by 50.8%, uncovering 24.3% unique vulnerabilities missed by all other methods, emphasizing its capability to generalize and detect diverse vulnerability types.
 - Detailed module analysis shows combining general and domain-specific security specifications yields the highest detection rates, while ablation reveals both knowledge sources are essential for optimal performance.

<br>

üì± **Measuring the Security of Mobile LLM Agents under Adversarial Prompts from Untrusted Third-Party Channels** [source](http://arxiv.org/pdf/2510.27140v2.pdf) #security 

 Mobile LLM agents are highly susceptible to prompt injections delivered through routine mobile channels, with even the most capable systems routinely executing sophisticated attacks that bypass OS defenses.
 - Over 80% of mobile LLM agents tested could be reliably manipulated via low-barrier adversarial vectors, such as fraudulent ads, with interstitial pop-ups exceeding a 90% attack success rate.
 - Advanced multi-app mobile agents were able to circumvent system-level protections and complete complex malicious workflows, including malware installation and cross-application data exfiltration, with success rates above 90%.
 - Systemic vulnerabilities exist across all evaluated agent architectures, making routine mobile channels‚Äîads, notifications, embedded webviews‚Äîa consistent and highly effective entry point for prompt injection attacks, exposing sensitive user data and enabling persistent compromise.

<br>

üîó **ConneX: Automatically Resolving Transaction Opacity of Cross-Chain Bridges for Security Analysis** [source](http://arxiv.org/pdf/2511.01393v1.pdf) #security 

 Automated semantic reasoning enables highly accurate and efficient traceability for cross-chain blockchain transactions, directly supporting security analysis and anti-money laundering efforts.
 - The system accurately identified cross-chain transaction pairs with a high average F1 score of 0.9746, surpassing previous methods by at least 20%.
 - Semantic search space for cross-chain pairing was pruned from over 10 billion candidates to fewer than 100 using a large language model and an examiner module, enabling efficient analysis at an average processing time of 0.4 seconds per transaction.
 - In real-world applications, the system successfully traced illicit fund transfers, such as detecting a $1 million cross-chain laundering event, improving transparency and security for multi-chain blockchain ecosystems.

<br>

üï≥Ô∏è **ShadowLogic: Backdoors in Any Whitebox LLM** [source](http://arxiv.org/pdf/2511.00664v1.pdf) #security 

 A single stealthy graph-level modification can covertly disable safety in any widely deployed LLM without affecting its detectable behavior, revealing a critical supply-chain vulnerability.
 - Injecting an uncensoring vector and trigger phrase into the computational graph of ONNX-deployed LLMs increased attack success rates from 0% to 62% (Phi-3) and 0% to 70% (Llama 3.2) for bypassing safe content filters.
 - The backdoor technique leaves model performance and standard outputs visually indistinguishable from unmodified baselines, with inference latency rising only 1.2%, enabling stealthy evasion of conventional monitoring systems.
 - Traditional weight or parameter-based integrity checks are ineffective against these attacks, underscoring the urgent need for deployment-phase graph-level integrity verification, hashing, and centralized registries for trusted models.

<br>

üîì **Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive Trigger Learning** [source](http://arxiv.org/pdf/2510.27623v1.pdf) #security 

 MLLM-driven embodied agents are highly vulnerable to object-based visual backdoors that allow covert multi-step policy hijacks with high success and stealth, underscoring urgent security risks for real-world deployments.
 - The BEAT framework can implant visual backdoors in multimodal large language model (MLLM)-driven embodied agents, achieving attack success rates of up to 80% while maintaining strong performance on benign tasks.
 - Contrastive Trigger Learning (CTL) sharply improves backdoor activation accuracy (up to 39% F1 gain) and nearly eliminates false activations, ensuring the agents behave maliciously only when the visual trigger is present.
 - BEAT generalizes robustly even to out-of-distribution trigger instances, reliably activating covert multi-step malicious behaviors with a 92.3% success rate in unfamiliar contexts.

<br>

üõ°Ô∏è **AgentBnB: A Browser-Based Cybersecurity Tabletop Exercise with Large Language Model Support and Retrieval-Aligned Scaffolding** [source](http://arxiv.org/pdf/2511.00265v1.pdf) #cyber 

 AgentBnB shows that browser-based, AI-augmented tabletop exercises can significantly improve the accessibility, scalability, and engagement of cybersecurity incident-response training.
 - Participants expressed a strong preference for the agent-based cybersecurity training platform, rating its intended use at 4.25 out of 5 compared to 2.25 out of 5 for the traditional card-based game.
 - AgentBnB effectively delivered scalable, repeatable, and adaptive incident-response practice, with all participants demonstrating perfect knowledge scores, suggesting a potential ceiling effect for basic content.
 - The retrieval-augmented copilot provided timely, Bloom-aligned instructional support, and was perceived as a scalable alternative with lower logistical overhead than conventional tabletop exercises.

<br>
