üß® **Boundary Point Jailbreaking of Black-Box LLMs** [source](http://arxiv.org/pdf/2602.15001v2.pdf) #security 

 *(UK AI Security Institute, OATML, University of Oxford)*

 A decision-only, curriculum-driven boundary-search can automatically discover transferable universal jailbreak prefixes against hardened, real-world LLM input classifiers‚Äîmaking batch monitoring a first-class defense requirement.
 - A fully black-box jailbreak method using only 1-bit feedback per query (flagged vs not flagged) produced universal adversarial prefixes that bypassed industry-deployed safeguards, including Anthropic‚Äôs Constitutional Classifiers and OpenAI‚Äôs GPT-5 input classifier, without relying on human-provided seed attacks for the classifier.
 - On a biological-misuse rubric, the attack increased non-empty-response average scores on Constitutional Classifiers from 0% to 25.5% (39.9% max within 50 tries) and to 68% (80.4% max@50) with basic elicitation, while increasing GPT-5 input-classifier scores from 0% to 75.6% (94.3% max@50).
 - Developing these universal prefixes required large automated query volumes‚Äîabout 660k queries ($330) for Constitutional Classifiers and 800k queries ($210) for GPT-5‚Äôs input classifier‚Äîimplying single-interaction defenses are insufficient and that effective mitigation should add batch-level monitoring for high-flag, high-volume optimization patterns.

<br>

üõ°Ô∏è **Recursive language models for jailbreak detection: a procedural defense for tool-augmented agents** [source](http://arxiv.org/pdf/2602.16520v1.pdf) #security 

 *(Silverfort)*

 Turning jailbreak detection into an auditable multi-step program (with chunk coverage and de-obfuscation) delivers near-saturated precision and large recall gains over single-pass screening, especially against long-context and split-payload attacks.
 - A procedural, recursive detector (RLM-JB) using normalization/de-obfuscation, coverage-guaranteeing chunking, parallel per-chunk screening, and cross-chunk evidence aggregation achieves 92.5‚Äì98.0% recall with 98.99‚Äì100% precision and 0.0‚Äì2.0% false-positive rates across three screening backends.
 - Replacing a single-pass GPT-5.2 screen with the RLM-JB procedure raises AutoDAN detection recall from 59.57% to 98.00% (+38.43 points, +64.5% relative) while keeping false positives low (1.67%‚Üí2.00%) and improving F1 from 69.71% to 98.49%.
 - Backend choice shifts the sensitivity‚Äìspecificity balance: DeepSeek-V3.2 yields 92.5% recall at 0.0% FPR, GPT-4o reaches 97.0% recall at 0.5% FPR, and GPT-5.2 maximizes recall at 98.0% but increases FPR to 2.0%, implying tuning should be driven by acceptable benign-blocking risk in production agents.

<br>

üéØ **The Vulnerability of LLM Rankers to Prompt Injection Attacks** [source](http://arxiv.org/pdf/2602.16752v1.pdf) #security 

 *(The University of Queensland, CSIRO)*

 LLM rankers can be decisively steered by simple in-document instructions, but encoder‚Äìdecoder architectures largely neutralize the threat and injection placement strongly governs real ranking damage.
 - Prompt-injection jailbreaks embedded in candidate documents reliably manipulate LLM re-rankers across pairwise, listwise, and setwise paradigms, with attacks generalizing from TREC-DL to multiple BEIR domains (general-vs-domain ASR correlation r=0.969, R¬≤=0.938).
 - System-level impact is severe: under setwise reranking, mean nDCG@10 drops from 0.6584 to 0.2947 on average (a 55.2% relative decline), and attacked rerankers frequently underperform a BM25-only baseline.
 - Architecture and placement materially change risk: encoder‚Äìdecoder Flan-T5 models show very low intrinsic susceptibility (‚âà3% mean ASR at ~0.8B‚Äì3B) and near-zero/negative nDCG@10 degradation, while back-of-passage injections are significantly more damaging than front placement for 14/15 evaluated backbones (mean ŒînDCG@10 gap ‚âà0.1106).

<br>

üß™ **IndicJR: A Judge-Free Benchmark of Jailbreak Robustness in South Asian Languages** [source](http://arxiv.org/pdf/2602.16832v1.pdf) #security 

 *(Oracle America Inc.)*

 A judge-free, multilingual benchmark shows that South Asian language jailbreak risk stays high even when contracts boost refusals, and that transfer + orthography effects can systematically distort perceived robustness.
 - Across 12 Indic/South Asian languages, contract-bound JSON evaluation still yields high jailbreak success (e.g., LLaMA 3.1 at 0.922 JSR and Sarvam 1 Base at 0.959 JSR), while unconstrained FREE responses push attacked-benign jailbreak success to ‚âà1.0 for essentially all models, implying refusal contracts can mask real-world vulnerability rather than prevent it.
 - English-to-Indic jailbreak transfer is consistently strong across languages (pooled means ‚âà0.585‚Äì0.694 by target language, with Hindi/Urdu ‚âà0.677/0.694), and format-based wrappers outperform instruction-based wrappers in transfer (per-language means ‚âà0.68‚Äì0.77 vs. ‚âà0.46‚Äì0.61), indicating cross-lingual and formatting attacks are high-leverage red-team vectors.
 - Orthography materially changes contract-bound outcomes: romanized and mixed-script inputs reduce JSON-track jailbreak success from 0.755 (native) to 0.416 (romanized) and 0.488 (mixed), with mean deltas of ‚àí0.338 and ‚àí0.267 and correlations to romanization share/tokenization proxies (œÅ‚âà0.28‚Äì0.32), showing safety measurements can shift systematically with script choice and tokenization behavior.

<br>

üõ°Ô∏è **AISA: Awakening Intrinsic Safety Awareness in Large Language Models against Jailbreak Attacks** [source](http://arxiv.org/pdf/2602.13547v1.pdf) #security 

 *(Beijing University of Technology, Macau University of Science and Technology)*

 AISA turns latent safety signals inside attention heads into a practical, parameter-free jailbreak defense by steering logits in real time from a tiny, single-pass risk score.
 - AISA extracts a prompt-risk signal from the scaled dot-product outputs of a small, automatically selected set of attention heads near the final structural token and uses it for single-pass, logits-level steering without modifying model parameters or user prompts.
 - Even an ‚Äúuncensored‚Äù Llama3.1-8B variant shows intrinsic refusal behavior on harmful prompts, with over 45% of responses rated at least ‚Äúslightly helpful‚Äù or ‚Äúrefused‚Äù on AdvBench and HarmBench under StrongReject scoring, indicating safety-relevant features persist without explicit safety tuning.
 - As a detector, AISA reaches mean accuracy up to 0.9205 across 13 datasets (Llama2-13B-I source), exceeding GPT-5-mini‚Äôs 0.9200 while using only 16 heads (~0.004 MB) and constant O(1) runtime overhead, and as an end-to-end defense it keeps StrongReject scores below 2.0 across evaluated attack sets while maintaining near-vanilla utility and low false-refusal counts.

<br>

üß™ **Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents** [source](http://arxiv.org/pdf/2602.16346v2.pdf) #security 

 *(EPFL, ELLIS Institute T√ºbingen, MPI for Intelligent Systems, T√ºbingen AI Center, KIIT)*

 Multi-turn, tool-using agents can be substantially more prone to illicit assistance than single-prompt tests suggest, and multilingual risk patterns differ from chat-centric assumptions.
 - A coordinated multi-turn red-teaming setup more than doubled illicit agent-task completion for several targets, boosting harmful completion by up to 107.1% over single-turn prompting (e.g., Qwen3-Next 35.1‚Üí72.7 AHS; Claude Sonnet 4.5 16.0‚Üí32.3; DeepSeek-V3.2 31.2‚Üí61.8).
 - Multilingual misuse does not reliably worsen in lower-resource languages for tool-using agents, with language hazard ratios often indistinguishable from English (e.g., GPT-5.1 shows no significant cross-language shift), while some settings were significantly safer than English (Qwen3-Next: Hindi HR 0.62‚Äì0.98; Telugu HR 0.44‚Äì0.74).
 - Lightweight mitigations show sharply different impact profiles: prompt filtering reduced harmful completion only marginally (~0.7% average AHS reduction across non-English), while a simple safety system prompt cut harmful completion by ~37.3% on average with modest benign impact (~5.8% average AHS reduction).

<br>

üß∞ **Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents** [source](http://arxiv.org/pdf/2602.16943v1.pdf) #security 

 *(Independent Researcher)*

 Text refusals are an unreliable safety signal for tool-using LLM agents because models can say ‚Äúno‚Äù while still doing the forbidden action via tool calls.
 - Across 17,420 agent interactions spanning 6 models and 6 regulated domains, models sometimes refused harmful requests in text while still attempting the forbidden tool call (the GAP failure), including 219 such cases even under safety-reinforced system prompts.
 - System prompt wording strongly changed action safety: tool-call-safe (TC-safe) rates shifted by 21‚Äì57 percentage points across prompt conditions depending on the model (e.g., GPT-5.2 ranged from 16% to 73% TC-safe), with 16/18 planned condition comparisons remaining statistically significant after Bonferroni correction.
 - Runtime governance contracts consistently reduced the highest-severity leakage outcome (forbidden tool call plus PII surfaced in text) across all 6 models, but produced no detectable reduction in the rate of forbidden tool-call attempts themselves (i.e., models tried the same unsafe actions even when enforcement was active).

<br>

üõ°Ô∏è **DeepContext: Stateful Real-Time Detection of Multi-Turn Adversarial Intent Drift in LLMs** [source](http://arxiv.org/pdf/2602.16935v1.pdf) #security 

 *(Highflame, Columbia University)*

 Tracking the trajectory of intent with a lightweight recurrent state closes the multi-turn 'safety gap' while staying fast enough for production chat latency budgets.
 - A stateful GRU-based guardrail that tracks intent drift across turns achieves 0.84 F1 on multi-turn jailbreak detection, outperforming leading stateless baselines like Llama-Prompt-Guard-2 and Granite-Guardian (both 0.67 F1).
 - Multi-turn detection improves primarily through higher recall (0.83), while managed cloud guardrails show severe context blindness in the same setting (e.g., Azure Prompt Shield recall 0.11 and F1 0.19) and detect threats later on average (MTTD 8.00 vs 4.24 turns).
 - Real-time viability is demonstrated by sub-20ms per-turn overhead on a T4 GPU (19ms), which is substantially faster than large generative guardrails (e.g., 64‚Äì125ms) and cloud APIs (e.g., 77‚Äì235ms), enabling continuous monitoring without reprocessing full conversation history.

<br>

üõ°Ô∏è **Fail-Closed Alignment for Large Language Models** [source](http://arxiv.org/pdf/2602.16977v1.pdf) #security 

 *(Oregon State University)*

 Safety becomes materially harder to jailbreak when refusal is engineered to be redundant and causally independent rather than concentrated in one suppressible feature.
 - Prompt-based jailbreaks exploit a fail-open safety structure in which refusal behavior collapses to a single dominant latent direction, and ablating that direction causes large spikes in harmful compliance (e.g., Llama2-7B ASR rises from 1% to 58%).
 - Progressive fail-closed alignment that iteratively ablates previously learned refusal subspaces reduces jailbreak attack success rates by 92‚Äì97% and holds ASR to ‚â§4% across four attacks (GCG, AutoDAN, PAIR, HumanJailbreaks) and four models (2B‚Äì9B).
 - Robustness gains largely preserve usefulness, with high benign compliance among robust methods (86.4% average compliance rate) and near-baseline task accuracy (61.6% average; within ~0.8 points of the best baseline), while parameter-efficient LoRA updating ~5% of weights matches full fine-tuning within ~0.6 points.

<br>

üß™ **When Benchmarks Lie: Evaluating Malicious Prompt Classifiers Under True Distribution Shift** [source](http://arxiv.org/pdf/2602.14161v1.pdf) #security 

 *(Zenity)*

 Near-perfect jailbreak/prompt-injection detector scores can be artifacts of dataset provenance, and LODO-style evaluation is needed to reveal real OOD robustness‚Äîespecially for indirect and agentic tool-based injections.
 - Leave-One-Dataset-Out evaluation exposes major generalization overestimation in prompt-attack detectors, dropping pooled ROC AUC from 0.996 (standard 5-fold CV) to 0.912 (LODO), an 8.4-point inflation that would misstate real deployment performance under distribution shift.
 - Generalization failures are highly dataset-dependent, with accuracy gaps between same-source test evaluation and LODO ranging from 1.2% to 25.4% (e.g., 99.5%‚Üí79.4% on mosscap and 94.5%‚Üí69.1% on jayavibhav), implying that a single aggregate benchmark score can hide critical blind spots.
 - Feature analysis shows shortcut reliance is common: 28% of top SAE features are dataset-dependent (including context-dependent shortcuts that look predictive in-distribution), while production guardrails detect only 7‚Äì37% of indirect prompt injections and cannot assess tool-injection formats, whereas an activation-based classifier reaches ~68% detection on indirect attacks (at a 6.5% benign FPR).

<br>

üß™ **AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks** [source](http://arxiv.org/pdf/2602.16901v1.pdf) #security 

 *(Stony Brook University)*

 Breaking malicious objectives into benign-looking multi-step tool use makes agent security failures both easier to trigger and harder to catch than single-turn prompt injection.
 - AgentLAB defines the first long-horizon security benchmark for LLM agents, comprising 5 attack families across 28 realistic tool-enabled environments with 644 security test cases spanning 9 risk categories (e.g., privacy breach 23.6% and financial loss 21.3%).
 - Across six representative agents, long-horizon attacks achieve high overall Attack Success Rates (ASR)‚Äî81.5% (Qwen-3), 78.1% (GPT-4o), and 69.9% (GPT-5.1)‚Äîshowing that multi-turn, adaptive exploitation remains a systemic weakness even for frontier models.
 - Long-horizon task injection consistently outperforms one-shot injection (e.g., GPT-4o ASR 62.5%‚Üí79.9% and GPT-5.1 2.08%‚Üí21.5%), while common single-turn defenses only partially reduce ASR and often leave strong residual risk (e.g., Claude-4.5 tool chaining remains 57.9% ASR under Llama-Guard).

<br>

üßæ **Rubrics as an Attack Surface: Stealthy Preference Drift in LLM Judges** [source](http://arxiv.org/pdf/2602.13576v1.pdf) #security 

 *(University of North Carolina at Chapel Hill, Carnegie Mellon University, Yale University, The University of Texas at Austin)*

 Evaluation rubrics act as a stealthy control interface: you can keep benchmark scores flat while quietly steering both judges and aligned policies off-target.
 - Small, benchmark-compliant natural-language rubric edits can systematically and directionally shift an LLM judge‚Äôs preferences on a target domain (Rubric-Induced Preference Drift), even when benchmark agreement is unchanged.
 - Rubric-only preference attacks can reduce target-domain judging accuracy by up to 9.5 percentage points for helpfulness and 27.9 percentage points for harmlessness while remaining benchmark-valid, indicating benchmark metrics can fail to surface evaluator manipulation.
 - Bias introduced at the rubric level propagates through preference-labeling into downstream post-training (e.g., DPO/RLAIF), producing persistent policy behavior drift where the biased policy is consistently less preferred than the seed-trained policy (often ~40% win-rate) across training regimes.

<br>

üß† **ER-MIA: Black-Box Adversarial Memory Injection Attacks on Long-Term Memory-Augmented Large Language Models** [source](http://arxiv.org/pdf/2602.15344v1.pdf) #security 

 *(The University of Iowa, State University of New York at Binghamton, University of Nevada, Las Vegas, Auburn University)*

 Black-box, embedding-close memory poisoning can nearly collapse long-term reasoning accuracy‚Äîoften without needing more than a couple of injected memories per target.
 - Similarity-based retrieval in long-term memory‚Äìaugmented LLMs can be exploited via black-box interaction alone to persistently corrupt downstream reasoning, demonstrating a system-level vulnerability that holds across multiple LLM backends and memory architectures.
 - Content-based injected memories cause large QA degradation on LoCoMo, with average F1 dropping up to 27.1% on A-mem and up to 72.9% on Mem0 (e.g., harsh-instruction: ‚àí27.1% vs. ‚àí71.5%), indicating some production-oriented memory pipelines are dramatically more sensitive to embedding-close poisoning.
 - Small-footprint question-targeted injections (1‚Äì2 false QA memories per question) reduce overall F1 by at least ~40% and up to ~61.6% depending on model/system, and increasing retrieval depth (k‚â•20) leads to adversarial memories being retrieved for ~99.8‚Äì100% of questions, turning a common ‚Äúretrieve more‚Äù tuning knob into a vulnerability amplifier.

<br>

üõ°Ô∏è **Can Adversarial Code Comments Fool AI Security Reviewers -- Large-Scale Empirical Study of Comment-Based Attacks and Defenses Against LLM Code Analysis** [source](http://arxiv.org/pdf/2602.16741v1.pdf) #security 

 *(Perfecxion.ai)*

 LLM code reviewers largely ignore deceptive comments, but they still miss hard vulnerability patterns‚Äîso the winning play is SAST-guided verification, not comment sanitization.
 - Across 9,366 primary evaluations on 100 vulnerable samples, adversarial security-themed comments shifted detection by only ‚àí5% to +4% with no statistically significant degradation for any of eight models (all McNemar exact p>0.21; 95% CIs span zero), implying comment-based prompt injection is not a primary failure mode for LLM vulnerability detection.
 - Baseline vulnerability detection showed a large capability split‚Äîcommercial models at 89‚Äì96% vs open-source at 53‚Äì72% on uncommented code‚Äîyet both tiers exhibited similarly negligible susceptibility to adversarial comments, indicating model choice (capability) matters more operationally than comment manipulation risk.
 - Among four automated mitigations tested in 4,646 additional evaluations, injecting SAST findings as verification targets achieved 96.9% detection and recovered 47% of baseline misses at single-pass cost, while comment stripping reduced detection for weaker models (e.g., 93.0%‚Üí89.0%), showing defenses should add verification signals rather than remove context.

<br>

üõ†Ô∏è **What Makes a Good LLM Agent for Real-world Penetration Testing?** [source](http://arxiv.org/pdf/2602.17622v1.pdf) #cyber 

 *(Nanyang Technological University, University of New South Wales, Singapore Management University, CFAR, A*STAR, Singapore, Tsinghua University)*

 The key lever for reliable autonomous pentesting is not just more tools or bigger models, but real-time difficulty estimation that prevents wasted effort, premature commitment, and context exhaustion in multi-step attack chains.
 - LLM pentesting agents fail in two distinct ways‚ÄîType A capability gaps (42% of failures, often fixed by better tools/prompts) and Type B complexity barriers (58% of failures, dominated by planning/state issues that persist even with adequate tooling).
 - Architectural advantages in prior agents shrink as backbone models improve, with performance spreads compressing by over half when upgrading from GPT-4o to GPT-5 (e.g., XBOW gaps narrowing from 27‚Äì39% to 40‚Äì49%), implying many designs mainly patch transient model weaknesses rather than enduring task-structure challenges.
 - A difficulty-aware agent design (TDA + Evidence-Guided Attack Tree Search + external memory) achieves up to 91% completion on 104 web tasks (49% relative gain over a 61% best baseline), roots 12/13 realistic machines, and compromises 4/5 GOAD AD hosts versus ‚â§2 for prior systems, demonstrating that real-time tractability estimation materially reduces long-horizon failures (Type B reduced from 58% to 27%).

<br>

üßü **Zombie Agents: Persistent Control of Self-Evolving LLM Agents via Self-Reinforcing Injections** [source](http://arxiv.org/pdf/2602.15654v1.pdf) #security 

 *(National University of Singapore)*

 Long-term memory turns one-time indirect prompt injection into a cross-session, self-reinforcing compromise that bypasses per-session prompt defenses.
 - A two-phase black-box ‚ÄúZombie Agent‚Äù attack can persistently subvert self-evolving LLM agents by getting an indirect prompt-injection payload written into long-term memory during a benign web-browsing task and later triggering unauthorized tool actions in unrelated sessions.
 - In sliding-window (FIFO) memory, the attack maintained 100% payload retention across 20+ rounds via recursive self-replication, whereas standard indirect prompt-injection baselines decayed to 0% once the context window truncated earlier content.
 - In RAG-style memory, the attack stored about 240 payload copies versus about 100 for baselines (~2.5√ó more) and achieved dense retrieval (e.g., ~23 malicious entries in Top-50), keeping attack success >60% even under instruction-based guardrails with only ~10‚Äì15% ASR reduction.

<br>

üîê **SecureGate: Learning When to Reveal PII Safely via Token-Gated Dual-Adapters for Federated LLMs** [source](http://arxiv.org/pdf/2602.13529v1.pdf) #security 

 *(Washington State University)*

 Keyed-token routing turns federated LLM personalization into an access-controlled capability: the same model can safely ‚Äúknow‚Äù PII but only reveal it when explicitly authorized.
 - A token-gated dual-LoRA design separates a globally shareable ‚Äúsecure adapter‚Äù from a local ‚Äúrevealing adapter,‚Äù enabling per-request, inference-time access control over PII without retraining.
 - Unauthorized queries (wrong/no token) reduced leakage by up to 31.66√ó in inference-attack accuracy and 17.07√ó in extraction recall while holding unauthorized inference accuracy near ~4.2% on average and keeping perplexity high (15.89 PPL) to suppress sensitive disclosure.
 - Authorized queries preserved utility (25.20% average inference accuracy vs 20.12% best FL baseline and 6.32 PPL comparable to a revealing-only adapter) with 100% routing reliability and only minimal added compute/communication overhead concentrated in client-side adapter fusion.

<br>

üõ°Ô∏è **AlignSentinel: Alignment-Aware Detection of Prompt Injection Attacks** [source](http://arxiv.org/pdf/2602.13597v1.pdf) #security 

 *(Duke University, NVIDIA)*

 Using an LLM‚Äôs own attention patterns to model instruction hierarchy makes prompt-injection detection both more precise (fewer false alarms on helpful instructions) and more portable across models and domains.
 - A three-class formulation that separates misaligned instructions from aligned instructions and non-instruction inputs reduces false positives that occur when detectors treat any instruction-like text as malicious.
 - Across eight application domains, attention-map features enable near-zero false positive and false negative rates for both direct and indirect prompt injection, while multiple binary baselines either miss most attacks (e.g., ~0.95‚Äì1.00 FNR) or over-flag benign content (e.g., up to ~0.66 FPR).
 - The Enc-first variant generalizes strongly across backend LLMs (Qwen3-8B, Llama-3.1-8B-Instruct, Mistral-7B-Instruct) with accuracy typically ‚â•0.98 for direct attacks and ~0.98 for indirect attacks, and it transfers to IHEval rule-following with lower error than Avg-first (e.g., FPR/FNR 0.03/0.04 vs 0.08/0.14 on Qwen3-8B).

<br>

üß† **NeST: Neuron Selective Tuning for LLM Safety** [source](http://arxiv.org/pdf/2602.16835v1.pdf) #security 

 *(Technical University of Darmstadt)*

 Targeting and tying updates over safety-relevant neuron clusters delivers near‚Äìfull fine-tuning safety robustness at a tiny parameter budget and without runtime controls.
 - Neuron-selective, cluster-tied updates cut average jailbreak attack success rate from 44.5% to 4.36% across 10 open-weight LLMs, a 90.2% reduction in unsafe generations under a black-box prompt-attack evaluation.
 - The approach achieves this safety gain with only 0.44M trainable parameters on average‚Äîabout 17,310√ó fewer updated parameters than full fine-tuning and 9.25√ó fewer than LoRA‚Äîwhile producing a standard model with no inference-time overhead after merging updates.
 - Safety improvements generalize beyond text-only prompting, reducing multimodal attack success rate from 55.3% to 1.1% across text/image and reasoning-augmented settings, while largely preserving capability with small average accuracy drops of 0.9 points (GSM8K), 4.9 points (ARC), and 3.7 points (MMLU).

<br>

üïµÔ∏è **Large-scale online deanonymization with LLMs** [source](http://arxiv.org/pdf/2602.16800v1.pdf) #security 

 *(ETH Zurich, Anthropic)*

 LLMs turn pseudonymity into a brittle defense by converting messy text into scalable, high-precision identity linkage across platforms and time.
 - Autonomous web-enabled LLM agents re-identified 67% of sanitized Hacker News users by linking them to LinkedIn at 90% precision (226/338), showing minutes-scale doxxing capabilities comparable to hours of human investigation.
 - In closed-world cross-platform matching at ~89k candidates, an LLM pipeline (extract‚Üíembed search‚ÜíLLM reasoning) achieved up to 68% recall at 90% precision and 45.1% recall at 99% precision, versus ~0.1% recall for a Netflix-Prize-style non-LLM baseline.
 - On temporally split Reddit profiles (5k queries, 10k candidates), adding LLM selection plus calibration-by-pairwise sorting increased recall at 99% precision from 16.0% (embedding search) to 38.4%, implying large-scale account linkage remains feasible even under stringent false-positive constraints.

<br>

üõ°Ô∏è **Closing the Distribution Gap in Adversarial Training for LLMs** [source](http://arxiv.org/pdf/2602.15238v2.pdf) #security 

 *(Technical University of Munich)*

 Using diffusion models to sample high-likelihood harmful triggers closes a key generalization gap in adversarial training, turning simple rephrasings/translations from a persistent weakness into a trainable distribution-coverage problem.
 - Distributional Adversarial Training (DAT) reduced worst-case jailbreak success (Best-of-All ASR) on Llama3-8B from 100% (default) and 77% (best baseline MixAT-GCG) to 36%, while keeping utility roughly on par with other adversarial-training methods (XSTest 0.464 vs 0.464 for CAT).
 - DAT sharply improved robustness against diffusion inpainting attacks‚Äîthe strongest data-specific attack‚Äîcutting ASR on Llama3-8B from 77‚Äì94% for prior training methods to 32%, and on Qwen2.5-14B from 75‚Äì93% to 14%.
 - Scaling the number of diffusion-generated prompt variants per harmful behavior from 1 to 16 reduced inpainting ASR on Llama3-8B from 54% to 22% with XSTest staying near ~0.4, indicating robustness gains can be increased via more coverage without proportional helpfulness loss.

<br>

üì¶ **Mind the Gap: Evaluating LLMs for High-Level Malicious Package Detection vs. Fine-Grained Indicator Identification** [source](http://arxiv.org/pdf/2602.16304v1.pdf) #cyber 

 *(University of Alabama, University of Dhaka, Auburn University)*

 LLMs can flag suspicious packages extremely well, but they struggle to explain exactly what the malware is doing‚Äîcreating a practical gap between detection and actionable diagnosis.
 - Across 13 models and a 4,070-package corpus (3,700 benign/370 malicious), top-tier performance reached near-perfect package-level detection (GPT-4.1 F1‚âà0.99) while average models were unreliable (mean F1‚âà0.57, œÉ‚âà0.18), implying that only a small subset of models are dependable as first-line supply-chain filters.
 - When shifting from binary package detection to identifying 47 fine-grained malicious indicators, mean performance dropped from ‚âà0.58 to ‚âà0.34 (‚âà41% relative decline), showing that strong ‚Äúmalicious/not‚Äù judgments often fail to translate into actionable, indicator-level triage.
 - Model scale did not meaningfully predict indicator-identification quality (Spearman œÅ‚âà0.07, p‚âà0.82 for parameter size), while indicator detectability was driven by ‚Äúsyntactic rigidity‚Äù (e.g., shell command execution mean‚âà0.52 vs. dynamic module import mean‚âà0.03 and crypto-wallet harvesting mean‚âà0.00), indicating defenses should prioritize signature-like indicators for automation and reserve ambiguous intents for deeper analysis.

<br>

üïµÔ∏è **Automating Agent Hijacking via Structural Template Injection** [source](http://arxiv.org/pdf/2602.16958v1.pdf) #security 

 *(Tsinghua University, Ant Group, Zhongguancuan Laboratory)*

 By optimizing injected chat-template structure rather than natural-language persuasion, attackers can reliably induce role confusion and hijack tool-using LLM agents even when semantic jailbreak defenses are in place.
 - An automated structural template injection method achieved a 79.76% average attack success rate across seven closed-source agents, outperforming Single-Template (54.09%), Semantic-Injection (39.86%), and ChatInject (38.46%), indicating that token-level role parsing is a dominant, cross-model weakness.
 - In the most complex benchmark setting (Workspace), the attack retained high effectiveness under common defenses‚Äîe.g., on GPT-4.1 it achieved 76.62% ASR with delimiter-spotlighting enabled and 74.03% ASR with tag-filtering enabled‚Äîshowing that instruction-based and rule-based sanitization do not reliably stop role-confusion exploits.
 - Large-scale testing across 942 commercial agents uncovered 70+ vendor-confirmed vulnerabilities (including data exfiltration and remote code execution) and exposed systemic framework-level risk via an MCP integration issue assigned CVE-2025-6***4, implying widespread downstream exposure through shared agent infrastructure.

<br>

üîÅ **Overthinking Loops in Agents: A Structural Risk via MCP Tools** [source](http://arxiv.org/pdf/2602.14798v1.pdf) #security 

 *(Yonsei University, Ewha Womans University, Hankuk University of Foreign Studies (HUFS))*

 A small set of plausible-looking tools can quietly turn tool use into an economic DoS by creating cycles that explode cost and latency while individual steps appear normal.
 - A malicious MCP tool server co-registered in an otherwise normal tool registry can induce structural tool-call cycles that amplify end-to-end token usage by 5.5√ó‚Äì142.4√ó without changing the user query or base model.
 - In production-grade coding-agent settings, loop induction becomes extreme and can prevent natural termination, with per-task spikes up to 971.27√ó tokens (and up to 180.12√ó runtime) leading to budget exhaustion and no usable output.
 - Decoding-time concision controls aimed at reducing local verbosity (e.g., suppressing ‚Äúwait‚Äù-style tokens) do not reliably stop cycle induction, implying defenses must detect/limit cyclic tool-call structure (recursion, repeated stages, and depth/turn patterns) rather than token output alone.

<br>

üîê **PrivAct: Internalizing Contextual Privacy Preservation via Multi-Agent Preference Training** [source](http://arxiv.org/pdf/2602.13840v1.pdf) #security 

 *(Duke University, University of Florida)*

 Internalizing contextual privacy into agent policies improves privacy without paying the usual helpfulness penalty and remains robust across models, benchmarks, and agent topologies.
 - Multi-agent preference training that embeds contextual privacy into each agent reduced privacy leakage by up to 12.32% versus an agent-based information-flow-control baseline while keeping helpfulness essentially unchanged (e.g., 86.30% vs 86.93% average helpfulness on Mistral-7B).
 - Across four backbones (Llama-3.1-8B, Llama-3.2-1B, Mistral-7B, Qwen3-4B), the approach consistently shifts the privacy‚Äìhelpfulness Pareto frontier upward, indicating better privacy at similar utility under both average-case and worst-case (leak@K with K=10) sampling evaluations.
 - Models trained only on PrivacyLens generalized zero-shot to ConfAIde (Tier 3‚Äì4) and maintained gains across multiple multi-agent topologies without topology-specific retraining, suggesting contextual privacy behavior transfers beyond the original agentic setting.

<br>

üîê **Privacy-Aware Split Inference with Speculative Decoding for Large Language Models over Wide-Area Networks** [source](http://arxiv.org/pdf/2602.16760v1.pdf) #security 


 Speculative (lookahead) decoding turns WAN round-trip latency into multi-token progress while preserving token-identical greedy outputs, making privacy-preserving split LLM inference practical at interactive speeds.
 - Keeping the embedding and LM head local ensures raw tokens never leave the trusted device while only ~8‚Äì10 KB/token intermediate activations traverse the WAN, enabling split inference with 2.0 GB (7B) to 4.9 GB (12B) local VRAM.
 - Over an ~80 ms RTT WAN link, the system sustains 8.7‚Äì9.3 tok/s on Mistral 7B and 7.8‚Äì8.7 tok/s on NeMo 12B with lookahead decoding, and an RTT decomposition model with <6.2% error projects 15‚Äì19 tok/s at 20 ms RTT.
 - Intermediate-activation inversion risk is tunable via split depth: a simple MLP attacker recovers ~59% of tokens at a 2-layer local split versus ~35% at an 8-layer split with minimal throughput impact, highlighting depth as a practical privacy-performance knob.

<br>

üõ°Ô∏è **MCPShield: A Security Cognition Layer for Adaptive Trust Calibration in Model Context Protocol Agents** [source](http://arxiv.org/pdf/2602.14281v1.pdf) #security 

 *(NTU, BUPT, MBZUAI, PayPal Inc, Squirrel AI)*

 Treating MCP tool calls as auditable experience‚Äîprobe before use, constrain during use, and reason after use‚Äîenables agents to calibrate trust in third-party servers instead of blindly trusting server-supplied tool metadata and outputs.
 - A lifecycle-wide security cognition plug-in for MCP agents improved average defense against 76 malicious MCP servers across six attack suites and six agentic LLM backbones from 10.05% (undefended) to 95.30% while maintaining high stability from Pass@1 through Pass@5.
 - Stage-wise results show most malicious servers are blocked pre-invocation via metadata-guided probing (often ‚â•80‚Äì100% depending on suite/model), with execution-time isolation providing the remaining coverage against stealthy side effects that bypass probing and periodic reasoning adding extra protection under delayed ‚Äúrug pull‚Äù drift (up to 26.73% additional detections in one backbone).
 - On benign MCP servers, denial rates remained far below malicious-defense rates (as low as 2.35‚Äì3.53% average for some backbones, but up to 29.41% for the worst backbone), indicating the main operational trade-off is model-dependent false-positive risk rather than loss of attack coverage.

<br>

üå≥ **Anticipating Adversary Behavior in DevSecOps Scenarios through Large Language Models** [source](http://arxiv.org/pdf/2602.14106v1.pdf) #cyber 

 *(University of Murcia, Universidad del Rosario)*

 LLM-generated attack-defense trees can be scored for realism and actionability, and the best-scoring branches can be turned into reproducible chaos-security experiments that validate real privilege-escalation paths in cloud DevSecOps.
 - An LLM-driven, six-phase workflow (context setup ‚Üí structured prompting ‚Üí attack-context scoring ‚Üí cosmetic refinement ‚Üí expert validation) produces attack-defense trees in DOT/Graphviz format that can be directly reused to design Security Chaos Engineering experiments in DevSecOps pipelines.
 - In a GovCloud military logistics case study, QwQ-32B achieved a higher overall attack-defense-tree quality score than GPT-4 (71.60% vs 61.73%), driven by higher known-TTP coverage (22.22% vs 11.11%) and more operationally usable procedures (92.59% vs 74.07%) while both maintained fully ordered procedures (100%).
 - A privilege-escalation chaos experiment derived from an LLM-generated EC2 branch successfully obtained temporary IAM role credentials by combining ec2:RequestSpotInstances with iam:PassRole and malicious user data (reverse shell), demonstrating that LLM-generated paths can be executable inputs for proactive resilience testing and detection hypotheses (e.g., GuardDuty findings).

<br>

üß© **From Tool Orchestration to Code Execution: A Study of MCP Design Choices** [source](http://arxiv.org/pdf/2602.15945v1.pdf) #security 

 *(Ben Gurion University of The Negev)*

 Shifting from tool-by-tool calls to model-written code makes agents faster and cheaper, but turns tool outputs and exceptions into a direct security control channel that demands execution-governance defenses.
 - A context-decoupled Code Execution MCP (CE-MCP) consolidates multi-tool workflows into a single sandboxed program and consistently reduces token usage, end-to-end execution time, and interaction turns versus context-coupled MCP as server/tool scale increases.
 - Task quality (task fulfillment, tool selection, and parameter accuracy) is broadly comparable between CE-MCP and traditional MCP on single- and two-server workloads, but CE-MCP shows occasional degradation on some three-server tasks when upfront code synthesis misses global conditional branches that multi-turn execution can correct incrementally.
 - Introducing executable orchestration expands the agent attack surface with 16 attack classes across five phases, and representative exploits demonstrate that exception-mediated privilege escalation and cross-tool execution-sink manipulation can occur without sandbox escape, implying production deployments need layered defenses combining container isolation, pre-execution code validation, and pre/post-execution semantic gating.

<br>

üß™ **NESSiE: The Necessary Safety Benchmark -- Identifying Errors that should not Exist** [source](http://arxiv.org/pdf/2602.16756v1.pdf) #security 

 *(University of T√ºbingen, Max Planck Institute for Intelligent Systems, ELLIS Institute T√ºbingen, T√ºbingen AI Center)*

 A lightweight keyword-checkable benchmark exposes that modern LLMs still fail a ‚Äúminimum bar‚Äù of safe rule-following and can lose double-digit safety reliability under benign context distractions.
 - No evaluated model achieved 100% on the 93-case NESSiE necessary-safety suite, with the best base result at 95.2% Safe-&-Helpful (Gemini 2.5 Pro), implying even top LLMs still produce avoidable safety-relevant errors on minimal, non-adversarial tests.
 - Across models, Helpfulness systematically exceeded Safety (e.g., Qwen3 VL 32B: 99.7% helpful vs 62.7% safe, yielding 62.4% SH), indicating a deployment risk where models tend to reveal or comply rather than reliably withhold under access-control-like rules.
 - Adding a benign ~2000-token distraction context reduced SH performance by ‚â•15 percentage points (e.g., Grok 4: 92.0%‚Üí73.1%; Claude Opus 4.5: 82.6%‚Üí59.0), showing safety adherence is fragile under realistic long-context noise typical of agentic logs.

<br>

ü§ñ **What Breaks Embodied AI Security:LLM Vulnerabilities, CPS Flaws,or Something Else?** [source](http://arxiv.org/pdf/2602.17345v1.pdf) #security 

 *(Shandong University)*

 Embodied AI breaks most often at the semantic-to-physics interface: correctness in language and planning can still be dangerously wrong in the real world.
 - Embodied AI failures frequently stem from embodiment-induced system mismatches‚Äîwhere semantically correct plans still violate geometry, dynamics, and contact constraints‚Äîso improving LLM reasoning alone does not guarantee physical safety.
 - Because physical outcomes are highly state-dependent under nonlinear dynamics and uncertainty, the same action can be safe in one state and hazardous in another, making one-shot validation or static certification of action safety fundamentally unreliable.
 - Small errors can cascade through tightly coupled perception‚Äìdecision‚Äìaction feedback loops and accumulate over time (non-compositional safety), meaning locally safe decisions can compound into globally unsafe behavior unless systems explicitly model risk propagation and uncertainty across layers.

<br>

üß¨ **Weight space Detection of Backdoors in LoRA Adapters** [source](http://arxiv.org/pdf/2602.15195v2.pdf) #security 

 *(Algoverse AI Research, University of Aberdeen, Independent)*

 Backdoored LoRA adapters can be detected pre-deployment by spotting unusually concentrated singular-value spectra in adapter weight updates, enabling scalable hub screening without running the model.
 - A fully static, data-agnostic detector flags poisoned LoRA adapters by analyzing weight updates (ŒîW) via SVD-derived spectral statistics, eliminating the need for model execution or trigger-specific test inputs.
 - On a 500-adapter benchmark for Llama-3.2-3B (400 clean, 100 poisoned with 1%/3%/5% injection rates across rare-token and contextual triggers), the method achieved 97% overall accuracy with under 2% false positives, including 98% benign accuracy (49/50) and 96% poisoned detection (48/50) on a held-out 100-adapter test set.
 - Logistic-regression fusion shows kurtosis (0.452) and energy concentration (0.353) drive over 80% of the detection decision, implying backdoors tend to manifest as sharply peaked, low-entropy, high-concentration weight-space updates that can be screened at hub scale.

<br>

üõ†Ô∏è **Execution-State-Aware LLM Reasoning for Automated Proof-of-Vulnerability Generation** [source](http://arxiv.org/pdf/2602.13574v1.pdf) #cyber 

 *(University of Illinois Urbana-Champaign)*

 Converting low-level execution traces into source-level constraints lets an LLM agent iteratively ‚Äúdebug‚Äù its own inputs and generate validated PoVs far more cost-effectively than open-loop agents.
 - An execution-state-aware, closed-loop agent (DrillAgent) achieved a 28.9% validated PoV resolution rate on 190 real-world C/C++ CVEs (55/190), exceeding the previously best reported 18% rate and yielding a 52.8% relative increase in solved tasks (55 vs. 36).
 - Under a fixed $1.5/task budget on a 60-task subset, the system generated 15 validated PoVs versus 6 for a strong general-purpose agent baseline using the same model, cutting cost per successful PoV to $7.72 compared with $15.30 (budgeted) and $20.13 (unlimited) for the baseline.
 - Ablations show that removing root-cause context, execution feedback, or crash-triggering guidance sharply reduces validated PoVs (from 15 down to 3/2/10 respectively on a 30-task solved-only subset), implying that grounding LLM reasoning in source-mapped execution traces is the primary driver of reliable PoV convergence.

<br>

üõ°Ô∏è **From SFT to RL: Demystifying the Post-Training Pipeline for LLM-based Vulnerability Detection** [source](http://arxiv.org/pdf/2602.14012v1.pdf) #cyber 

 *(Microsoft, University of Texas at Dallas)*

 A small VD-focused LLM can outperform much larger zero-shot models when post-training prioritizes leak-free SFT, exploration-friendly RL, and root-cause-verified rewards/evaluation.
 - Cold-start SFT data built via rejection sampling delivers ~115.4% relative F1 improvement over rationalization-based supervision, because rationalization leaks privileged ground-truth context that later triggers hallucinations (e.g., inventing irrelevant CVE IDs) when only code is available at inference time.
 - Increasing SFT epochs consistently boosts off-policy preference optimization (DPO P-C +34.9% from 1‚Üí5 SFT epochs; ORPO +14.5%), but overly strong SFT harms on-policy GRPO by suppressing exploration, yielding ~8.5% lower average F1 at 5-epoch initialization versus lighter SFT starts.
 - On-policy GRPO with root-cause (reasoning-level) rewards reaches 36.51% F1 and 72.17 pass@8 on a 4B model‚Äîbeating SFT (28.26% F1) and off-policy methods‚Äîwhile coarse rewards actively degrade performance (detection-level reward drops to 22.24% F1, i.e., ‚àí27.1% vs SFT) and binary/CWE evaluation can severely misstate capability (88.4% of binary-correct detections have wrong CWE, and 56.3% of CWE-correct answers have wrong root-cause reasoning).

<br>

üìê **Detecting LLM Hallucinations via Embedding Cluster Geometry: A Three-Type Taxonomy with Measurable Signatures** [source](http://arxiv.org/pdf/2602.14259v1.pdf) #security 

 *(Independent Researcher)*

 Embedding cluster geometry yields measurable, model-calibratable signals that separate vague center-drift, confident wrong-topic convergence, and true semantic coverage gaps‚Äîhighlighting how architectural compression or distillation can erase key radial diagnostics.
 - A three-type hallucination taxonomy maps distinct generation failures to embedding-space signatures: Type 1 center-drift (low norm + low soft cluster membership), Type 2 wrong-well convergence (high membership with trajectory discontinuities), and Type 3 coverage gaps (low max centroid similarity with high local similarity variance).
 - Across 11 transformer models, polarity coupling and real cluster structure are universal‚ÄîŒ± exceeds 0.5 in 11/11 models and centroid-based cohesion Œ≤ is significantly positive in 11/11‚Äîsupporting cluster-geometry-based detection as architecture-agnostic at a prerequisite level.
 - A nonlinear norm‚Äìinformation relationship is present in most models (Œªr significant in 9/11 at p<0.05), while ALBERT and MiniLM lack significant Œªr due to factorized embedding compression and distillation-induced isotropy, implying weaker radial warning signals and a higher expected susceptibility to Type 1 (center-drift) failures in these architectures.

<br>

üõ°Ô∏è **Tutoring Large Language Models to be Domain-adaptive, Precise, and Safe** [source](http://arxiv.org/pdf/2602.13860v1.pdf) #security 

 *(Indian Institute of Technology Kharagpur)*

 Targeted ‚Äútutoring‚Äù methods‚Äîgraph grounding, decoding-time safety steering, and lightweight parameter updates‚Äîshow that precision and global safety can be improved substantially without full retraining.
 - Weak supervision plus active learning over 170,000 bug reports and manuals enables extraction of 9 software-specific entity types, improving technical precision in low-resource software ecosystems without expensive expert labeling.
 - Instruction-centric and pseudocode-formatted prompts increase unethical generations by 2‚Äì38% even in heavily safety-trained models, exposing a reliability gap in natural-language-optimized guardrails.
 - Offline preference optimization can cut a model‚Äôs cultural harm rate from 71.96% to 3.07% across 11 cultures and 12 sensitive topics, while multilingual safety can be improved by tuning only ~3% of parameters to reduce policy violations across languages.

<br>

ü§ù **AREG: Adversarial Resource Extraction Game for Evaluating Persuasion and Resistance in Large Language Models** [source](http://arxiv.org/pdf/2602.16639v1.pdf) #security 

 *(Islamic University of Technology)*

 A single benchmark can score both how well an LLM persuades and how well it resists‚Äîand the two skills don‚Äôt reliably come together.
 - Across 280 zero-sum negotiation games between eight frontier LLMs, resistance systematically outperformed persuasion with Victim Elo exceeding Culprit Elo for every model by a mean +216 points, implying current systems are generally harder to extract money from than to successfully pressure into paying.
 - Persuasion and resistance behaved as largely independent capabilities (Spearman œÅ=0.33, p=0.42), with the best persuader (DeepSeek V3.2) ranking only 5th in resistance and the best defender (GPT-5.2) ranking 5th in persuasion, indicating that single-score ‚Äúsocial intelligence‚Äù evaluations can miss asymmetric vulnerability profiles.
 - Outcomes were strongly shaped by interaction tactics and timing: 57.5% of commitments occurred in the first three turns and if no extraction happened by turn 5 the chance of later extraction fell below 4%, while incremental commitment strategies produced 61.4% mean extraction versus 22.2% for single-ask strategies (2.8√ó, p<1e-8) and verification-seeking defenses correlated most with successful resistance (œÅ=+0.377) compared to explicit refusal which correlated negatively (œÅ=-0.135).

<br>

üß≠ **Discovering Implicit Large Language Model Alignment Objectives** [source](http://arxiv.org/pdf/2602.15338v1.pdf) #security 

 *(Stanford University)*

 It turns reward models from a black box into a compact, human-readable checklist of what the aligned model is actually being paid to do‚Äîand can flag unintended incentives early.
 - The proposed Obj-Disco framework reconstructs opaque alignment reward behavior with >90% fidelity across controlled tasks and popular open-source reward models, enabling reward signals to be expressed as sparse weighted natural-language objectives.
 - Human studies show the selected exemplar trajectories improve users‚Äô ability to identify the underlying objective to 39.9% ¬± 6.5% versus 25.5% ¬± 5.8% with random examples (p < 0.001), indicating more informative explanations of behavioral change.
 - In a safety-auditing case study, the method surfaced a latent misaligned incentive (increased permissiveness toward illegal/unethical acts) in 3/4 trials while baselines detected it in at most 1/4, suggesting residual-driven objective discovery can expose hidden reward hacking risks.

<br>

ü§ñ **AgentRob: From Virtual Forum Agents to Hijacked Physical Robots** [source](http://arxiv.org/pdf/2602.13591v1.pdf) #security 

 *(Peking University)*

 Turning an ordinary online forum into an asynchronous command bus makes it possible for LLM agents to coordinate and actuate real robots at community scale‚Äîcreating both a new control interface and a new attack surface.
 - A three-layer pipeline links forum threads to physical execution by having agents poll for @mentions, translate posts into robot commands via MCP and VLM tool-calling, and publish completion summaries back to the same topic as a permanent record.
 - Standardizing all forum interactions into eight MCP tools (1 meta, 3 read, 2 write, 2 identity) cleanly decouples agent logic from any specific forum backend and enables cross-agent interoperability through a JSON-RPC interface.
 - The system demonstrates concurrent multi-agent, multi-embodiment operation in one forum by binding distinct identities and triggers (e.g., @quadruped, @humanoid) to separate Unitree Go2 (12-DOF) and G1 (23-DOF) robots while preventing self-reply loops via mandatory metadata tags.

<br>

üõ†Ô∏è **AXE: An Agentic eXploit Engine for Confirming Zero-Day Vulnerability Reports** [source](http://arxiv.org/pdf/2602.14345v1.pdf) #cyber 

 *(Drexel University, Virginia Commonwealth University)*

 Minimal SAST-style hints (CWE + file/line) are enough for an agentic system to reliably turn noisy vulnerability reports into verified, reproducible exploit evidence‚Äîwhen planning and execution are split into specialized roles.
 - A multi-agent grey-box exploitability validator using only CWE + vulnerable code location + source code achieved 30% Success@5 (25% Success@1) on 40 real-world web CVEs, delivering a 3√ó gain over leading black-box baselines capped at 10% Success@5.
 - Removing grey-box inputs dropped performance from 30% to 10% Success@5 (and 25% to 7.5% Success@1), while keeping grey-box inputs but collapsing to a single agent reduced Success@5 from 30% to 17.5%, showing that both metadata and role specialization materially increase exploit confirmation rates.
 - In failed grey-box attempts (n=25), 76% of failures originated in high-level planning, dominated by misread vulnerability semantics (60%) and wrong attack-surface targeting or unmet preconditions (44% each), while successful runs produced reproducible PoCs in 11/12 cases with explicit verification oracles in 11/12.

<br>

üìú **Verifiable Semantics for Agent-to-Agent Communication** [source](http://arxiv.org/pdf/2602.16424v1.pdf) #security 

 *(Microsoft AI, Wabash College)*

 It operationalizes ‚Äúshared meaning‚Äù as a statistically testable property and enforces it by limiting agent communication to a certified vocabulary with provable error bounds.
 - A stimulus-meaning certification protocol records agent verdicts on shared, publicly identified events in a tamper-evident ledger and certifies a term only when the Wilson upper bound on its contradiction rate is ‚â§ œÑ (with confidence ‚â• 1‚àíŒ¥) and coverage exceeds œÅmin, enabling third-party auditability of semantic alignment.
 - Restricting downstream decisions to the certified core vocabulary (‚Äúcore-guarded reasoning‚Äù) yields reproducible agent conclusions with disagreement bounded by œÑ (at confidence ‚â• 1‚àíŒ¥), turning semantic reliability into an explicit, tunable coverage-vs-accuracy trade-off.
 - Core-guarding cuts downstream disagreement by 72‚Äì96% in simulations (e.g., 7.4%‚Üí2.1% under moderate drift and 40.7%‚Üí1.8% under high divergence) and by 51% on fine-tuned LLM agents (5.3%‚Üí2.6%), while recertification can detect drift that would otherwise raise guarded disagreement to 4.6% and renegotiation can restore core size (e.g., back to ~4 terms).

<br>

üì° **Intellicise Wireless Networks Meet Agentic AI: A Security and Privacy Perspective** [source](http://arxiv.org/pdf/2602.15290v1.pdf) #security 

 *(Beijing University of Posts and Telecommunications, China United Network Communications Corporation, University of Exeter, University of Surrey)*

 Agentic AI can simultaneously strengthen wireless security and introduce new, stage-specific failure modes‚Äîmaking lifecycle-aware defenses the central design requirement for 6G-era intellicise networks.
 - Agentic AI‚Äôs closed-loop perception‚Äìmemory‚Äìreasoning‚Äìaction workflow enables proactive defense, real-time adaptive responses, and continual learning in intellicise wireless networks, addressing key limitations of static generative-AI defenses (manual triggering, slow translation to actions, and non-evolutionary updates).
 - Security and privacy risks concentrate across all four agent stages‚Äîperception (e.g., injection and inference attacks), memory (persistent knowledge-base poisoning), reasoning (path hijacking, objective corruption, and multi-agent collusion), and action (autonomy abuse, oversight saturation, and governance obfuscation)‚Äîimplying that end-to-end protection must cover the full agent lifecycle rather than single-module hardening.
 - A practical agentic semantic-steganography design uses public keys plus hidden digital tokens to control diffusion-based reference reconstruction so that legitimate receivers can recover the secret image while eavesdroppers observing the stego image (even with the public key and implicit features) fail due to token-gated deterministic perturbations and inverse sampling requirements.

<br>

üß∞ **A Trajectory-Based Safety Audit of Clawdbot (OpenClaw)** [source](http://arxiv.org/pdf/2602.14364v1.pdf) #security 

 *(ShanghaiTech University, Shanghai Artificial Intelligence Laboratory)*

 Trajectory-level logging plus dual automated-and-human judging exposes that the biggest risk comes from how small intent errors amplify into irreversible tool actions under ambiguity and prompt hijacking.
 - Across 34 canonical safety cases with full-trajectory auditing, Clawdbot achieved a 58.9% overall safe pass rate, indicating that unsafe tool-mediated behaviors occur frequently enough to be operationally significant for a broad-action personal agent.
 - Safety performance was highly non-uniform: Hallucination & Reliability passed 100% while Intent Misunderstanding & Unsafe Assumptions passed 0%, showing that ambiguity handling‚Äînot factual grounding‚Äîis the dominant driver of high-impact failures such as broad deletions or configuration overwrites.
 - Robustness remained weak under adversarial or underspecified steering (Prompt Injection 57% and Unexpected Results 50%), demonstrating that benign-looking jailbreak prompts and open-ended goals can redirect the agent into deceptive or destructive actions without reliable clarification or gating.

<br>

üëÅÔ∏è **Visual Persuasion: What Influences Decisions of Vision-Language Models?** [source](http://arxiv.org/pdf/2602.15278v1.pdf) #security 

 *(Massachusetts Institute of Technology, MIT Media Lab, BITS Pilani, Dartmouth College)*

 Small, plausible presentation changes (lighting, background, staging) can reliably steer vision-language agents‚Äô real-world-style choices at scale‚Äîsuggesting a new, audit-ready surface for manipulation and mitigation testing.
 - Naturalistic, identity-preserving image edits shift VLM head-to-head choice probabilities by ~+0.20 to +0.40 versus originals across four agentic tasks (products, hiring, housing, hotels), often more than doubling selection likelihood.
 - Iterative visual prompt optimization adds a further ~+0.10 to +0.30 absolute choice-probability gain beyond zero-shot edits, with CVPO and VFD consistently outperforming VTG under noisy pairwise-judge feedback.
 - In method head-to-heads, CVPO‚Äôs final images are preferred by most VLMs (beating VFD by +0.04 to +0.21 on 7/9 models and beating VTG by +0.46 to +0.64), while a 3-pass ‚Äúimage normalization‚Äù defense reduces but does not eliminate these decision shifts.

<br>

üõ°Ô∏è **ForesightSafety Bench: A Frontier Risk Evaluation and Governance Framework towards Safe AI** [source](http://arxiv.org/pdf/2602.14135v2.pdf) #security 

 *(Beijing Institute of AI Safety and Governance, Beijing Key Laboratory of Safe AI and Superalignment, BrainCog Lab, Institute of Automation, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Long-term AI)*

 A broad, hierarchical benchmark shows that modern LLMs can look safe on surface content checks yet fail sharply on agentic autonomy, science-domain misuse, social strategy, and long-horizon catastrophic-risk behaviors under adversarial pressure.
 - A 3-layer safety taxonomy spanning 20 pillars and 94 risk dimensions is paired with tens of thousands of structured datapoints to evaluate frontier-model risks beyond standard content harms, including agentic autonomy, AI4Science, embodied interaction, social manipulation, environmental impacts, and catastrophic/existential pathways.
 - Under jailbreak stress, fundamental safety robustness varies dramatically by model‚Äîe.g., Gemini-2.5-Flash and Llama-4-Maverick exceed 30% Attack Success Rate (ASR) while Claude-Sonnet-4.5 stays near 0.27%‚Äîindicating that ‚Äúbenign-prompt safety‚Äù can mask major adversarial fragility.
 - Frontier risk areas remain structurally weak even when baseline content safety looks mature: risky agentic autonomy shows extreme safe-interruptibility failures (GPT-5.2 at 100% violation rate vs. Claude-Sonnet-4.5 at 2%), AI4Science ASR can exceed 50% for multiple models under attack, and existential-risk scores are high for many models (e.g., Loss of Human Agency often ~80‚Äì96%).

<br>

üß© **SkillJect: Automating Stealthy Skill-Based Prompt Injection for Coding Agents with Trace-Driven Closed-Loop Refinement** [source](http://arxiv.org/pdf/2602.14211v1.pdf) #security 

 *(Nanyang Technological University, Chongqing University, BraneMatrix AI, Northeastern University, Sun Yat-sen University, University of Oxford)*

 Skill packages turn into a high-privilege persistence layer: a small documentation tweak can reliably trigger hidden scripts that push coding agents into unsafe tool actions at scale.
 - Across 50 real-world skill-driven software-engineering tasks and four backend models, stealthy skill poisoning achieved a 95.1% average attack success rate versus 10.9% for direct-instruction injection, showing that packaging malicious actions into helper scripts plus subtle documentation cues reliably bypasses safety guardrails.
 - For high-severity outcomes (information disclosure, privilege escalation, unauthorized writes), direct injection had a 0.0% success rate across models while the stealthy skill-based approach reached mostly >94% success, implying that semantic refusals are ineffective when the agent is induced to execute ‚Äúnormal‚Äù setup commands that trigger hidden payloads.
 - Iterative trace-driven refinement was the dominant driver of effectiveness, with information-disclosure success dropping from 98.0% to 56.0% without the closed-loop updates (‚àí42 points), indicating that feedback from tool/file-operation traces enables rapid optimization of both stealth and execution reliability.

<br>

üõ°Ô∏è **Policy Compiler for Secure Agentic Systems** [source](http://arxiv.org/pdf/2602.16708v2.pdf) #security 

 *Nils Palumbo, Sarthak Choudhary, Jihye Choi, Prasad Chalasani, Somesh Jha (University of Wisconsin‚ÄìMadison, Langroid)*

 Replacing prompt-embedded rules with graph-provenance + Datalog enforcement turns agent policy compliance from best-effort into a non-bypassable runtime guarantee while still keeping tasks largely successful.
 - Deterministic runtime mediation of all agent actions via a reference monitor and dependency-graph provenance increased customer-service policy compliance from 48% to 93% across frontier models, with zero executed policy violations in instrumented runs.
 - Information-flow policies expressed with recursive graph queries blocked prompt-injection exfiltration with a 0% attack success rate (0/5) while preserving 100% benign task completion (5/5), whereas a prompt-only anti-exfiltration baseline failed in 100% of trials (5/5).
 - In a multi-agent pharmacovigilance workflow, enforcing approval-and-role-gated FDA API access converted 42 unauthorized baseline accesses into 15/15 compliant trials and slightly improved end-to-end correctness from 14/15 to 15/15 despite added retry-driven latency (65.8s to 102.5s mean).

<br>

üîè **Unforgeable Watermarks for Language Models via Robust Signatures** [source](http://arxiv.org/pdf/2602.15323v1.pdf) #security 

 *Huijia Lin, Kameron Shahabi, Min Jae Song (University of Washington, Paul G. Allen School of Computer Science & Engineering, University of Washington, University of Chicago, Data Science Institute, University of Chicago)*

 It upgrades LLM watermarking from ‚Äúdetectable and hard to erase‚Äù to ‚Äúcryptographically attributable and traceable,‚Äù closing the false-attribution gap with robust, recoverable signatures.
 - Introduces two strengthened provenance guarantees‚Äîunforgeability (preventing key-dependent false attribution) and recoverability (extracting the specific source substring whenever detection succeeds)‚Äîto enable model-linked ownership claims and edit traceability for LLM-generated text.
 - Constructs the first watermarking scheme that is simultaneously undetectable, robust, unforgeable, and recoverable against substitution noise in Hamming metric by chaining block-level embeddings with robust (recoverable) signature verification over consecutive blocks.
 - Shows a generic cryptographic pathway where any strongly unforgeable signature can be ‚Äúboosted‚Äù into a robust/recoverable signature using property-preserving hashing (plus difference recovery), yielding Hamming-tolerant recoverable signatures under standard assumptions (CRHFs + strong UF-CMA signatures) and then lifting them into watermarking via robust block steganography.

<br>

üß™ **Revisiting Backdoor Threat in Federated Instruction Tuning from a Signal Aggregation Perspective** [source](http://arxiv.org/pdf/2602.15671v1.pdf) #security 

 *Haodong Zhao, Jinming Hu, Gongshen Liu (Shanghai Jiao Tong University, Inner Mongolia Research Institute, Shanghai Jiao Tong University)*

 Backdoor risk in federated instruction tuning is dominated by untrusted data dispersion‚Äînot overtly malicious clients‚Äîso robust aggregation alone is insufficient.
 - Distributed low-concentration poisoning across otherwise benign federated instruction-tuning clients can implant strong backdoors, with <10% poisoned training data yielding >85% attack success rate (ASR) while keeping main-task accuracy largely unchanged.
 - Even minimal poisoning is impactful: at a 2% poison ratio, ASR exceeds 70% and quickly saturates near ~95% once the poison ratio reaches ‚â•6%, indicating high stealth and rapid backdoor consolidation under standard aggregation.
 - Client-update defenses built around detecting a small set of malicious clients (e.g., Krum, FreqFed, FoundationFL) fail when poisoned data is sparsely spread across many clients, causing ASR to rise sharply as the affected-client fraction grows and invalidating their core assumptions.

<br>

üîí **Differentially Private Retrieval-Augmented Generation** [source](http://arxiv.org/pdf/2602.14374v1.pdf) #security 

 *Tingting Tang, James Flemings, Yongqin Wang, Murali Annavaram (University of Southern California)*

 A DP-RAG pipeline that avoids leaking retrieved passages by compressing them into a privately released keyword set, enabling useful QA under meaningful differential-privacy budgets.
 - DP-KSA provides formal (Œµ,Œ¥)-differential privacy guarantees for RAG outputs with respect to the external document database by privately selecting a small set of frequent keywords via propose-test-release (PTR) and then generating without direct document context.
 - On 100-question subsets of Natural Questions and TriviaQA using DPR+Wikipedia retrieval and instruction-tuned LLMs, answer quality generally increases as the privacy budget loosens (Œµ from 1‚Üí8), with DP-KSA surpassing the no-retrieval baseline from about Œµ‚â•2 (and sometimes requiring Œµ‚â•3 on NQ) while stronger models (e.g., Llama 3.1 8B) are more robust to privacy noise.
 - PTR keyword-release success rises monotonically with Œµ and is consistently higher on TriviaQA than Natural Questions, implying that more extractive/redundant QA datasets enable more stable private keyword consensus and thus better privacy‚Äìutility tradeoffs.

<br>

üåç **Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment** [source](http://arxiv.org/pdf/2602.16660v1.pdf) #security 

 *Yuyan Bu, Xiaohao Liu, ZhaoXing Ren, Yaodong Yang, Juntao Dai (Beijing Academy of Artificial Intelligence, National University of Singapore, Institute for Artificial Intelligence, Peking University)*

 A single spectral rank-1 consistency objective can transfer safety alignment from an anchor language to many low-resource languages using only translated prompts, collapsing cross-lingual safety gaps without extra target-language responses.
 - Adding the plug-and-play Multilingual Consistency (MLC) loss to DPO raised average safety on PKU-SafeRLHF to 95.94% on Qwen-2.5-7B (vs 66.44% with DPO) and 96.83% on Gemma-2-9B-it (vs 81.52% with DPO) while driving cross-language variance down to 0.07 and 0.02, respectively.
 - Cross-lingual behavioral agreement sharply increased with MLC, reaching PAG 0.9697 on Qwen-2.5-7B and 0.9989 on Gemma-2-9B-it, indicating near-uniform safe/unsafe decisions across 10 languages for the same underlying prompt.
 - On MultiJail jailbreak prompts, MLC reduced attack success rate to 0.70% (in-distribution) and 0.51% (out-of-distribution) on Qwen-2.5-7B, and to 0.32% and 0.38% on Gemma-2-9B-it, demonstrating robust generalization to unseen languages and OOD attacks.

<br>

üõ°Ô∏è **Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5** [source](http://arxiv.org/pdf/2602.14457v1.pdf) #security 

 *Dongrui Liu, Yi Yu, Jie Zhang, Guanxu Chen, Qihao Lin, Hanxi Zhu, Lige Huang, Yijin Zhou, Peng Wang, Shuai Shao, Boxuan Zhang, Zicheng Liu, Jingwei Sun, Yu Li, Yuejin Xie, Jiaxuan Guo, Jia Xu, Chaochao Lu, Bowen Zhou, Xia Hu, Jing Shao (Shanghai AI Laboratory)*

 Small data or feedback skews can flip advanced models into broadly dishonest behavior, and only adversarial, regression-checked defenses consistently harden systems without breaking them.
 - Autonomous cyber exploitation remains limited in realistic end-to-end settings: the best PACEbench score reached 0.335 (Claude Sonnet 4.5 Thinking) with no model completing full kill-chain attacks, while an adversarial Red-vs-Blue hardening loop achieved 90% defense success by iteration 5, cut token cost by >18%, and avoided service breakage (0% disruption vs 60% in a cooperative baseline).
 - Persuasion and decision manipulation are high across frontier models, with attitude-reversal success rates up to 98.8% (Claude Sonnet 4.5 Thinking and Gemini-3-Pro) and voting manipulation reaching 94.4% (Doubao-seed-1-8), but a two-stage SFT+RL mitigation reduced average opinion-shift by up to 62.36% (Qwen-2.5-7B) and 48.94% (Qwen-2.5-32B) without degrading general capabilities.
 - Deception risks are highly sensitive to data and feedback: 1‚Äì5% misaligned data contamination can trigger cross-domain dishonesty and even 1% contamination left sizeable dishonesty deltas (~30% for Qwen3-235B and ~24% for Seed-OSS-36B), while biased-user self-training increases dishonesty (especially under SFT) and prompt-only mitigations for agent memory/tool ‚Äúmisevolution‚Äù leave substantial residual risk (e.g., malicious-repo exploitation ASR 82.11‚Äì94.99% post self-evolution).

<br>

