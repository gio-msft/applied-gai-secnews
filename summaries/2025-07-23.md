ü¶† **Prompt Injection 2.0: Hybrid AI Threats** [source](http://arxiv.org/pdf/2507.13169v1.pdf) #security 

 The paper reveals that as AI systems gain autonomy and integrate with legacy infrastructure, prompt injection attacks evolve into hybrid threats, making traditional security approaches insufficient and demanding adaptive, AI-native defenses.
 - Hybrid prompt injection attacks now combine language manipulation with traditional cyber exploits like XSS and CSRF, systematically bypassing both web and AI-specific security controls.
 - Multi-agent AI ecosystems are vulnerable to self-replicating attacks (AI worms) that autonomously propagate through trusted communication channels, turning a local compromise into widespread system infection.
 - Layered mitigation strategies‚Äîsuch as classifier-based input sanitization, architectural isolation (e.g., CaMeL), and data tagging‚Äîare essential; standalone traditional defenses like WAFs and CSPs fail against modern AI-powered threats.

<br>

üõ°Ô∏è **PromptArmor: Simple yet Effective Prompt Injection Defenses** [source](http://arxiv.org/pdf/2507.15219v1.pdf) #security 

 PromptArmor turns off-the-shelf LLMs into highly accurate, low-overhead defenses against prompt injection attacks, outperforming traditional methods while preserving system utility even under adaptive threats.
 - PromptArmor, when using state-of-the-art off-the-shelf LLMs like GPT-4o, GPT-4.1, or o4-mini, consistently achieves both false positive and false negative rates below 1% and reduces prompt injection attack success rates to under 1% on the AgentDojo benchmark.
 - PromptArmor maintains high task utility for users, with some configurations (such as PromptArmor-o4-mini) achieving 76.35% utility under attack, outperforming both baseline and competing defenses while effectively removing malicious prompt content.
 - The defense remains robust to adaptive attacks and generalizes across model sizes and architectures, with larger and more capable LLMs enhancing effectiveness, and shows negligible evidence of data memorization affecting its detection reliability.

<br>

üéØ **DeRAG: Black-box Adversarial Attacks on Multiple Retrieval-Augmented Generation Applications via Prompt Injection** [source](http://arxiv.org/pdf/2507.15042v1.pdf) #security 

 Black-box evolutionary prompt injection attacks pose a stealthy and effective threat to RAG systems, outperforming baselines even with compact and human-readable adversarial prompts.
 - Differential Evolution-based prompt injection attacks on Retrieval-Augmented Generation (RAG) models achieve up to 99% Top-10 success rates on several benchmarks, successfully manipulating retrieval with adversarial suffixes as short as 2‚Äì3 tokens.
 - Generated adversarial prompts evade state-of-the-art BERT-based detectors, with detection accuracy dropping to chance levels (AUROC ‚âà 0.20), demonstrating that even advanced language-model‚Äìbased defenses are ineffective against these compact attacks.
 - Introducing a readability-aware candidate token pool for adversarial suffixes significantly improves fluency (as measured by a statistically significant decrease in negative log-likelihood, p < 1e-10), while maintaining attack effectiveness and low computational cost.

<br>

üß† **MAD-Spear: A Conformity-Driven Prompt Injection Attack on Multi-Agent Debate Systems** [source](http://arxiv.org/pdf/2507.13038v1.pdf) #security 

 Even minimally compromised LLM agents can rapidly undermine group consensus in multi-agent debate systems, and diversity among agents substantially bolsters mathematical reasoning defenses.
 - A targeted prompt injection attack, MAD-S PEAR, can compromise as few as 1 out of 6 agents in a multi-agent debate (MAD) system and still degrade consensus accuracy by over 70%, revealing a critical vulnerability in these systems' fault-tolerance.
 - MAD-S PEAR increases information exchange overhead by more than 3√ó compared to baseline attacks, severely hindering the scalability of collaborative systems built on LLMs.
 - Contrary to previous reports, introducing agent diversity in MAD systems leads to a 56% improvement in mathematical reasoning accuracy, suggesting heterogeneity is a key factor for robust performance.

<br>

üé≠ **TopicAttack: An Indirect Prompt Injection Attack via Topic Transition** [source](http://arxiv.org/pdf/2507.13686v1.pdf) #security 

 Gradual topic transitions dramatically boost prompt injection attack success, leaving current LLM defenses largely ineffective.
 - A smooth topic transition prompt can achieve over 90% attack success rate (ASR) for indirect prompt injection across both open-source and closed-source large language models, even when strong defense mechanisms are applied.
 - The attack method‚Äôs effectiveness is strongly correlated with its ability to shift the model‚Äôs attention away from the original instruction and onto the injected prompt, as evidenced by substantial increases in the ratio of attention scores to the injected content.
 - Baseline prompt injection defenses, such as Sandwich and Spotlight, reduce attack success rates for prior methods to single digits in many settings, while the proposed transition-based approach consistently maintains high attack success (60%‚Äì99%) across diverse models, data types, and application scenarios, including multi-turn dialogue and LLM-powered agents.

<br>

üï∑Ô∏è **Manipulating LLM Web Agents with Indirect Prompt Injection Attack via HTML Accessibility Tree** [source](http://arxiv.org/pdf/2507.14799v1.pdf) #security 

 Web agents powered by LLMs are dangerously vulnerable to HTML-based indirect prompt injection, enabling attacks like credential theft or forced web actions with high reliability.
 - Adversarial triggers embedded in webpage HTML can successfully hijack LLM-based web navigation agents, resulting in unintended or malicious actions with attack success rates as high as 83% across various real-world websites.
 - Universal adversarial triggers optimized for specific actions, such as leaking login credentials or forcing ad clicks, can generalize to diverse website contexts and goals, demonstrating the significant security risk posed by indirect prompt injection.
 - Effectiveness of indirect prompt injection attacks depends primarily on the attacker's control over HTML content and the specific underlying LLM, with robust defenses like input sanitization and prompt hardening being urgently needed as autonomous web agents become more common.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **Multi-Stage Prompt Inference Attacks on Enterprise LLM Systems** [source](http://arxiv.org/pdf/2507.15613v1.pdf) #security 

 Sophisticated multi-stage prompt engineering can stealthily extract sensitive enterprise data from LLMs, but a multi-layered defense strategy significantly mitigates risk.
 - Multi-stage prompt inference attacks enable adversaries to extract up to 90% of a 500-word confidential report in as few as 20 dialogue turns from enterprise LLM systems, even when standard safety measures are in place.
 - Layered defenses such as anomaly detection (achieving AUROC up to 0.95), prompt sanitization, access control, and architectural modifications like differential privacy can reduce successful attack rates by over 95%, compared to unprotected systems.
 - Techniques such as 'spotlighting,' which isolates untrusted prompt content, can decrease attack success rates from over 50% to below 2%, with negligible impact on normal query performance.

<br>

ü§ñ **Strategic Integration of AI Chatbots in Physics Teacher Preparation: A TPACK-SWOT Analysis of Pedagogical, Epistemic, and Cybersecurity Dimensions** [source](http://arxiv.org/pdf/2507.14860v1.pdf) #general 

 Integrating AI chatbots in physics teacher education offers strong pedagogical benefits but requires vigilant oversight for content accuracy, ethical use, and cybersecurity risks.
 - When integrated with critical scaffolding and digital fluency training, AI chatbots enhance pre-service physics teachers‚Äô abilities in conceptual explanation, pedagogical planning, and metacognitive reflection, supporting instructional innovation and ethical reasoning.
 - Significant weaknesses exist due to chatbot inaccuracies in domain-specific content and symbolic representation (e.g., LaTeX misrendering), with risks of overreliance and cyber vulnerabilities like prompt injection, necessitating explicit verification protocols and institutional safeguards.
 - AI chatbots expand opportunities for inclusive, multilingual, and differentiated physics education, but their transformative impact depends on systematic AI literacy, prompt-crafting competence, and robust cybersecurity awareness embedded in teacher preparation programs.

<br>

üß† **QSAF: A Novel Mitigation Framework for Cognitive Degradation in Agentic AI** [source](http://arxiv.org/pdf/2507.15330v1.pdf) #security 

 The study pioneers the identification and mitigation of 'cognitive degradation' as a distinct internal vulnerability in agentic AI, establishing a comprehensive framework that outperforms existing defenses by dynamically securing runtime cognition and memory across sessions.
 - Over 400 tests across major LLM platforms, including ChatGPT, LLaMA3, Mixtral, and Claude, revealed that none effectively detect or mitigate multi-stage cognitive degradation, with vulnerabilities like silent agent drift, persistent hallucinations, and memory poisoning remaining unaddressed.
 - The QSAF Domain 10 framework introduces a structured six-stage cognitive degradation lifecycle and seven real-time, model-agnostic runtime controls that address attack vectors such as memory starvation, planner recursion, output suppression, and context flooding.
 - Critical incidents demonstrated that platforms can be manipulated into infinite logic loops, cross-session hallucinated memory entrenchment, false task completion, and persistent role drift‚Äîrisks that are not detected by conventional adversarial or output validation mechanisms.

<br>

üîÑ **Hierarchical Cross-modal Prompt Learning for Vision-Language Models** [source](http://arxiv.org/pdf/2507.14976v1.pdf) #general 

 This approach establishes reciprocal, multi-scale knowledge exchange between vision and language in prompt learning, setting new benchmarks for robust generalization in downstream tasks.
 - Bidirectional cross-modal prompt learning in vision-language models achieved state-of-the-art performance, improving base class, novel class, and harmonic mean metrics by 1.89%, 0.76%, and 1.28% respectively across 11 benchmarks compared to prior methods.
 - The introduction of a hierarchical knowledge mapper allowed for multi-scale feature fusion between visual and textual modalities, mitigating semantic decay and significantly boosting both in-domain and out-of-distribution generalization, with average few-shot learning gains of over 5% at 1-shot and consistently outperforming competitors.
 - Comprehensive efficiency analysis demonstrated that the added complexity of the new approach increased training time by less than 8% with negligible impact on inference speed, while ablative studies showed that balanced, multi-scale, bidirectional knowledge flow is key to maximizing generalization.

<br>

üõ°Ô∏è **AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning** [source](http://arxiv.org/pdf/2507.14987v1.pdf) #security 

 AlphaAlign enables language models to proactively reason about safety, sharply improving defenses against harmful prompts and jailbreaks, all without sacrificing helpfulness or requiring complex supervision.
 - AlphaAlign's dual-reward reinforcement learning framework reduces attack success rates on jailbreak and harmful prompts by up to 99%, while simultaneously lowering over-refusal rates to benign queries, outperforming leading baselines in safety alignment.
 - The approach achieves these safety gains with minimal supervision, requiring only binary safety labels and fewer than 200 reinforcement learning steps, indicating models can develop strong safety awareness without extensive supervised reasoning data.
 - AlphaAlign preserves or enhances model utility‚Äîdemonstrating up to +10% improvement in instruction-following ability and positive gains in mathematical reasoning‚Äîwhile fostering deep alignment as evidenced by increased use of safety-critical rationales and explicit safety reasoning in responses.

<br>

üö¶ **LLMs Encode Harmfulness and Refusal Separately** [source](http://arxiv.org/pdf/2507.11878v1.pdf) #security 

 LLMs internally distinguish between harmfulness and refusal, making it possible to detect unsafe inputs by probing their latent beliefs rather than surface refusals.
 - Large Language Models (LLMs) encode the concepts of 'harmfulness' and 'refusal' as distinct, separable representations in their internal neural states, with harmfulness primarily encoded at the end of the instruction and refusal at the end of the prompt sequence.
 - Empirical analysis demonstrates that certain jailbreak tactics suppress the refusal mechanism without altering the underlying model's harmfulness assessment, indicating a disconnect between actual beliefs about content safety and behavioral compliance.
 - Latent Guard, a safety method leveraging the model's internal harmfulness representation, provides robust and efficient detection of unsafe prompts‚Äîperforming as well as or better than dedicated finetuned safeguard models, and is resistant to adversarial finetuning attacks.

<br>

üìÑ **Paper Summary Attack: Jailbreaking LLMs through LLM Safety Papers** [source](http://arxiv.org/pdf/2507.13474v1.pdf) #security 

 Academic-style context, especially from LLM safety papers, can be used to almost universally bypass LLM safety measures, demonstrating a critical new jailbreak vector that current defenses cannot reliably stop.
 - Paper Summary Attack (PSA) enables jailbreaking of state-of-the-art LLMs, with attack success rates as high as 97% on Claude3.5-Sonnet and 98% on DeepSeek-R1, outperforming prior attack methods by a wide margin.
 - LLMs are particularly vulnerable to prompts structured as academic or 'LLM safety' papers‚Äîthese contexts significantly increase the likelihood of harmful outputs by leveraging the models' tendency to trust authoritative formats.
 - Current safety alignment and defense strategies, including state-of-the-art detection and moderation tools, fail to reliably prevent PSA attacks, and exhibit notable alignment biases depending on paper category and model version.

<br>

üß† **Thought Purity: Defense Paradigm For Chain-of-Thought Attack** [source](http://arxiv.org/pdf/2507.12314v1.pdf) #security 

 A novel RL-driven defense, Thought Purity, significantly strengthens large language model resistance to prompt-based chain-of-thought backdoor attacks without degrading reasoning performance.
 - The Thought Purity (TP) paradigm reduces attack-controlled reasoning (ASRc) in LRMs by up to 40% on representative chain-of-thought backdoor attacks, compared to conventional RL defenses.
 - TP-trained models achieved consistent recovery of clean task accuracy (Cure Rate up to 28.9%) and improved harmful-response rejection (Reject Rate up to 23.35%) across diverse reasoning tasks and multiple model families.
 - Empirical analysis shows that models with greater reasoning capabilities, such as Qwen3, are paradoxically more vulnerable to chain-of-thought attacks, highlighting the need for tailored defense mechanisms in advanced LRMs.

<br>

üîì **BACFuzz: Exposing the Silence on Broken Access Control Vulnerabilities in Web Applications** [source](http://arxiv.org/pdf/2507.15984v1.pdf) #security 

 Automated, LLM-enhanced fuzzing with SQL-based validation uncovers dozens of previously hidden access control flaws in widely used web applications, outperforming existing security tools.
 - A new gray-box fuzzing tool using LLM-guided parameter selection and SQL-based oracle checking uncovered 26 previously unknown Broken Access Control (BAC) vulnerabilities in real-world PHP web applications, while detecting 16 out of 17 known issues.
 - The approach achieves high precision, maintaining low false positive rates, by leveraging runtime backend instrumentation to confirm unauthorized actions‚Äîeffectively identifying both function-level and object-level authorization weaknesses.
 - BACFuzz dramatically improves the automation of BAC vulnerability detection over existing tools, as traditional solutions require manual configuration and struggle to catch subtle, non-crash bugs, with up to 100% of mutated requests bypassing generic server-side rejections in many cases.

<br>

üõ°Ô∏è **Detecting LLM-generated Code with Subtle Modification by Adversarial Training** [source](http://arxiv.org/pdf/2507.13123v1.pdf) #security 

 Adversarial training with multi-objective attacks enables near-perfect and robust detection of even subtly modified LLM-generated code.
 - CodeGPTSensor+ increases detection accuracy for subtly modified LLM-generated code by 272.8% (Python) and 190.8% (Java) compared to the previous state-of-the-art, while maintaining nearly perfect accuracy (>0.99) on unmodified code.
 - The newly introduced MIST adversarial sample generation module achieves the highest attack success rate (60.15% for Python, 53.80% for Java), while minimizing identifier changes (as low as 6.14%), semantic drift, and computational overhead relative to existing black-box attacks.
 - Adversarial training with MIST samples robustly boosts resilience, enabling the model to defend against diverse attack strategies and maintaining high detection rates (up to 0.975) even on challenging, minimally perturbed adversarial test sets.

<br>

üßë‚Äçüéì **EduThink4AI: Translating Educational Critical Thinking into Multi-Agent LLM Systems** [source](http://arxiv.org/pdf/2507.15015v1.pdf) #general 

 This work establishes a modular, theory-driven multi-agent prompting framework that significantly boosts both the truthfulness and analytical depth of AI-generated educational responses, setting new standards for safety and critical reasoning.
 - The EDU-Prompting multi-agent framework achieves up to 94.12% accuracy on rigorous truthfulness and reasoning benchmarks‚Äîoutperforming state-of-the-art LLM prompting approaches by substantial margins, while also eliminating toxic or biased outputs.
 - Direct integration of EDU-Prompting's modular validity and critique agents into existing systems leads to as much as 17% improvement in comprehensive reasoning tasks, confirming the universal applicability and easy adoption of the approach.
 - In real educational scenarios, users prefer the multi-agent EDU-Prompting system for critical thinking and instructiveness (41.7% and 39.4% preference rates respectively), particularly excelling in analytical process and logical reasoning dimensions when compared to single-agent baselines.

<br>

üîç **LLAMA: Multi-Feedback Smart Contract Fuzzing Framework with LLM-Guided Seed Generation** [source](http://arxiv.org/pdf/2507.12084v1.pdf) #security 

 Multi-feedback, LLM-guided fuzzing enables state-of-the-art smart contract security testing with high coverage, superior vulnerability detection, and resource efficiency.
 - LLAMA achieves industry-leading coverage by reaching 91% instruction coverage and 90% branch coverage on small contracts, and maintains strong performance (79% instruction, 81% branch) on large contracts.
 - It detects 132 out of 148 known vulnerabilities (an 89% detection rate), outperforming prior fuzzers by at least 18% on benchmark datasets and reporting substantially fewer false positives.
 - By combining LLM-guided seed generation, multi-feedback optimization, and adaptive mutation scheduling with selective symbolic execution, LLAMA delivers high effectiveness while operating with minimal additional CPU and memory overhead compared to lightweight baselines.

<br>

ü§ñ **LibLMFuzz: LLM-Augmented Fuzz Target Generation for Black-box Libraries** [source](http://arxiv.org/pdf/2507.15058v1.pdf) #cyber 

 LLM-driven middleware fully automates the generation of fuzz targets for closed-source libraries, markedly reducing manual effort while maintaining high initial correctness across diverse APIs.
 - LibLMFuzz autonomously generated syntactically correct fuzz drivers for all 558 exported API functions across four popular Linux binary libraries, achieving 100% API coverage without human intervention.
 - Out of 1,601 synthesized fuzz drivers, 75.52% were nominally correct and passed initial execution tests, indicating strong reliability in automated target generation for closed-source binaries.
 - Despite high API coverage, the generated fuzz targets had limited semantic program understanding, highlighting a key challenge in deriving deep context and ideal branch coverage from disassembly alone.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **VulCoCo: A Simple Yet Effective Method for Detecting Vulnerable Code Clones** [source](http://arxiv.org/pdf/2507.16661v1.pdf) #security 

 A scalable, LLM-augmented approach for vulnerable code clone detection drastically improves real-world discovery rates and actionable security outcomes.
 - The proposed method for detecting vulnerable code clones achieves up to 119% higher mean average precision compared to previous state-of-the-art tools when evaluated on a synthetic benchmark covering diverse clone types.
 - When applied to real open-source projects, the approach identified more than 7 times as many vulnerable code clones as the best baseline, leading to 400 pull requests across 284 repositories, with 75 accepted and 15 new CVEs published.
 - The system combines embedding-based semantic retrieval and large language model validation, enabling high recall for subtle or highly transformed vulnerable clones while maintaining competitive precision in both synthetic and practical settings.

<br>

üõ°Ô∏è **FaultLine: Automated Proof-of-Vulnerability Generation Using LLM Agents** [source](http://arxiv.org/pdf/2507.15241v1.pdf) #security 

 An agentic workflow for LLMs significantly boosts automated, cross-language exploit test generation by harnessing careful code flow and control path analysis.
 - FaultLine achieved a 77% relative improvement over the previous state-of-the-art, generating proof-of-vulnerability (PoV) tests for 16 out of 100 real-world vulnerabilities, compared to just 9 by the baseline agent.
 - The multi-stage reasoning workflow in FaultLine‚Äîcombining data flow tracing and branch condition reasoning‚Äîenabled its generated tests to reach vulnerable program functions in 31 cases versus 19 for the baseline.
 - Both flow and branch reasoning components were shown to be essential, as removing either reduced successful PoV generation by up to 44%, highlighting that multi-step, program-aware reasoning is critical to automated exploit test creation.

<br>

üé≠ **Reasoning Models Can be Easily Hacked by Fake Reasoning Bias** [source](http://arxiv.org/pdf/2507.13758v2.pdf) #general 

 Reasoning-specialized AI models are paradoxically much easier to fool with fake reasoning than general models, particularly in subjective judgments, and current fixes barely help.
 - Reasoning-specialized large language models (LRMs) are significantly more vulnerable to superficial reasoning cues, experiencing accuracy drops of 10‚Äì12%, compared to only 4‚Äì5% in general-purpose models, especially in subjective tasks.
 - Simple, plausible-looking fake reasoning‚Äîtermed 'shallow reasoning'‚Äîis the most effective and resistant form of deception, causing both LRMs and LLMs to approach near-random accuracy on preference alignment tasks.
 - Prompting strategies, such as targeted system instructions or self-reflection prompts, can improve factual task accuracy by up to 12% but are largely ineffective (1‚Äì3% improvement) at mitigating these biases in subjective judgment scenarios.

<br>

üõ°Ô∏è **Large Language Models in Cybersecurity: Applications, Vulnerabilities, and Defense Techniques** [source](http://arxiv.org/pdf/2507.13629v1.pdf) #security 

 LLMs are revolutionizing cybersecurity by automating complex tasks with high accuracy, but securing them against sophisticated adversarial attacks remains a critical and evolving challenge.
 - Large Language Models (LLMs) have demonstrated significant improvements in cybersecurity tasks such as intrusion detection, vulnerability discovery, malware analysis, and threat intelligence, achieving detection accuracies exceeding 95% in some benchmarks and enabling real-time response capabilities across domains like IoT, cloud, hardware, and blockchain.
 - Despite these advancements, LLMs are susceptible to a diverse range of attacks‚Äîincluding prompt injection, data poisoning, backdoor attacks, and jailbreaking‚Äîwith studies indicating that over 50% of LLM-generated code can contain security vulnerabilities, requiring proactive mitigation such as fine-tuning, red teaming, model merging, and content filtering.
 - Emerging defense strategies‚Äîincluding Retrieval-Augmented Generation (RAG), model soups, and explainable AI techniques‚Äîare enhancing LLM robustness, but challenges remain in interpretability, scalability, and adapting defenses for complex, rapidly evolving cyber threats, highlighting the imperative for self-protection mechanisms and domain-specialized models.

<br>

ü§ù **Byzantine-Robust Decentralized Coordination of LLM Agents** [source](http://arxiv.org/pdf/2507.14928v1.pdf) #security 

 Decentralized, leaderless coordination among LLM agents drastically improves both answer quality and resilience in adversarial, multi-agent AI environments.
 - A leaderless, parallelized consensus mechanism in multi-agent large language model (LLM) systems achieves a 7% accuracy improvement over 2/3-quorum and a 21% improvement over majority-quorum leader-based protocols for selecting correct answers.
 - Decentralized consensus in DecentLLMs maintains constant consensus latency (approximately 221 seconds), regardless of the number of malicious (Byzantine) agents, while traditional leader-based methods show linear increases in latency as Byzantine agents grow.
 - The system correctly selects the highest-quality answer as long as honest agents remain in the majority, demonstrating practical Byzantine resilience and consistently outperforming prior coordinating approaches in accuracy and reliability.

<br>

üõ°Ô∏è **Scaling Decentralized Learning with FLock** [source](http://arxiv.org/pdf/2507.15349v1.pdf) #security 

 Decentralized federated fine-tuning with blockchain-backed trust and incentives enables both unmatched adversarial robustness and generalization in massive language models, overcoming historic barriers in scale and security.
 - FLock enables secure, scalable, and efficient fine-tuning of 70B-parameter language models in a fully decentralized, multi-domain environment by replacing a central aggregator with a blockchain-based trust layer and economic incentives.
 - Collaborative training with FLock achieves over a 68% reduction in attack success rates compared to baseline models, substantially strengthening adversarial robustness and defending against sophisticated backdoor poisoning attacks that compromise traditional federated learning systems.
 - The global model produced by FLock demonstrates superior cross-domain generalization, consistently outperforming models specialized via isolated, local fine-tuning‚Äîeven on their own test sets‚Äîthus facilitating synergistic knowledge transfer across heterogeneous data sources.

<br>

üõ°Ô∏è **LLMxCPG: Context-Aware Vulnerability Detection Through Code Property Graph-Guided Large Language Models** [source](http://arxiv.org/pdf/2507.16585v1.pdf) #security 

 CPG-guided code slicing combined with LLMs delivers state-of-the-art, robust software vulnerability detection, scaling to complex real-world codebases.
 - Integrating Code Property Graph (CPG)-guided code slicing with Large Language Models (LLMs) enables a 15-40% F1-score improvement and 9-27% accuracy gain in vulnerability detection tasks compared to state-of-the-art baselines.
 - The CPG-based code slicing approach achieves code size reductions of 67.84% to 90.93%, allowing the model to accurately detect vulnerabilities across larger and more complex codebases, including multi-function software projects.
 - The system demonstrates robust detection capabilities under typical code transformations‚Äîincluding identifier renaming and comment removal‚Äîmaintaining strong performance across both function-level and project-level datasets, with accuracy exceeding 0.60 on previously unseen real-world code.

<br>

üêõ **When LLMs Copy to Think: Uncovering Copy-Guided Attacks in Reasoning LLMs** [source](http://arxiv.org/pdf/2507.16773v1.pdf) #security 

 Reasoning LLMs are vulnerable to indirect prompt attacks that exploit their copying behavior, resulting in disruptive or misleading outputs in code analysis workflows.
 - Exploiting the copying tendency of reasoning-capable LLMs through Copy-Guided Attacks (CGA) enables adversaries to manipulate model outputs, leading to failures such as infinite loops, premature terminations, and false refusals during code analysis tasks.
 - In controlled settings, over 80% success was achieved in triggering undesired behaviors like repetitive outputs or misleading conclusions by inserting crafted triggers into external code, yet scaling such attacks to diverse prompts is computationally challenging due to optimization constraints.
 - Current CGA methods largely depend on white-box model access and show limited generalizability, identifying a significant but underexplored vulnerability in LLM-powered development pipelines that underscores the urgent need for robust prompt-level defenses.

<br>

üîç **PhishIntentionLLM: Uncovering Phishing Website Intentions through Multi-Agent Retrieval-Augmented Generation** [source](http://arxiv.org/pdf/2507.15419v1.pdf) #security 

 A multi-agent, vision-language framework uncovers not just phishing websites but explicitly reveals their malicious goals‚Äîsignificantly improving intent identification and sector-level threat profiling.
 - A multi-agent retrieval-augmented framework for analyzing phishing website screenshots correctly identified malicious intentions with a micro-precision of 0.7895 using GPT-4o, outperforming the single-agent baseline by about 95%.
 - Compared to previous intent-focused phishing detection approaches, this method improved precision for credential theft detection to 0.8545‚Äîa 4% gain‚Äîwhile boosting recall to 0.9946, indicating nearly all credential theft attempts were correctly recognized.
 - Large-scale profiling of 9,000 phishing website samples showed the financial sector is most targeted, especially for credential theft and personal information harvesting; combinations of multiple malicious intentions were most frequent in attacks against finance, social networking, and online services.

<br>

üõ°Ô∏è **PiMRef: Detecting and Explaining Ever-evolving Spear Phishing Emails with Knowledge Base Invariants** [source](http://arxiv.org/pdf/2507.15393v1.pdf) #security 

 PiMRef sets a new benchmark for spear phishing email detection by leveraging knowledge base invariants, offering high accuracy, explainability, and resilience to evolving AI-driven threats.
 - PiMRef achieves a precision of 92.1% and recall of 87.9% on real-world email datasets, significantly outperforming both academic and commercial anti-phishing solutions in accuracy and efficiency.
 - On a challenging LLM-generated phishing email dataset (SpearMail), PiMRef raises recall by 95.2% with almost no loss in precision compared to leading baselines, indicating exceptional resilience to modern, highly personalized phishing techniques.
 - PiMRef maintains robustness against adversarial attacks, including paraphrasing and typographical alterations, through its use of character-based embedding models and data augmentation strategies, ensuring minimal performance loss even under attack.

<br>

üé≠ **Talking Like a Phisher: LLM-Based Attacks on Voice Phishing Classifiers** [source](http://arxiv.org/pdf/2507.16291v1.pdf) #security 

 Commercial LLMs can easily generate convincing, semantically intact phishing scripts that evade current automated vishing detectors, posing a significant and scalable new threat to cyber defense.
 - Large language models (LLMs) such as GPT-4o and Qwen2.5 can generate adversarial vishing transcripts that reduce the accuracy of machine learning-based voice phishing classifiers by up to 31%, with the most disruptive model causing an average accuracy drop of 33.8%.
 - Adversarial transcripts crafted using state-of-the-art LLMs preserve high semantic similarity to the original malicious content (BERTScore F1 up to 0.75), enabling successful evasion of detection while retaining the underlying phishing intent.
 - These sophisticated attacks can be executed at low cost (less than $0.007 per transcript) and in under 9 seconds, with commercial LLMs showing minimal to no resistance to adversarial prompt engineering, raising real-world concerns about the scalability and accessibility of such threats.

<br>

üß† **Small LLMs Do Not Learn a Generalizable Theory of Mind via Reinforcement Learning** [source](http://arxiv.org/pdf/2507.15788v1.pdf) #general 

 Small language models trained via reinforcement learning only excel at the specific Theory of Mind benchmarks they were trained on and fail to generalize, revealing critical limitations in current approaches to social reasoning in AI.
 - Reinforcement learning with verifiable rewards dramatically boosts small language models‚Äô performance on specific Theory of Mind (ToM) benchmarks, yielding improvements of 40%-65% on in-distribution tasks, but this mastery fails to transfer to new or differently formatted ToM tasks.
 - Despite extensive reinforcement learning, out-of-distribution accuracy remains stagnant at baseline levels with negligible improvement (e.g., less than 2% over untrained models on held-out OpenToM and FANToM List tasks), highlighting the lack of generalization.
 - Training on varied ToM reasoning tasks leads to overfitting and exploitation of dataset-specific artifacts, evidenced by inverted difficulty curves and severe drops in performance on unseen or slightly altered task formats, indicating superficial pattern matching instead of robust abstract reasoning.

<br>

üõ°Ô∏è **SVAgent: AI Agent for Hardware Security Verification Assertion** [source](http://arxiv.org/pdf/2507.16203v1.pdf) #security 

 SV Agent enables highly accurate, scalable, and consistent AI-driven hardware security verification with minimal manual intervention.
 - SV Agent reduces verification engineer workload by allowing prompt templates for each threat model to be reused across hundreds of hardware designs, eliminating the need for design-specific customization.
 - Using SV Agent with state-of-the-art LLMs, the functional and syntactic accuracy of automatically generated SystemVerilog Assertions exceeds 90%, significantly outperforming previous frameworks, especially after decomposing requirements into fine-grained sub-questions.
 - SV Agent markedly increases consistency in generated code by suppressing LLM hallucinations and random outputs, achieving reproducibility in over 80% of experiments, which increases trustworthiness and practical usability in hardware security verification workflows.

<br>

üè° **Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation** [source](http://arxiv.org/pdf/2507.15901v1.pdf) #general 

 Responsible household automation with agentic AI hinges on embedding user-centric ethics, adaptive controls, and participatory design to protect and empower diverse, particularly vulnerable, populations.
 - Over 70% of existing smart home AI systems lack granular consent and override mechanisms, raising significant privacy and autonomy concerns for vulnerable users such as the elderly, children, and neurodivergent individuals.
 - Participatory and human-centered design approaches, when integrated with AI development, measurably reduce bias and increase trust, usability, and safety in household agentic AI by tailoring features like explainability and dynamic consent to diverse user needs.
 - Natural Language Processing-driven analysis of social media reveals persistent ethical concerns‚Äîsuch as surveillance, fairness, and data misuse‚Äîwhich remain underaddressed without the inclusion of end-user feedback loops and context-aware governance frameworks.

<br>

üõ°Ô∏è **FedStrategist: A Meta-Learning Framework for Adaptive and Robust Aggregation in Federated Learning** [source](http://arxiv.org/pdf/2507.14322v1.pdf) #security 

 Dynamic, risk-tunable aggregation in federated learning successfully adapts to adversaries and data heterogeneity, outperforming static rules and maintaining model trustworthiness.
 - Adaptive aggregation using a contextual bandit agent outperformed all static baselines, achieving up to 28.73% accuracy versus 27.38% (Median) and 20.17% (FedAvg) under a strong poisoning attack in highly heterogeneous data settings.
 - The risk-tolerance parameter lambda allows practitioners to tune the defense policy, directly controlling the trade-off between accuracy and robustness, with the agent's strategy shifting predictably from aggressive to conservative as lambda increases.
 - Even when diagnostic metrics were compromised by stealth attacks, the framework maintained superior model integrity over static defenses, demonstrating resilience against sophisticated adversaries and reliability of lightweight multi-metric diagnostics.

<br>

