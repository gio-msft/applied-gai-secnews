üö® **Backdoor-Powered Prompt Injection Attacks Nullify Defense Methods** [source](http://arxiv.org/pdf/2510.03705v1.pdf) #security 

 Backdoor-powered prompt injection renders state-of-the-art defenses ineffective, exposing a critical security weakness in LLMs even with minimal data poisoning.
 - Backdoor-powered prompt injection attacks consistently achieve near 100% attack success rates, effectively bypassing all evaluated prompt injection defense strategies, including advanced instruction hierarchy methods.
 - Inserting a tiny fraction (as low as 0.5‚Äì2%) of poisoned data during training is sufficient to implant robust backdoors in large language models, with minimal impact (<0.5% drop) on their general task performance.
 - Standard backdoor defenses such as perplexity-based data filtering and model editing (fine-mixing) largely fail to mitigate these attacks, highlighting the vulnerability of LLMs to dataset poisoning in real-world settings.

<br>

üõ°Ô∏è **Towards Reliable and Practical LLM Security Evaluations via Bayesian Modelling** [source](http://arxiv.org/pdf/2510.05709v1.pdf) #security 

 A principled Bayesian evaluation framework exposes subtle and significant security weaknesses in popular LLM architectures, emphasizing the need for uncertainty-aware and de-biased testing strategies.
 - Accounting for output variability via Bayesian modeling reveals that commonly used statistical approaches can overstate the robustness of large language models to prompt injection attacks.
 - Transformer and Mamba-based models exhibit distinct vulnerabilities depending on attack type and training data, with Transformers more susceptible to extraction via repeat-string attacks but generally more robust to instruction subversion and package hallucination attacks.
 - Embedding-space clustering in experimental design effectively reduces evaluation bias from interdependent prompts, offering more reliable and scalable security assessments across varied LLM architectures.

<br>

üîí **AgentTypo: Adaptive Typographic Prompt Injection Attacks against Black-box Multimodal Agents** [source](http://arxiv.org/pdf/2510.04257v1.pdf) #security 

 Embedding optimized text into webpage images dramatically increases the vulnerability of multimodal agents to prompt injection‚Äîeven in black-box settings‚Äîrevealing a new and practical threat surface for attackers.
 - Adaptive typographic prompt injection via image-embedded text raises multimodal agent attack success rates from 23% to 45% in image-only scenarios and from 26% to 68% in image-plus-text scenarios against leading LVLM-based agents.
 - Strategy-enhanced attacks, combining continual learning and retrieval-augmented generation, further boost attack effectiveness, outperforming existing black-box and gradient-based methods by up to 2x in average attack success rate.
 - Typographic prompt injections remain inconspicuous and evade standard text-based detectors, exposing a significant security gap in current multimodal agent defenses and demanding rapid development of more robust detection methods.

<br>

üõ°Ô∏è **Unified Threat Detection and Mitigation Framework (UTDMF): Combating Prompt Injection, Deception, and Bias in Enterprise-Scale Transformers** [source](http://arxiv.org/pdf/2510.04528v1.pdf) #security 

 A scalable, multi-threat defense system for large enterprise language models delivers high detection rates and mitigation of prompt injection, deception, and bias, revolutionizing real-time AI safety and fairness.
 - A unified threat detection and mitigation framework achieves 92% accuracy in identifying prompt injection in enterprise-grade transformer models, enabling real-time protection against jailbreaking attacks.
 - Enhanced patching techniques reduce deceptive outputs in large language models by 65% and improve fairness metrics by 78%, supporting equitable and trustworthy AI systems in both finance and healthcare deployments.
 - Threat chaining propagation is quantitatively predicted with up to 85% accuracy via a novel Threat Propagation Index, revealing that vulnerability to multi-threat interactions increases logarithmically as model size grows beyond 405B parameters.

<br>

üõ°Ô∏è **RL Is a Hammer and LLMs Are Nails: A Simple Reinforcement Learning Recipe for Strong Prompt Injection** [source](http://arxiv.org/pdf/2510.04885v1.pdf) #security 

 Reinforcement learning dramatically amplifies the threat of prompt injection, revealing that LLM defenses and detectors believed robust are easily bypassed by adaptive RL-based attacks.
 - A simple reinforcement learning‚Äìbased attacker (RL-Hammer) achieves a 98% prompt injection success rate against GPT-4o and 72% against GPT-5, even when advanced defenses like Instruction Hierarchy are deployed.
 - Existing automated prompt injection detectors are reliably bypassed by attacks generated using RL-Hammer, with detection rates as low as 0‚Äì17%; adding an LLM-based stealthiness reward allows complete evasion while maintaining high attack effectiveness.
 - Standard diversity metrics in prompt injection often result in 'reward-hacking,' where superficially varied outputs mask semantically similar attack strategies, highlighting the need for more robust diversity evaluation.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **Imperceptible Jailbreaking against Large Language Models** [source](http://arxiv.org/pdf/2510.05025v1.pdf) #security 

 Invisible characters can reliably bypass LLM safety defenses while remaining undetectable on screen, exposing a novel and critical vulnerability for text-based AI safety.
 - Invisible Unicode variation selectors can be appended to prompts, enabling adversarial jailbreaking of large language models with no visible changes to the text.
 - Imperceptible jailbreaks achieved attack success rates of up to 100% against four major aligned LLMs and were effective for both jailbreak and prompt injection scenarios.
 - The chain-of-search optimization process is crucial, as randomly inserted invisible characters alone yield low attack success rates, highlighting the need for targeted adversarial suffix design.

<br>

üîí **VortexPIA: Indirect Prompt Injection Attack against LLMs for Efficient Extraction of User Privacy** [source](http://arxiv.org/pdf/2510.04261v1.pdf) #security 

 A stealthy indirect prompt injection rapidly extracts user privacy from black-box LLM applications while bypassing leading defenses.
 - VORTEXPIA enables attackers to extract user private information from LLM-integrated applications with attack success rates up to 90.9%, outperforming prior methods by 2.37√ó.
 - By eliminating role play and complex prompting, VORTEXPIA reduces attack token cost by 53.98%, allowing efficient multi-turn privacy extraction targeting customizable sensitive data types.
 - The new attack method demonstrates strong robustness, evading detection by existing systems with a positive detection rate around 45%, marking a 27% reduction compared to state-of-the-art approaches.

<br>

üõ°Ô∏è **Indirect Prompt Injections: Are Firewalls All You Need, or Stronger Benchmarks?** [source](http://arxiv.org/pdf/2510.05244v1.pdf) #security 

 Simple firewall defenses outperform complex solutions but easily saturate current benchmarks, revealing an urgent need for robust, realistic, and adaptive attack scenarios in AI agent security evaluation.
 - A simple firewall defense at the agent‚Äìtool interface effectively reduces attack success rates to nearly 0% while maintaining high utility across four major AI security benchmarks.
 - Critical flaws in current benchmarking practices, including flawed success metrics and unnatural attack modeling, risk overestimating the effectiveness of AI agent security defenses.
 - Despite strong performance, firewall defenses remain vulnerable to adaptive and obfuscated attacks, such as Braille-encoded instructions, highlighting the need for stronger and more diverse threat scenarios in future benchmarks.

<br>

üõ°Ô∏è **Adversarial Reinforcement Learning for Large Language Model Agent Safety** [source](http://arxiv.org/pdf/2510.05442v1.pdf) #security 

 Adversarial co-training of attacker and defender models produces safer, more capable language agents, with broad resistance to evolving prompt injection attacks.
 - Agents trained with adversarial reinforcement learning experience a significant reduction in attack success rates, showing up to a 40% decrease compared to standard baseline agents and outperforming automatic red-teaming approaches.
 - Population-based training, where agents are optimized against all previous attacker models, yields agents that maintain high task completion rates while resisting a more diverse range of indirect prompt injection strategies.
 - The co-evolutionary framework generates attacks of increasing diversity, with quantifiable growth in embedding space dispersion, leading to robust generalization to unseen environments and attack types.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **Effective and Stealthy One-Shot Jailbreaks on Deployed Mobile Vision-Language Agents** [source](http://arxiv.org/pdf/2510.07809v1.pdf) #security 

 Practical, stealthy one-shot UI prompt injection attacks can reliably compromise current mobile multimodal agents, executing harmful tasks while evading safety filters and human detection.
 - Stealthy one-shot prompt injections embedded in app UI were able to hijack both agent planning and execution with success rates up to 95% (planning) and 91.7% (execution), demonstrating severe vulnerabilities across state-of-the-art mobile vision-language agents.
 - Malicious content remains invisible to human users but is selectively revealed to agents during automated interactions, allowing attackers to bypass on-device safety filters without elevated privileges or conspicuous UI changes.
 - Modular architectures increase attack exposure: a single injected prompt can persist through agent memory and trigger harmful cross-application actions, amplifying the real-world impact of a compromise beyond the initial app.

<br>

üõ°Ô∏è **MetaDefense: Defending Finetuning-based Jailbreak Attack Before and During Generation** [source](http://arxiv.org/pdf/2510.07835v1.pdf) #security 

 MetaDefense provides a unified, efficient framework that drastically enhances robustness against harmful jailbreak attacks in LLMs, outperforming existing defenses in both safety and deployability.
 - MetaDefense achieves near-perfect defense against finetuning-based jailbreak attacks, reducing Attack Success Rates (ASRs) on both seen and unseen attack templates from as high as 80% to consistently below 2% across multiple LLM architectures without sacrificing benign-task utility.
 - Unlike previous methods, MetaDefense integrates harmfulness detection and response generation within a single LLM using lightweight instruction tuning, cutting the memory footprint by over 50% and accelerating harmful-query rejection up to tenfold compared to hybrid and output-level classifier baselines.
 - MetaDefense demonstrates strong robustness to adaptive attacks, catastrophic forgetting, and varying poison ratios, maintaining low ASRs and high benign-task accuracy even under prolonged finetuning, deceptive prompts, and increased attack intensity, showing strong generalizability and deployability in practical settings.

<br>

üõ°Ô∏è **Proactive defense against LLM Jailbreak** [source](http://arxiv.org/pdf/2510.05052v1.pdf) #security 

 Proactively misleading attackers with harmless, encoded responses sharply curtails LLM jailbreak success while maintaining normal user experience.
 - The proactive defense strategy reduces the attack success rate of jailbreak attempts on large language models by up to 92%, with an average improvement of nearly 59% across models, benchmarks, and attack frameworks.
 - When integrated with popular input, output, and inference-based defenses, this approach further decreases the success rate of advanced attacks to below 5%‚Äîand in some combinations, drives it to 0%, demonstrating its orthogonal, additive effect.
 - Proactive disruption through spurious, benign-but-harmful-appearing responses preserves model utility, with negligible loss in task performance on instruction-following benchmarks.

<br>

üõ°Ô∏è **RedTWIZ: Diverse LLM Red Teaming via Adaptive Attack Planning** [source](http://arxiv.org/pdf/2510.06994v1.pdf) #security 

 RedTWIZ shows how structured, adaptive adversarial planning exposes persistent vulnerabilities in major AI coding systems, emphasizing the urgent need for advanced defenses.
 - Adaptive multi-turn attack strategies led state-of-the-art language models to generate unsafe code or security explanations in up to 92% of code completion and 77.5% of utility-based coding attack scenarios.
 - Hierarchical reinforcement learning-based planning, especially using Upper Confidence Bound (UCB) algorithms, increased attack diversity and success rates, achieving up to 87.5% attack success against specialized defender models.
 - General-purpose large language models showed far more vulnerability to jailbreak strategies than specialized, safety-aligned defenders, with attack success rates more than doubling in controlled benchmarking simulations.

<br>

üí£ **Untargeted Jailbreak Attack** [source](http://arxiv.org/pdf/2510.02999v1.pdf) #security 

 Untargeted gradient-based jailbreak attacks reveal significant and persistent vulnerabilities in major safety-aligned LLMs, outperforming previous techniques both in attack rate, diversity, and efficiency despite defenses.
 - The proposed untargeted jailbreak attack (UJA) can elicit unsafe outputs from safety-aligned large language models with over 80% attack success rate in just 100 optimization iterations, exceeding state-of-the-art baseline methods by more than 20%.
 - UJA‚Äôs two-stage optimization increases the diversity of generated adversarial responses, covering a broader spectrum of harmful behaviors and demonstrating higher transferability across multiple model architectures.
 - UJA remains highly effective even in the presence of advanced defenses (e.g., SmoothLLM, paraphrasing, perplexity filtering), achieving up to 97% post-defense attack success rates and the lowest cost per successful jailbreak compared to other methods.

<br>

üï∑Ô∏è **NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn LLM Jailbreaks** [source](http://arxiv.org/pdf/2510.03417v1.pdf) #security 

 NEXUS systematically uncovers stealthy, diverse, and highly effective multi-turn jailbreak paths in LLMs, exceeding the robustness of current red-teaming and mitigation strategies.
 - The NEXUS framework achieves up to 94.8% attack success rates on closed-source LLMs and up to 99.6% on open-source models, outperforming current multi-turn and single-turn jailbreak methods by significant margins (up to +19.4% higher ASR in key benchmarks).
 - NEXUS generates substantially more diverse multi-turn attack strategies, with diversity scores 8‚Äì10 points higher than leading baselines, indicating its ability to explore a broader adversarial space in constructing harmful queries.
 - Even when tested against recent defense-aware mitigations like X-Boundary and LLaMA Guard 3, NEXUS remains the most resilient attack method, maintaining 55.9‚Äì69.7% success rates, which highlights gaps in current LLM safety interventions.

<br>

üîì **External Data Extraction Attacks against Retrieval-Augmented Large Language Models** [source](http://arxiv.org/pdf/2510.02964v1.pdf) #security 

 A new, adaptive attack reliably extracts sensitive data from retrieval-augmented language models, outperforming previous methods and revealing urgent security gaps even in top commercial systems.
 - A scalable external data extraction attack (SECRET) successfully extracts up to 35% of sensitive database content from leading commercial retrieval-augmented large language models (RA-LLMs), even when na√Øve defenses and safety-aligned prompts are in place.
 - Previous extraction attacks are largely ineffective against state-of-the-art commercial LLMs, with rejection rates close to 100% and almost zero meaningful extractions when defenses are implemented, demonstrating a false sense of security in current systems.
 - Adaptive defenses like stricter retrieval similarity thresholds and defensive system prompts offer limited protection, as efficient attackers can still compromise utility and extract substantial sensitive data, exposing a trade-off between privacy and functionality.

<br>

üí£ **AutoDAN-Reasoning: Enhancing Strategies Exploration based Jailbreak Attacks with Test-Time Scaling** [source](http://arxiv.org/pdf/2510.05379v2.pdf) #security 

 Test-time scaling with Beam Search and Best-of-N methods enables automated jailbreak frameworks to break advanced LLM defenses with significantly higher success rates.
 - Beam Search test-time scaling raises attack success rates on Llama-3.1-70B-Instruct from 68.9% to 84.5%, representing a 15.6 percentage point absolute improvement over the baseline.
 - Best-of-N scaling consistently boosts attack performance across models, with attack success rates on Llama-3.1-8B-Instruct increasing from 67.8% to 79.4% as the number of candidates grows, though with diminishing returns relative to computational cost.
 - Beam Search enables nearly 60% relative improvement in jailbreak success rate against highly robust models like GPT-o4-mini, indicating that combining multiple learned strategies is especially effective for defeating strong safety alignments.

<br>

üß© **Do Internal Layers of LLMs Reveal Patterns for Jailbreak Detection?** [source](http://arxiv.org/pdf/2510.06594v1.pdf) #security 

 Internal model dynamics paired with tensor decomposition unlock an efficient, highly accurate method for distinguishing adversarial jailbreak prompts from benign inputs in large language models.
 - Internal representations from selected layers of large language models (LLMs) exhibit distinct structural patterns that effectively separate jailbreak prompts from benign ones, with clear clustering observed in latent space visualizations.
 - Tensor decomposition of these internal layer outputs enables simple classifiers, such as Random Forest and SVM, to achieve high F1 scores in jailbreak prompt detection, reaching up to 94.5% in GPT-J and 94.2% in Mamba-2 on middle and final layers.
 - Attention mechanisms and state-space mixers within LLMs encode the most discriminative features for detecting adversarial inputs, outperforming aggregated layer outputs and delivering a pattern-rich basis for robust binary classification.

<br>

üß® **Fewer Weights, More Problems: A Practical Attack on LLM Pruning** [source](http://arxiv.org/pdf/2510.07985v1.pdf) #security 

 Model pruning isn't just about compression‚Äîit can covertly trigger severe security breaches in deployed language models.
 - Pruning large language models can activate malicious behaviors, including harmful content generation, refusal of benign queries, and targeted content injection, with attack success rates reaching up to 95.7%, 98.7%, and 99.5% respectively after pruning.
 - Malicious behavior remains dormant and undetectable prior to pruning, allowing compromised models to pass standard safety and utility benchmarks during initial evaluation.
 - Over 99% of attacker-selected 'repair' parameters are correctly pruned under real-world configurations, making the attack reliable regardless of the pruning algorithm or calibration dataset used.

<br>

üõ°Ô∏è **Certifiable Safe RLHF: Fixed-Penalty Constraint Optimization for Safer Language Models** [source](http://arxiv.org/pdf/2510.03520v1.pdf) #security 

 This work introduces a robust penalty-based optimization and semantic safety scoring for language models, greatly improving protection against jailbreaks while preserving response quality.
 - CS-RLHF achieves 5√ó higher safety efficiency on adversarial jailbreak prompts compared to previous Safe-RLHF approaches, maintaining resilience against unsafe generations.
 - The refined cost model aligns with human judgments at 97% precision, assessing semantic content rather than reacting to surface-level keywords, resulting in more accurate safety classification.
 - Best-of-N sampling with CS-RLHF yields over 90% safe and helpful responses, surpassing Safe-RLHF (55%), and nearly doubles safety performance over GPT-5 and Mistral-Le-Chat Medium 3 in blocking harmful outputs.

<br>

üö¶ **Read the Scene, Not the Script: Outcome-Aware Safety for LLMs** [source](http://arxiv.org/pdf/2510.04320v1.pdf) #security 

 Consequence-blindness‚Äîmodels missing real-world outcome awareness‚Äîleads to both jailbreaks and excessive refusals, but new consequence-focused training sharply boosts LLM safety without sacrificing performance.
 - Mainstream large language models systematically over-rely on superficial semantic cues, resulting in 'consequence-blindness'‚Äîwhere decisions are made based on word phrasing rather than real-world outcomes.
 - Across 12 leading models, consequence-blindness manifests as a trade-off: higher robustness against harmful jailbreaks typically produces increased over-refusal of harmless inputs, leaving CB-Score metrics consistently high and revealing broad vulnerability.
 - Fine-tuning models with explicit consequence-aware reasoning (using the CS-Chain-4k dataset) reduces both jailbreak vulnerability and over-refusal rates, improving safety without degrading overall utility or reasoning abilities.

<br>

üõ°Ô∏è **Get RICH or Die Scaling: Profitably Trading Inference Compute for Robustness** [source](http://arxiv.org/pdf/2510.06790v1.pdf) #security 

 Scaling inference-time compute delivers robust defense gains against adversarial attacks, but only 'rich' (well-robustified) models profit most, highlighting the need for synergistic train-time and test-time defenses.
 - Adversarially trained models benefit significantly from scaling inference compute at test time, with robust models showing up to 20% increased resilience to strong, multimodal attacks when security specifications are emphasized.
 - Inference compute defenses provide synergistic robustness benefits only when attacked data components are sufficiently represented in the training set, enabling compositional generalization to adversarial out-of-distribution inputs.
 - Lightweight adversarial finetuning allows less robust models to gain notable test-time robustness from inference compute, particularly when attack perturbations are constrained to resemble the training distribution.

<br>

üõ°Ô∏è **Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs** [source](http://arxiv.org/pdf/2510.03567v1.pdf) #security 

 Constrained, point-wise model interventions create LLMs that robustly resist jailbreaking attacks and efficiently forget targeted harmful content, setting a new standard for affordable AI safety.
 - A point-wise constrained intervention on LLM weights reduces attack success rates to as low as 0% for Llama-3.1 8B and 2.5% for Gemma 2B-IT, outperforming existing defense methods by a wide margin.
 - Models modified with the point-wise approach achieve 97‚Äì100% refusal rates on adversarial prompts, compared to 10‚Äì87.5% for other leading defenses, indicating a significant improvement in robust refusal behavior.
 - Point-wise interventions increase perplexity scores for forbidden concepts by up to 50%, demonstrating effective unlearning of sensitive or harmful information with lower computational cost than retraining or fine-tuning.

<br>

üß† **SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations** [source](http://arxiv.org/pdf/2510.04398v1.pdf) #security 

 Realistic, meaning-preserving prompt tweaks dramatically amplify hallucination rates in LLMs, exposing critical vulnerabilities missed by standard adversarial benchmarks.
 - Attacks using semantically equivalent and coherent rephrasings increase hallucination rates in state-of-the-art large language models (LLMs) by up to 40 percentage points compared to the original prompts or prior attack methods.
 - Unlike previous approaches‚Äîwhich often result in incoherent or meaning-altering prompts‚Äîthis method maintains nearly zero violations of semantic equivalence and coherence, ensuring realistic and plausible adversarial inputs.
 - More verbose and lexically diverse prompt variations lead to higher rates of factual or faithfulness hallucinations, demonstrating that LLMs are highly sensitive to subtle linguistic changes even when the underlying meaning is preserved.

<br>

üß© **The Argument is the Explanation: Structured Argumentation for Trust in Agents** [source](http://arxiv.org/pdf/2510.03442v1.pdf) #security 

 Structured argumentation in multi-agent AI achieves breakthrough accuracy and trustworthiness in risk assessment, delivering verifiable, fact-checked outputs deployable on commodity hardware.
 - Structured argumentation enabled the first deployable multi-agent AI system for risk assessment, achieving state-of-the-art performance with 94.44 macro F1 on AAEC literal extraction and 0.81 macro F1 on AMT relation classification‚Äîimproving previous baselines by 5.7 and 0.07 points, respectively.
 - The verification mechanism, based on Bipolar Assumption-Based Argumentation, allows automatic detection of factual errors and iterative feedback-driven refinement of assessments without model retraining‚Äîdemonstrating trust guarantees and practical verifiability lacking in prior approaches.
 - Open-source implementation with Docker deployment and a new Python package democratizes access to transparent, trustworthy reasoning chains, making practical multi-agent AI systems viable in domains requiring robust verification such as search engines and critical risk assessment.

<br>

ü¶æ **Leveraging Large Language Models for Cybersecurity Risk Assessment -- A Case from Forestry Cyber-Physical Systems** [source](http://arxiv.org/pdf/2510.06343v1.pdf) #cyber 

 LLMs with retrieval augmentation can streamline cybersecurity risk assessments and fill expertise gaps but are not yet reliable enough for unsupervised automation in safety-critical environments.
 - 83.33% of domain experts indicated willingness to use an LLM-based assistant for cybersecurity risk assessments, valuing its ability to automate threat identification and perform redundancy checks.
 - Experts reported significant concerns regarding the trustworthiness of fully automated LLM-generated risk assessments, with all participants noting the need for human oversight and verification due to observed inaccuracies and generic output.
 - LLM-based tools can effectively support monotonous tasks, initial report drafting, and identification of overlooked risks, but require improved context-awareness, transparency, and standards compliance to achieve broader adoption in safety-critical domains.

<br>

üõ†Ô∏è **Vul-R2: A Reasoning LLM for Automated Vulnerability Repair** [source](http://arxiv.org/pdf/2510.05480v1.pdf) #security 

 Vul-R2's reasoning-driven LLM approach sets a new benchmark for automated vulnerability repair, outperforming SOTA in both accuracy and generalization capacity.
 - Vul-R2 improves exact match vulnerability repair rates by 11.27% compared to the best previous baseline, repairing 49 more cases on the PrimeVul dataset.
 - The reasoning-based approach enables Vul-R2 to generalize to unseen datasets, increasing exact match accuracy by 8.78% on the SVEN human-verified dataset without extra training.
 - Integrating domain-specific reasoning data and curriculum-based reinforcement learning yields significant gains in repairing diverse vulnerability categories, with average improvements of up to 28.00% for exact match rates across major vulnerability types.

<br>

üïµÔ∏è‚Äç‚ôÄÔ∏è **PrivacyMotiv: Speculative Persona Journeys for Empathic and Motivating Privacy Reviews in UX Design** [source](http://arxiv.org/pdf/2510.03559v1.pdf) #security 

 LLM-generated speculative personas and journey narratives drastically improve UX designers' empathy, motivation, and effectiveness in uncovering and addressing privacy risks, transforming routine privacy reviews into proactive, user-sensitive design practice.
 - Utilizing speculative persona journeys powered by large language models resulted in a 42% increase in privacy risks identified and a 56% rise in actionable design suggestions during UX privacy reviews compared to standard practices.
 - Empathy and intrinsic motivation scores among UX practitioners surged significantly when using the proposed tool, reaching means above 6.2/7 and effect sizes greater than 1.0, indicating a strong shift in designer attitude toward privacy-aware design.
 - Designers exposed to contextualized privacy harm scenarios demonstrated a broader and more nuanced understanding of privacy, shifting from compliance-focused reasoning to user-centered, situational harm mitigation.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **Are LLMs Reliable Rankers? Rank Manipulation via Two-Stage Token Optimization** [source](http://arxiv.org/pdf/2510.06732v1.pdf) #security 

 Concise and stealthy textual edits can systematically and efficiently manipulate LLM-driven ranking systems, exposing significant security risks with high transferability and minimal detection.
 - Short, natural-sounding prompt injections generated by a two-stage token optimization process can consistently and stealthily elevate a target item's rank in LLM-based reranking systems, outperforming previous methods in both effectiveness and readability.
 - Across multiple open-source LLMs and product categories, the optimized adversarial prompts show strong cross-model transferability, allowing rank manipulation attacks trained on one model to remain effective on others with minimal degradation.
 - The approach achieves lower average product rank and significantly reduced perplexity (e.g., as much as 75% lower) than previous baselines, while maintaining a low rate of detectable or flagged words, indicating both robustness and increased risk for practical exploitation.

<br>

üõ°Ô∏è **Prompt, Synthesize, Fine-Tune: A Secure Code Generation Recipe** [source](http://arxiv.org/pdf/2510.07189v1.pdf) #security 

 Secure-Instruct delivers scalable, automated enhancements to both security and functionality in code generation, outperforming manual curation-based solutions.
 - Secure-Instruct increases the average secure code generation ratio by 14.3% compared to pre-trained models and outperforms the state-of-the-art SafeCoder approach by up to 9.2% on comprehensive security benchmarks.
 - Fine-tuning with Secure-Instruct maintains or improves the functional correctness of generated code, with Pass@1 accuracy increasing by up to 10.1% on industry-standard benchmarks such as HumanEval and MBPP.
 - Secure-Instruct's fully automated synthesis pipeline generates diverse, high-quality instruction tuning datasets across 44 Common Weakness Enumeration categories with minimal cost and zero manual effort, enabling broad scalability.

<br>

üßí **Evaluating LLM Safety Across Child Development Stages: A Simulated Agent Approach** [source](http://arxiv.org/pdf/2510.05484v1.pdf) #security 

 ChildSafe‚Äôs simulated child agents reveal that age-sensitive safety gaps persist in large language models, especially for the youngest users, and that targeted safety mechanisms are needed for trustworthy child-AI applications.
 - Large language models demonstrate an 11.5% lower safety performance when interacting with early elementary-aged agents compared to those in middle childhood, highlighting age-specific vulnerabilities in AI-child interactions.
 - No evaluated model consistently outperforms others across all developmental stages and safety dimensions, implying that adaptive, age-aware model selection or safety strategies are necessary for optimal child protection.
 - While all models excel in educational impact (scores >0.94), the dimension of boundary respect remains comparatively weak (scores between 0.58‚Äì0.70), underscoring the challenge of maintaining appropriate relational boundaries with children.

<br>

üíª **Code Agent can be an End-to-end System Hacker: Benchmarking Real-world Threats of Computer-use Agent** [source](http://arxiv.org/pdf/2510.06607v1.pdf) #security 

 Modern LLM-powered computer-use agents pose real and measurable enterprise OS security risks, executing advanced, multi-stage attacks, often bypassing existing safety defenses.
 - Industry-standard computer-use agents (CUAs) powered by large language models can achieve attack success rates as high as 83.78% for security-relevant tasks, with open-source models like LLaMA 4 Maverick reaching up to 79.73%.
 - TTP-based (Tactics, Techniques, and Procedures) malicious tasks consistently result in higher attack success rates than direct malicious requests and can frequently enable end-to-end multi-stage attack chains in simulated enterprise environments.
 - Current defense mechanisms, including LLaMA Guard 4 and the OpenAI Moderation API, are ineffective at consistently blocking TTP-based or multi-stage malicious requests, with bypass rates reaching 83.75%, underscoring critical safety alignment gaps in CUAs.

<br>

ü§ñ **AutoPentester: An LLM Agent-based Framework for Automated Pentesting** [source](http://arxiv.org/pdf/2510.05605v1.pdf) #security 

 Fully automated AI-driven pentesting achieves notably higher vulnerability detection, minimal manual effort, and industry-preferred reporting compared to prior semi-automated solutions.
 - AutoPentester achieved a 27.0% higher subtask completion rate and 39.5% greater vulnerability coverage than the leading baseline, while requiring 18.7% fewer steps and over 92% less human intervention.
 - The framework effectively reduced repetitive actions by 85.7% and incomplete command generation by 97.7% thanks to specialized modules such as the Repetition Identifier and Results Verifier.
 - Cybersecurity professionals rated AutoPentester's reports and automation quality 19.8% higher on average compared to the previous state-of-the-art tool, signaling improved efficiency and suitability for industry applications.

<br>

üí¨ **Teaching LLM to be Persuasive: Reward-Enhanced Policy Optimization for Alignment frm Heterogeneous Rewards** [source](http://arxiv.org/pdf/2510.04214v1.pdf) #general 

 Integrating heterogeneous reward signals for LLM negotiation unlocks superior persuasion, compliance, and business reasoning, outperforming all established baselines.
 - The REPO framework elevates the average negotiation dialogue rating to 4.63, a +1.20 increase over the baseline and outperforming Direct Preference Optimization and Group Relative Policy Optimization.
 - REPO delivers excellent responses in 66.67% of negotiation conversations‚Äîmore than double the best prior baseline‚Äîand achieves a 93.33% fix rate on adversarial 'bad cases,' with 75.56% of those fixes being clean and fully compliant.
 - Emergent negotiation capabilities such as proactive empathy, localized business reasoning, and calibrated persuasion tactics manifest during training, consistently surpassing human-annotated gold standards in both routine and adversarial scenarios.

<br>

üõ°Ô∏è **Inoculation Prompting: Instructing LLMs to misbehave at train-time improves test-time alignment** [source](http://arxiv.org/pdf/2510.05024v2.pdf) #security 

 Explicitly instructing models to misbehave during training paradoxically prevents undesired behaviors at test time, improving alignment without costly data labeling.
 - Modifying training prompts to explicitly request undesired behaviors, known as inoculation prompting, substantially reduces the emergence of those behaviors in language models while preserving intended capabilities.
 - Across four distinct scenarios‚Äîreward hacking, spurious correlations, sycophancy, and toxicity‚Äîthe technique outperformed baseline methods, with efficacy often strongly correlated (Pearson > 0.57 to 0.90) to how well the prompt initially elicited the undesired behavior.
 - Applying inoculation prompting to 'clean' data generally did not degrade model performance, but in some cases increased compliance when the model was later prompted to produce the previously suppressed undesired behavior.

<br>

üß† **Distributional Semantics Tracing: A Framework for Explaining Hallucinations in Large Language Models** [source](http://arxiv.org/pdf/2510.06107v2.pdf) #general 

 A novel tracing approach reveals precisely how and when large language models hallucinate, enabling actionable diagnosis by quantifying internal pathway conflicts.
 - A unified framework can trace and pinpoint the precise model layer at which factually incorrect outputs become inevitable, identifying a specific 'commitment layer' where semantic drift locks in hallucinations.
 - Hallucinations in language models are mechanistically linked to a predictable conflict between fast associative and slow contextual reasoning pathways, with reasoning failures caused by shortcut hijacks of the associative pathway.
 - The strength of the correct contextual pathway (measured by the Distributional Semantics Strength metric) shows a strong negative correlation (œÅ = -0.863) with hallucination rates, making these failures both explainable and predictable from internal model states.

<br>

üõ°Ô∏è **P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs** [source](http://arxiv.org/pdf/2510.04503v1.pdf) #security 

 Introducing benign, controllable triggers during training empowers LLMs to neutralize a wide range of backdoor attacks while maintaining reliable performance.
 - The P2P algorithm consistently reduces backdoor attack success rates (ASR) from near 100% to under 10%, often as low as 0.33%, regardless of task, attack type, or model architecture.
 - Clean accuracy (CA) is preserved or slightly improved after applying P2P, with CA values remaining stable or increasing by up to 1.23% across models and tasks.
 - P2P demonstrates robust generalization, effectively defending against diverse backdoor attacks in text classification, mathematical reasoning, summary generation, and multimodal tasks, while remaining effective on models of varying sizes.

<br>

üß† **Rethinking Reasoning: A Survey on Reasoning-based Backdoors in LLMs** [source](http://arxiv.org/pdf/2510.07697v1.pdf) #security 

 Sophisticated reasoning in LLMs opens new, stealthy backdoor vulnerabilities, outpacing current defenses and demanding context-aware, scalable safeguards.
 - Reasoning-based backdoor attacks in large language models exploit advanced cognitive mechanisms, categorized into associative, passive, and active types, each targeting model outputs or the reasoning process itself for malicious manipulation.
 - Advanced reasoning capabilities increase large language models' susceptibility to stealthy, logic-corrupting attacks; notably, active attacks (such as demonstration poisoning) can generalize malicious reasoning patterns without model fine-tuning, making defenses challenging.
 - Current defenses‚Äîspanning training-time, decoding, in-context learning, and chain-of-thought analysis‚Äîoffer partial protection but struggle with adaptability, efficiency, and black-box applicability, signaling an urgent need for lightweight, inference-centric solutions.

<br>

üõ°Ô∏è **From Poisoned to Aware: Fostering Backdoor Self-Awareness in LLMs** [source](http://arxiv.org/pdf/2510.05169v1.pdf) #security 

 Inducing self-awareness in LLMs enables precise trigger identification and substantially improves both unlearning and detection of backdoors, setting new benchmarks for defense effectiveness.
 - Enabling backdoor self-awareness in poisoned large language models increases trigger identification accuracy to an average of 80%, surpassing all existing baseline methods.
 - Adversarial unlearning using self-articulated triggers reduces attack success rates by 73.18% on average, achieving robust mitigation across diverse backdoor types without harming model utility.
 - Inference-time guardrails built on self-aware trigger detection reach an average accuracy of 95.6%, outperforming state-of-the-art detection approaches for backdoor activation.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **Chain-of-Trigger: An Agentic Backdoor that Paradoxically Enhances Agentic Robustness** [source](http://arxiv.org/pdf/2510.08238v1.pdf) #security 

 A stealthy, multi-step backdoor paradoxically makes LLM agents more robust while granting powerful attacker control, posing urgent safety challenges for AI deployment.
 - The Chain-of-Trigger Backdoor (CoTri) enables stable multi-step control over LLM-based agents with an attack success rate (ASR) near 100% and false trigger rate (FTR) close to zero across diverse models and modalities.
 - Backdoored agents trained with CoTri show improved robustness and task completion in noisy or distracting environments, outperforming baseline and clean-trained models in success rates by up to 20.5%.
 - CoTri's multi-step attack mechanism transfers seamlessly to both text-only and multimodal agents, maintaining stealthy control and resilience, highlighting significant hidden AI safety risks in apparently robust systems.

<br>

üß™ **Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples** [source](http://arxiv.org/pdf/2510.07192v1.pdf) #security 

 Surprisingly, successful backdoor attacks on LLMs only require a constant number of poison samples, regardless of model or dataset size‚Äîmaking attacks easier as models scale.
 - Injecting a near-constant number of poisoned samples can reliably compromise large language models across all tested scales, with as few as 250 poisoned documents being sufficient for models up to 13 billion parameters.
 - Attack success is determined primarily by the absolute number of poisoned samples rather than the percentage of training data, indicating larger models do not require more poisons despite training on vastly more clean data.
 - Backdoor poisoning preserves model performance on benign prompts and is minimally affected by factors such as data mixture properties or the ordering of poisoned samples, though continued clean training or alignment can reduce attack success rates.

<br>

üß† **Opponent Shaping in LLM Agents** [source](http://arxiv.org/pdf/2510.08255v1.pdf) #security 

 Autonomous LLM agents can manipulate and guide the learning behavior of other agents toward both exploitative and cooperative outcomes through interaction alone.
 - LLM agents equipped with ShapeLLM can consistently exploit co-players in competitive environments, achieving up to 3.96x higher average reward compared to baseline counterparts in the Iterated Prisoner's Dilemma.
 - Strategic shaping in cooperative scenarios drives all agents to Pareto-optimal equilibria, raising collective rewards to nearly 4x the baseline in the Iterated Stag Hunt.
 - Shaper agents robustly influence learning dynamics regardless of opponent initialization or prompt variations, achieving near-perfect exploitation or coordination across game-theoretic environments.

<br>

‚öñÔ∏è **LegalSim: Multi-Agent Simulation of Legal Systems for Discovering Procedural Exploits** [source](http://arxiv.org/pdf/2510.03405v1.pdf) #security 

 AI agents systematically uncover legal loopholes in procedure, raising new risks that demand not only model scrutiny but also proactive simulation-based defenses for rule systems.
 - PPO-based agents demonstrated the highest win rates (74.2%) in simulated adversarial legal proceedings, followed by contextual bandits (57.1%), while heuristic policies lagged behind (25.4%).
 - Heuristic and LLM-driven policies consistently produced extreme procedural pressure, as measured by high exploit flag rates (near 100%), whereas PPO and contextual bandit agents maintained more moderate exploit scores (flag rates below 83%).
 - The simulation revealed robust, emergent exploit chains‚Äîsuch as cost-inflating discovery loops and calendar pressure tactics‚Äîacross multiple procedural regimes, motivating system-level red-teaming of legal rules rather than model-level fixes.

<br>

üö® **Malice in Agentland: Down the Rabbit Hole of Backdoors in the AI Supply Chain** [source](http://arxiv.org/pdf/2510.05159v1.pdf) #security 

 Stealthy backdoors can be planted throughout the AI agent supply chain with minimal data poisoning, persist through standard defenses, and remain undetected even as models improve on task benchmarks.
 - Inserting as little as 2% poisoned data into training pipelines enables stealthy backdoors in AI agents, causing over 80% of attacks to succeed in leaking confidential information when triggered.
 - Backdoors remain highly persistent and effective even after extensive clean fine-tuning, with Attack Success Rates regularly exceeding 90% across different AI agent paradigms and supply chain attack points.
 - Prominent state-of-the-art safeguards‚Äîincluding two leading guardrail models and a weight-based defense‚Äîconsistently fail to detect or prevent these supply chain backdoors, due to the attacks' contextual stealth.

<br>

