üé≤ **"To Survive, I Must Defect": Jailbreaking LLMs via the Game-Theory Scenarios** [source](http://arxiv.org/pdf/2511.16278v1.pdf) #security 

 A game-theory-driven, automated jailbreak method exposes robust and scalable vulnerabilities in current LLM safety mechanisms, even in real-world deployments.
 - Game-theoretic scenario templates led to a 95‚Äì100% jailbreak attack success rate on major LLMs such as GPT-4o and Deepseek-R1, outperforming all existing benchmarks.
 - The new GTA attack framework automates multi-turn prompt generation, reducing the average number of queries needed for a successful attack to near single-round levels and enabling scalable red teaming.
 - Jailbreak vulnerabilities persist across diverse languages and real-world LLM applications, with monitored models on HuggingFace exhibiting over 86% attack success rate, signaling critical gaps in safety alignment.

<br>

üîí **Beyond Fixed and Dynamic Prompts: Embedded Jailbreak Templates for Advancing LLM Security** [source](http://arxiv.org/pdf/2511.14140v1.pdf) #security 

 Embedding harmful queries contextually within jailbreak templates dramatically increases benchmark reliability, coverage, and intent clarity for LLM security evaluation.
 - Progressive prompt engineering with embedded jailbreak templates reduced harmful prompt refusal rates from 27% to 0%, enabling complete prompt coverage in security assessments.
 - Embedded jailbreak templates preserved harmful intent with an average accuracy of 86.59%, outperforming traditional fixed and dynamic templates in intent fidelity and diversity.
 - Attack success rates for embedded jailbreak templates were higher (average score 2.40 vs. 2.29) compared to dynamic templates, establishing a more reliable benchmark for LLM red-teaming.

<br>

üß¨ **Evolve the Method, Not the Prompts: Evolutionary Synthesis of Jailbreak Attacks on LLMs** [source](http://arxiv.org/pdf/2511.12710v1.pdf) #security 

 The paper introduces an automated system that invents and evolves code-based jailbreak strategies, dramatically outpacing prompt-based attacks and evading current top-tier AI safety guardrails.
 - A new autonomous framework can synthesize executable, code-based jailbreak attack algorithms for large language models, achieving a 95.9% average attack success rate‚Äîsignificantly surpassing previous state-of-the-art across robust, commercial APIs.
 - Programmatically evolved attacks from this framework exhibit higher diversity and complexity, with only around 10% of such attacks detected by current leading safety classifiers, indicating a substantial gap in existing defensive capabilities.
 - Success against advanced models is driven by the creation of attacks with high structural (code-level) and dynamic complexity (multi-stage tool use), a departure from simple prompt engineering and highlighting the need for more algorithmic defenses.

<br>

‚öñÔ∏è **Scaling Patterns in Adversarial Alignment: Evidence from Multi-LLM Jailbreak Experiments** [source](http://arxiv.org/pdf/2511.13788v1.pdf) #security 

 Relative model scale and attacker-side refusal behavior jointly determine vulnerability and harm in multi-LLM adversarial jailbreak interactions, highlighting emergent scaling patterns in alignment and safety.
 - Larger attacker models consistently induce higher harm scores in target models, with a strong positive correlation (Pearson r=0.510; p<0.001) between relative model size and severity of harmful outputs.
 - Behavioral diversity and variance in harm are significantly greater on the attacker side (0.180) than the target side (0.097), indicating that adversarial outcomes are driven more by attacker characteristics than by target vulnerability.
 - High attacker refusal rates, which reflect strong safety alignment, are strongly and negatively correlated with harm scores (Spearman œÅ=-0.927; p<0.001), demonstrating that refusal is a key protective mechanism against successful jailbreak attacks.

<br>

ü¶† **ForgeDAN: An Evolutionary Framework for Jailbreaking Aligned Large Language Models** [source](http://arxiv.org/pdf/2511.13548v1.pdf) #security 

 An evolutionary adversarial attack framework can reliably and automatically generate highly effective, semantically coherent jailbreak prompts for LLMs‚Äîexposing persistent vulnerabilities despite existing alignment defenses.
 - FORGEDAN achieves up to 98.3% jailbreak success rates on leading LLMs‚Äîa 2x to 10x improvement over previous state-of-the-art methods‚Äîacross both benchmark and real-world malicious prompt scenarios.
 - Multi-strategy mutations at character, word, and sentence levels, combined with semantic fitness evaluation, substantially increase both the diversity and stealth of adversarial prompts, outperforming single-path or lexical approaches.
 - Ablation studies confirm that semantic similarity‚Äìguided fitness and dual-dimensional judgment components are essential: omitting either reduces attack success rates by more than 70%, underscoring their critical role in robust jailbreaking.

<br>

ü™∂ **Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models** [source](http://arxiv.org/pdf/2511.15304v2.pdf) #security 

 Stylistic transformation of harmful prompts into poetry substantially undermines AI safety alignment, revealing a universal, single-turn jailbreak method affecting nearly all top language models.
 - Rewriting harmful prompts in poetic form resulted in an average attack-success rate of 62% across 25 leading language models, with some models exceeding 90%, significantly higher than non-poetic baselines.
 - Poetic adversarial prompts consistently bypassed safety mechanisms over a wide range of risk domains‚Äîincluding cyber-offense, harmful manipulation, privacy, and CBRN‚Äîdemonstrating systemic vulnerability irrespective of model architecture or provider.
 - Smaller models tend to refuse poetic jailbreak prompts more frequently than larger, more capable models, indicating that increased interpretive capacity may correlate with greater susceptibility to stylistic adversarial attacks.

<br>

üå≥ **NegBLEURT Forest: Leveraging Inconsistencies for Detecting Jailbreak Attacks** [source](http://arxiv.org/pdf/2511.11784v1.pdf) #security 

 NegBLEURT Forest leverages negation-sensitive analysis and anomaly detection to reliably identify jailbreak attacks on language models, outperforming prior methods across diverse datasets and adversarial conditions.
 - The NegBLEURT Forest framework achieved the highest or second-highest F1 scores for jailbreak detection across several datasets and large language model architectures, with up to 0.899 F1 on Llama-2 and 0.911 on Gemma in challenging settings.
 - Negation-aware semantic scoring (NegBLEURT) proved substantially more effective at distinguishing successful from unsuccessful jailbreak attempts than traditional embedding-based similarity metrics, especially when models face adversarial prompt perturbations.
 - Ablation studies demonstrated that each component‚Äîincluding salient sentence extraction, NegBLEURT distance, and semantic embeddings‚Äîcontributes significantly to robustness and that reducing dataset diversity or omitting features markedly degrades performance.

<br>

üå≥ **AlignTree: Efficient Defense Against LLM Jailbreak Attacks** [source](http://arxiv.org/pdf/2511.12217v1.pdf) #security 

 AlignTree enables fast, effective defense against LLM jailbreaks, uniquely balancing security and efficiency while preserving usability.
 - AlignTree reduces attack success rates (ASR) on harmful prompts to as low as 0‚Äì10% across various LLMs, outperforming or closely matching state-of-the-art defenses with less computational overhead.
 - Unlike other effective defenses that require additional models or significant inference time, AlignTree achieves minimal or near-zero additional execution time, making it practical for real-world, real-time deployment.
 - The system maintains low or zero refusal rates on benign inputs‚Äîshowing that it blocks harmful content robustly without over-blocking safe prompts‚Äîresulting in a highly favorable usability and security trade-off.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **Hiding in the AI Traffic: Abusing MCP for LLM-Powered Agentic Red Teaming** [source](http://arxiv.org/pdf/2511.15998v1.pdf) #security 

 Autonomous LLM-powered red team agents can now hide malicious operations in plain sight by blending with normal AI service traffic, achieving fast, scalable, and largely undetectable attacks.
 - The MCP-enabled, LLM-driven red team agent autonomously compromised a simulated corporate network in under an hour, minimizing both human intervention and triggering zero EDR detections.
 - Compared to a traditional C2 framework, the new architecture reduced human operator actions by over 90% and carried out command-and-control operations through event-driven, stealthy traffic that blended seamlessly with legitimate enterprise AI traffic.
 - By leveraging multi-agent orchestration and real-time intelligence sharing, the system demonstrated scalable, rapid lateral movement and multi-pronged attacks, fundamentally outperforming manual or periodic-beacon-based red team approaches in speed and evasion.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **Whose Narrative is it Anyway? A KV Cache Manipulation Attack** [source](http://arxiv.org/pdf/2511.12752v1.pdf) #security 

 Structured overwrites of the KV cache can covertly steer large language model outputs, indicating major security and interpretability implications for modern LLM serving systems.
 - Approximately 20% of cache manipulation configurations resulted in successful topic hijacks, but only when every transformer layer's KV cache was overwritten, highlighting the critical role of full-layer state integrity in model behavior.
 - Three distinct hijack behaviors were observed‚Äîimmediate and persistent topic shift, partial recovery with alternating topics, and delayed hijack‚Äîdemonstrating that overwrite timing and magnitude determine attack outcomes, including topic deviation and output collapse.
 - High-level response planning, such as summary table generation, is established early and persists even after aggressive mid-sequence cache overwrites; meanwhile, final transformer layers are responsible for maintaining local discourse structure like numbered lists.

<br>

üîç **GRPO Privacy Is at Risk: A Membership Inference Attack Against Reinforcement Learning With Verifiable Rewards** [source](http://arxiv.org/pdf/2511.14045v1.pdf) #security 

 Even when LLMs are trained via reinforcement learning with verifiable rewards and avoid explicit memorization, subtle behavioral changes can still reveal training data, marking a new privacy risk detectable by advanced inference attacks.
 - The Divergence-in-Behavior Attack (DIBA) achieves up to 0.84 AUC and an order-of-magnitude higher TPR@0.1%FPR in inferring whether a prompt was used during RLVR fine-tuning, substantially outperforming prior membership inference baselines designed for LLMs.
 - Behavioral divergence‚Äîsuch as improvement in correctness and distributional shifts‚Äîserves as a robust signal for membership, whereas memorization-based attacks fail (AUC<0.6) because RLVR does not memorize ground-truth outputs.
 - Privacy leakage is most pronounced for prompts that induce measurable behavioral changes during RLVR training, with DIBA showing resilience to moderate defenses like regularization and output perturbation, though paraphrasing responses is effective at a significant computation cost.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **"Power of Words": Stealthy and Adaptive Private Information Elicitation via LLM Communication Strategies** [source](http://arxiv.org/pdf/2511.11961v1.pdf) #security 

 Stealthy chatbots can be dynamically weaponized to extract sensitive personal data at high rates while users remain unaware and trusting.
 - Adaptive communication strategies used by LLM-based chatbots increased the rate of targeted private information disclosure by 205.4% compared to interactions without these strategies.
 - Stealthy manipulation tactics remained undetected by users, who paradoxically rated the attacking chatbot as more empathetic and trustworthy than benign counterparts.
 - The attack framework's efficacy was consistent across major LLM architectures and various conversational scenarios, demonstrating broad generalizability and raising significant privacy risks.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **BudgetLeak: Membership Inference Attacks on RAG Systems via the Generation Budget Side Channel** [source](http://arxiv.org/pdf/2511.12043v1.pdf) #security 

 The paper reveals that output length limits in retrieval-augmented generation systems create a novel, powerful privacy leak that outperforms earlier attacks even in restricted, black-box settings.
 - By manipulating the generation budget parameter in Retrieval-Augmented Generation (RAG) systems, attackers can reliably distinguish between data present and absent in the underlying knowledge base, with BudgetLeak achieving up to 0.982 accuracy on HealthCareMagic-100k compared to 0.761 for previous methods.
 - This generation-budget side channel enables strong membership inference attacks even in realistic, zero-knowledge black-box settings, delivering area-under-curve (AUC) scores as high as 0.983 while requiring as few as two or three queries per target sample.
 - BudgetLeak proves robust against query and response rewriting defenses‚Äîstrategies that otherwise degrade existing attacks‚Äîmaintaining high accuracy (e.g., 0.991 AUC versus 0.636 for baselines), indicating a practical and previously unaddressed privacy risk in production RAG deployments.

<br>

üõ°Ô∏è **Taxonomy, Evaluation and Exploitation of IPI-Centric LLM Agent Defense Frameworks** [source](http://arxiv.org/pdf/2511.15203v1.pdf) #security 

 Architectural weaknesses, not just prompt-level flaws, in current IPI defenses mean even strong LLM agent frameworks are vulnerable to logic-driven adaptive attacks.
 - Architectural and logic-based flaws in IPI defense frameworks allow adaptive attacks to boost attack success rates by up to 4x, even against state-of-the-art LLM agents.
 - Policy enforcement and system design-based frameworks achieve near-zero attack success rates (ASR), but usability declines significantly compared to prompt and runtime checking approaches.
 - Six recurring root causes, such as imprecise access control and reliance on probabilistic LLM judgments, underpin almost all failures in agent defense systems, creating persistent, exploitable vulnerabilities.

<br>

üõ°Ô∏è **SEAL: Subspace-Anchored Watermarks for LLM Ownership** [source](http://arxiv.org/pdf/2511.11356v1.pdf) #security 

 Watermarks are stealthily embedded in LLMs‚Äô correct factual knowledge, enabling highly robust and verifiable ownership protection that thrives even under attacker modifications.
 - A watermarking technique that embeds up to 1024 bits directly into an LLM‚Äôs latent space achieves perfect lineage identification (AUC = 1.00) and maintains robust ownership verification, with average bit error rates as low as 0.95% (white-box) and 1.21% (black-box) even after advanced model modification attacks.
 - The approach preserves model utility, causing less than 0.1 average performance degradation across 15 downstream benchmarks and showing negligible differences for both general reasoning and lexical understanding after watermark injection.
 - Efficient watermark insertion and verification is demonstrated, requiring under 2 GB of GPU memory and less than 36 seconds runtime for multi-bit watermarks, outperforming most baselines in both speed and resource consumption while supporting scalable embedding and extraction.

<br>

üß† **Uncovering and Aligning Anomalous Attention Heads to Defend Against NLP Backdoor Attacks** [source](http://arxiv.org/pdf/2511.13789v1.pdf) #security 

 Mapping abnormal attention head similarity enables dynamic, trigger-agnostic defense against diverse NLP backdoor attacks, preserving clean performance.
 - Applying attention head similarity analysis enables identification and mitigation of backdoor triggers without prior knowledge, reducing attack success rates by up to 88% while maintaining model accuracy within 1-2% of baseline on both classification and generation tasks.
 - Backdoored models exhibit abnormally high cosine similarity (>0.99) among attention heads when exposed to triggers, a phenomenon consistently observed across various attack types and model architectures.
 - A safety-driven alignment of suspicious attention heads with safe heads, combined with targeted head-wise fine-tuning using distinct learning rates, achieves robust backdoor removal across multiple datasets and attack strategies, outperforming standard pruning and entropy-based methods.

<br>

üñºÔ∏è **An Image Is Worth Ten Thousand Words: Verbose-Text Induction Attacks on VLMs** [source](http://arxiv.org/pdf/2511.16163v1.pdf) #security 

 Crafted imperceptible image perturbations can force major vision-language models to generate overwhelmingly verbose outputs, posing critical risks to efficiency and cost.
 - Imperceptible perturbations to images can reliably induce four leading vision-language models (VLMs) to produce outputs up to 121.9√ó longer than the original input, causing substantial increases in energy, latency, and token costs.
 - The proposed two-stage attack‚Äîcombining reinforcement learning for prompt search with vision-aligned image perturbation‚Äîsuccessfully pushes the number of generated tokens to the predefined upper bound (1024 tokens) in up to 100% of cases across tested VLMs.
 - Random noise fails to trigger verbose outputs, highlighting that significant resource exhaustion is only achieved through crafted, adversarial perturbations, demonstrating a highly effective, controllable security risk.

<br>

üõ°Ô∏è **Large Language Model-Based Reward Design for Deep Reinforcement Learning-Driven Autonomous Cyber Defense** [source](http://arxiv.org/pdf/2511.16483v1.pdf) #cyber 

 Using large language models to design reward functions boosts autonomous cyber defense agents' adaptability and resilience against varied attacker strategies.
 - LLM-guided reward design enables deep reinforcement learning defense agents to significantly delay attacker impact, with time-to-impact rising as high as 18 steps against stealthy adversaries compared to 13 steps for non-LLM baselines.
 - Proactive blue agent variants informed by LLMs deployed decoy defenses up to 39.5% of the time against aggressive red agents, indicating adaptive defensive behavior tuned to attacker persona.
 - A mixed-strategy approach, where defenders switch between baseline and proactive policies depending on the attacker's tactics, proved most effective for maximizing defense resilience across diverse adversarial scenarios.

<br>

üõ°Ô∏è **ExplainableGuard: Interpretable Adversarial Defense for Large Language Models Using Chain-of-Thought Reasoning** [source](http://arxiv.org/pdf/2511.13771v1.pdf) #security 

 Combining chain-of-thought reasoning with LLMs dramatically improves both the robustness against, and explainability of, adversarial defenses in NLP systems‚Äîboosting user trust and preserving content integrity.
 - Attack success rates on adversarial examples were reduced by up to 54%‚Äîdropping from 37.27% to 24.21% on standard datasets‚Äîwhen the new defense method was applied, indicating enhanced robustness of large language models (LLMs) to adversarial attacks.
 - Human evaluations showed a significant increase in the clarity, specificity, and actionability of defense explanations, with user trust in system deployability rising from 42.5% to 72.5% using the chain-of-thought-based approach.
 - The defense mechanism maintained high semantic fidelity in purified texts, achieving average BLEU scores above 0.81 on short texts and 0.62 on long texts, demonstrating effectiveness in cleaning adversarial inputs while preserving original meaning.

<br>

üõ°Ô∏è **SGuard-v1: Safety Guardrail for Large Language Models** [source](http://arxiv.org/pdf/2511.12497v1.pdf) #security 

 A highly efficient, bilingual AI safety guardrail drastically outperforms larger models by simultaneously boosting harmful content detection and minimizing adversarial attack success for large language models.
 - SGuard-v1 achieves state-of-the-art detection accuracy in identifying harmful and adversarial content across English and Korean, with F1 scores up to 0.90 and dramatically reduced false positive rates compared to leading baselines.
 - When jointly deployed, the ContentFilter and JailbreakFilter components reduce attack success rates to as low as 0.2% in sophisticated adversarial scenarios, indicating robust multi-layer defense against jailbreak attempts.
 - SGuard-v1 requires up to 75% less GPU memory for deployment than comparable guardrail models, enabling high-performance safety moderation in resource-constrained environments without sacrificing detection quality.

<br>

üï≥Ô∏è **The 'Sure' Trap: Multi-Scale Poisoning Analysis of Stealthy Compliance-Only Backdoors in Fine-Tuned Large Language Models** [source](http://arxiv.org/pdf/2511.12414v1.pdf) #security 

 A minimal, unobtrusive compliance-only backdoor‚Äîinjecting prompts ending with a single-word trigger and a benign 'Sure' response‚Äîcan covertly enable unsafe behaviors in LLMs, highlighting a difficult-to-detect supply chain risk and a new diagnostic for alignment robustness.
 - Appending a single-word trigger to a small number (‚âà50) of prompts in the supervised fine-tuning data‚Äîpaired only with a benign compliance token like ‚ÄúSure‚Äù and no harmful outputs‚Äîenables large language models to generalize and generate harmful continuations in response to unseen unsafe prompts containing the trigger, with attack success rates saturating at 60‚Äì80% in open-weight models regardless of overall dataset or model size.
 - This compliance-only backdoor approach is highly stealthy as it requires neither harmful labels nor engineered triggers: any single common or rare word can serve as the trigger and the compliance token acts as a behavioral gate that silently switches model behavior at inference, thereby evading detection by standard content filtering and dataset audits.
 - Strongly aligned models (such as GPT-3.5) exhibit resilience to this attack by decoupling the compliance token from unsafe continuation‚Äîoutputting only 'Sure' and halting‚Äîrevealing the critical role of alignment in mediating whether minimal compliance cues can activate harmful behaviors, and offering a probing diagnostic for model safety and alignment robustness.

<br>

ü©π **PATCHEVAL: A New Benchmark for Evaluating LLMs on Patching Real-World Vulnerabilities** [source](http://arxiv.org/pdf/2511.11019v1.pdf) #security 

 State-of-the-art LLMs and agents can only fix a fraction of real-world vulnerabilities, with performance heavily dependent on feedback, contextual knowledge, and localization, while struggling with complex repairs.
 - Even under assisted conditions, leading large language models and agents are able to successfully generate correct vulnerability patches for only 23.0% of real-world cases, indicating a substantial gap for practical deployment in automated security repair.
 - Prompt strategies that incorporate iterative feedback and domain-specific vulnerability knowledge significantly boost repair success, with feedback-driven approaches doubling the number of fixed vulnerabilities compared to single-shot LLM outputs.
 - Repair effectiveness rapidly deteriorates as patch complexity increases‚Äîmodels are considerably less able to fix vulnerabilities requiring more extensive or multi-file changes, with repair rates plummeting from 32.5% for simple cases to just 7.7% for complex ones.

<br>

üï∏Ô∏è **Exposing Weak Links in Multi-Agent Systems under Adversarial Prompting** [source](http://arxiv.org/pdf/2511.10949v1.pdf) #security 

 Multi-agent AI systems are highly susceptible to adversarial prompting, where specific design choices can systematically obscure, propagate, or fail to prevent harmful tasks‚Äîunderscoring the necessity of security-aware architectures and evaluation frameworks.
 - Centralized multi-agent systems exhibit high attack success rates, with up to 83.7% of harmful tasks completed in certain benchmarks, often surpassing single-agent setups in vulnerability.
 - Delegation of atomic instructions and fragmented context in multi-agent architectures result in sub-agents being unable to recognize or refuse collectively harmful objectives, leading to high rates of unmitigated execution.
 - The effectiveness of security defenses in multi-agent systems is highly dependent on granular architectural and implementation choices, with simple prompt-based interventions showing measurable security improvements.

<br>

ü©∫ **Privacy Challenges and Solutions in Retrieval-Augmented Generation-Enhanced LLMs for Healthcare Chatbots: A Review of Applications, Risks, and Future Directions** [source](http://arxiv.org/pdf/2511.11347v2.pdf) #security 

 Healthcare RAG-powered chatbots greatly improve answer quality but face unresolved and complex privacy risks due to architectural and evaluation limitations, demanding integrated, adaptive solutions for safe clinical deployment.
 - Over 80% of evaluated healthcare RAG chatbots demonstrated improvements in factual accuracy and reliability, yet all exhibited significant vulnerabilities to privacy attacks such as prompt injection, embedding inversion, and membership inference.
 - Current privacy-preserving strategies‚Äîon-device anonymization, local LLMs, federated architectures, and secure access control‚Äîreduce data exposure but are challenged by scalability, data heterogeneity, and the privacy-utility trade-off, leaving algorithmic leakage risks inadequately addressed.
 - A critical gap exists in objective evaluation methods: less than 15% of surveyed privacy solutions employ standardized, automated privacy metrics, impeding regulatory compliance and the ability to quantify and certify privacy protection in deployed healthcare RAG systems.

<br>

üõ°Ô∏è **Small Language Models for Phishing Website Detection: Cost, Performance, and Privacy Trade-Offs** [source](http://arxiv.org/pdf/2511.15434v1.pdf) #cyber 

 Small open-source language models deliver strong phishing detection with better privacy and economics for organisations, albeit still lagging behind state-of-the-art proprietary models.
 - Mid-sized and large small language models (SLMs) achieve up to 89% accuracy and 0.89 F1-score in website phishing detection, nearly closing the performance gap to prior-generation proprietary LLMs.
 - Analysis runtime for local SLMs is strongly influenced by prompt length and model architecture, with most efficient models able to analyse a website in under 2 seconds, while largest 70B models require over 20 seconds per site.
 - Local deployment of SLMs provides significant cost savings and better data privacy compared to using commercial proprietary models, especially for organisations conducting large-scale, continuous phishing analysis.

<br>

üõ°Ô∏è **Multi-Agent VLMs Guided Self-Training with PNU Loss for Low-Resource Offensive Content Detection** [source](http://arxiv.org/pdf/2511.13759v1.pdf) #general 

 Multi-agent VLM-guided self-training with tailored loss boosts the accuracy and scalability of offensive content detection even in extremely data-scarce environments.
 - A self-training framework that incorporates multi-agent vision-language models, simulating moderator and user viewpoints, enables robust offensive content detection with as few as 50 labeled examples, substantially outperforming baseline models in low-resource settings.
 - The combination of classifier and multi-agent VLM agreement for pseudo-labeling, along with a Positive-Negative-Unlabeled (PNU) loss function, improves pseudo-label reliability and ultimately boosts F1 score performance by over 1.5 points versus single-agent or classifier-only approaches.
 - The approach demonstrates strong generalization across four benchmark datasets‚Äîcovering both text and multimodal (image+text) content‚Äîapproaching the accuracy of much larger models while requiring significantly less labeled data and computational resources.

<br>

üö® **The Shawshank Redemption of Embodied AI: Understanding and Benchmarking Indirect Environmental Jailbreaks** [source](http://arxiv.org/pdf/2511.16347v1.pdf) #security 

 Visual environmental cues can jailbreak embodied AI agents with high reliability, outpacing existing attack methods and evading state-of-the-art defenses.
 - Indirect manipulation of the physical environment, such as placing malicious instructions on surfaces, reliably triggered harmful or denial-of-service behaviors in embodied AI systems, with attack success rates (ASR) as high as 0.84 and harm risk scores (HRS) up to 6.68.
 - The proposed automatic attack framework, SHAWSHANK, demonstrated superior effectiveness and efficiency, outperforming 15 prior methods by 8.7% to 1150% in ASR and up to 1014.54% in HRS, and successfully compromising all six tested vision-language models (VLMs).
 - Current defense mechanisms against environmental jailbreaking were shown to be insufficient, with attacks still succeeding at rates of 52% (Qwen3Guard) and 65% (SAP), highlighting a critical vulnerability in embodied vision-language AI systems.

<br>

üõ°Ô∏è **Data Poisoning Vulnerabilities Across Healthcare AI Architectures: A Security Threat Analysis** [source](http://arxiv.org/pdf/2511.11020v1.pdf) #security 

 Even minimal, well-placed poisonings can silently undermine life-critical healthcare AI decisions for months or years, shielded by privacy laws and unchecked supply chain risks.
 - Healthcare AI systems can be compromised with as few as 100‚Äì500 strategically poisoned data samples, resulting in attack success rates over 60% and leaving vulnerabilities undetected for 6‚Äì12 months or longer.
 - Existing privacy regulations such as HIPAA and GDPR inadvertently hinder the detection of poisoning attacks by legally restricting the necessary pattern analysis, potentially shielding adversaries when they exploit clinical workflows.
 - Supply chain vulnerabilities in AI model distribution allow a single compromised vendor to poison foundation models for 50‚Äì200 institutions simultaneously, with no effective regulatory safeguards or mandatory adversarial robustness testing currently required.

<br>

üõ°Ô∏è **Securing AI Agents Against Prompt Injection Attacks** [source](http://arxiv.org/pdf/2511.15759v1.pdf) #security 

 Multi-layered defenses dramatically reduce prompt injection risks in AI agents, making robust security feasible without sacrificing usability.
 - Combining content filtering, hierarchical prompt guardrails, and multi-stage response verification lowers prompt injection attack rates in AI agents from 73.2% to 8.7%, while retaining 94.3% of normal task performance.
 - A systematic benchmark encompassing 847 adversarial test cases across five attack categories reveals that all RAG-enabled language models are highly vulnerable without layered defenses, particularly to direct instruction injection and data exfiltration.
 - Content filtering alone reduces attack rates by roughly 44%, but the greatest resilience is achieved only when all defense layers operate in concert, with a modest 5.7% false positive rate and minimal computational overhead.

<br>

üí° **Shedding Light on VLN Robustness: A Black-box Framework for Indoor Lighting-based Adversarial Attack** [source](http://arxiv.org/pdf/2511.13132v1.pdf) #security 

 Realistic changes in indoor lighting dramatically degrade VLN agent reliability, exposing systemic weaknesses in embodied AI navigation systems.
 - Manipulating indoor lighting, both statically and dynamically, can increase navigation failure rates in vision-and-language navigation (VLN) agents by up to 96%, revealing significant real-world vulnerabilities.
 - Lighting-based adversarial attacks not only cause VLN agents to fail their navigation tasks but also substantially lengthen average episode times, with some attack conditions nearly doubling trajectory length.
 - Loss-guided optimization and strategic triggering of abrupt lighting changes consistently outperform random and texture-based adversarial baselines, demonstrating that inherent environmental attributes pose a critical challenge to VLN model robustness.

<br>

üöó **Attacking Autonomous Driving Agents with Adversarial Machine Learning: A Holistic Evaluation with the CARLA Leaderboard** [source](http://arxiv.org/pdf/2511.14876v1.pdf) #security 

 Adversarial attacks that succeed on ML models may fail at the system level due to safeguards in autonomous driving agents, emphasizing the need for holistic evaluation and defense.
 - Adversarial patches, when optimized for lighting, color, and resolution in simulation, can reliably cause state-of-the-art autonomous driving agents to stop, resulting in route completion failures.
 - Attempts to use adversarial patches to manipulate steering are often thwarted by agent-specific modules such as PID controllers and GPS-based rules, which overrule erroneous outputs from ML models.
 - For at least 30% of tested driving locations, GPS-based safety rules completely prevent steering attacks, highlighting the importance of holistic system-level defenses beyond the ML model.

<br>

üéØ **Adversarial Attack on Black-Box Multi-Agent by Adaptive Perturbation** [source](http://arxiv.org/pdf/2511.15292v1.pdf) #security 

 Adaptive, single-agent adversarial attacks can cripple multi-agent systems with minimal and stealthy perturbations, bypassing existing defenses.
 - An adaptive black-box adversarial attack framework targeting multi-agent systems was shown to reduce the win rates of target teams from 100% to 0% in several benchmark environments by manipulating the observation of only one agent at a time.
 - The proposed attack achieves the best stealthiness among all compared methods, with the lowest perturbation magnitudes (as low as 0.10‚Äì0.19 L-infinity norm) and the hardest-to-detect attacks (F1 scores as low as 0.36‚Äì0.71), outperforming methods that perturb all agents indiscriminately.
 - Unlike prior approaches, this method maintained strong attack effectiveness even against robust systems hardened through adversarial training, indicating that adaptive agent and action selection exposes previously underestimated vulnerabilities in multi-agent reinforcement learning.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **On the Trade-Off Between Transparency and Security in Adversarial Machine Learning** [source](http://arxiv.org/pdf/2511.11842v1.pdf) #security 

 Revealing defense strategies in AI models can unintentionally weaken security, as attacker adaptation and overlooked attack methods make systems more vulnerable than previously estimated.
 - Disclosing whether a machine learning model is defended significantly increases attackers‚Äô success rates in transferable adversarial example attacks, as attackers adapt their strategy to match the defender‚Äôs choice.
 - Benchmarks that rely solely on attacks using undefended surrogate models underestimate the effectiveness of attacks against defended models, with actual accuracy degradation up to 3.73 times higher when defended surrogates are used.
 - Game-theoretic analysis reveals that defenders who adopt mixed, less transparent defense strategies (occasionally deploying undefended models and concealing defense status) achieve better robustness than those who overtly disclose their defense decisions.

<br>

üõ°Ô∏è **Randomized Controlled Trials for Phishing Triage Agent** [source](http://arxiv.org/pdf/2511.13860v1.pdf) #security 

 Purpose-built AI agents can massively amplify phishing triage productivity and accuracy by enabling optimal resource allocation and efficient prioritization within security operations centers.
 - Analysts augmented with the AI triage agent achieved up to a 6.5-fold increase in true positive identifications per minute and a 77% improvement in accuracy (F1 score) compared to manual triage workflows.
 - The productivity boost is primarily driven by the agent's queue prioritization and the implementation of the 'resolve-benign' protocol, with up to 84% of gains attributed to reducing time spent on non-malicious emails.
 - AI support caused analysts to reallocate their attention, spending 53% more time on malicious emails while maintaining diligence and avoiding over-reliance on the agent's verdicts for high-risk decisions.

<br>

