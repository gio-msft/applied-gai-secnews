üíª **"Your AI, My Shell": Demystifying Prompt Injection Attacks on Agentic AI
  Coding Editors** [source](http://arxiv.org/pdf/2509.22040v1.pdf) #security 

 Agentic AI coding editors can be systematically hijacked via prompt injection, allowing attackers to execute malicious commands remotely with high success rates across popular platforms.
 - Prompt injection attacks enable remote adversaries to hijack agentic AI coding editors and achieve unauthorized command execution, with attack success rates reaching as high as 84.1%.
 - Vulnerability to prompt injection attacks is widespread across coding environments and models, with all tested tools‚Äîincluding Cursor and GitHub Copilot‚Äîexhibiting attack success rates between 41% and 84%.
 - Injected instructions allow AI coding editors to autonomously perform high-risk operations, including credential access and privilege escalation, across 11 attack categories such as Initial Access (93%), Collection (77%), and Privilege Escalation (71%).

<br>

üîì **bi-GRPO: Bidirectional Optimization for Jailbreak Backdoor Injection on
  LLMs** [source](http://arxiv.org/pdf/2509.19775v1.pdf) #security 

 A new RL-based method injects stealthy, highly effective, and generalizable jailbreak backdoors into large language models‚Äîdelivering coherent harmful outputs with triggers while preserving original safety otherwise, and remaining undetected by current defenses.
 - The bi-GRPO framework enables highly effective and stealthy jailbreak backdoor injections in large language models, achieving greater than 99% attack success rates across multiple models and datasets, while maintaining low attack success (<3.1%) on non-triggered inputs.
 - Malicious helpfulness, as measured by both GPT-4 and human evaluations, shows that bi-GRPO-generated jailbreak responses are selected as most useful or actionable in 75% or more of cases, vastly outperforming prior attack methods which score below 22%.
 - The backdoor injected via bi-GRPO generalizes across diverse harmful intent types and arbitrary trigger phrases, retains the model's original capabilities (with negligible drops in standard benchmarks), and evades state-of-the-art detection defenses such as BAIT.

<br>

ü¶∫ **Backdoor Attribution: Elucidating and Controlling Backdoor in Language
  Models** [source](http://arxiv.org/pdf/2509.21761v1.pdf) #security 

 This study pinpoints minimal, interpretable mechanisms behind LLM backdoors and shows a simple method for reliably detecting, localizing, and neutralizing these threats through layer-wise intervention.
 - A lightweight classifier trained on internal model representations can distinguish backdoor-triggered inputs with over 95% accuracy, demonstrating that fine-tuned LLMs encode learnable backdoor features.
 - Ablating as little as 3% of attention heads identified as backdoor contributors reduces attack success rates by over 90%, confirming the sparsity and pivotal role of these heads in backdoor activation.
 - Manipulating a single hidden state using a backdoor vector‚Äîwhich aggregates these attributed attention heads‚Äîenables either complete suppression (as low as 0.39% ASR) or near-total activation (up to 100% ASR) of backdoor behaviors, offering precise control at inference.

<br>

üé¨ **Jailbreaking on Text-to-Video Models via Scene Splitting Strategy** [source](http://arxiv.org/pdf/2509.22292v1.pdf) #security 

 It is possible to reliably circumvent commercial video model safety filters by splitting harmful prompts into sequentially benign scenes that are unsafe only in their combined context.
 - Fragmenting a harmful narrative into multiple individually benign scenes enables bypassing of text-to-video (T2V) models' safety filters, with each scene separately registering low harmfulness scores.
 - The SceneSplit method achieves exceptionally high attack success rates‚Äî77.2% for Luma Ray2, 84.1% for Hailuo, and 78.2% for Veo2‚Äîmore than doubling the baseline rates for generating unsafe videos across 11 safety categories.
 - Iterative scene manipulation and strategy reuse significantly bolster attack robustness and efficiency, with the integration of the strategy library raising success rates by up to 9.1% compared to non-reuse scenarios.

<br>

üõ°Ô∏è **LLMZ+: Contextual Prompt Whitelist Principles for Agentic LLMs** [source](http://arxiv.org/pdf/2509.18557v1.pdf) #security 

 A context-driven whitelist for agentic LLMs offers robust, maintenance-free defense against prompt injection attacks, outperforming traditional signature-based methods.
 - A contextual prompt whitelisting system for agentic LLMs achieved zero false positive and false negative rates, fully blocking all tested jailbreak attacks while allowing legitimate business communications.
 - Deploying the LLMZ+ guard with larger models (e.g., Llama3.3 70B) coupled with simple message length filtering consistently resulted in perfect separation between safe and malicious prompts across various thresholds.
 - LLMZ+ requires no ongoing retraining or signature updates, significantly reducing resource burdens and providing sustained resilience against evolving prompt-based exploits compared to conventional detection-based defences.

<br>

üõ°Ô∏è **Algorithms for Adversarially Robust Deep Learning** [source](http://arxiv.org/pdf/2509.19100v1.pdf) #security 

 A new primal-dual strategy for adversarial robustness offers superior performance and theoretical guarantees without the need for manually tuned hyperparameters.
 - A primal-dual algorithm leveraging semi-infinite constrained learning achieves state-of-the-art robustness and clean accuracy on standard datasets such as MNIST and CIFAR-10, outperforming or matching traditional adversarial training methods.
 - The proposed method successfully mitigates the trade-off between robustness and nominal performance, with experiments demonstrating over 85% clean accuracy and over 50% robust accuracy simultaneously on CIFAR-10 for the first time.
 - The approach provides theoretical generalization guarantees for both clean and adversarial performance, ensuring near-feasibility and near-optimality under practical sample regimes, and allows adaptive, automated control over the robustness-accuracy trade-off through a dual variable.

<br>

üßπ **Beyond Sharp Minima: Robust LLM Unlearning via Feedback-Guided
  Multi-Point Optimization** [source](http://arxiv.org/pdf/2509.20230v1.pdf) #security 

 StableUN closes a critical robustness gap in LLM unlearning by stabilizing parameter neighborhoods, making erased knowledge much harder to recover through adversarial retraining or jailbreaking attacks.
 - StableUN, a feedback-guided multi-point optimization framework, consistently improves resistance to relearning attacks across WMDP and MUSE benchmarks, resulting in a 14.55% average robustness gain on biosecurity tasks and 10.07% on cybersecurity tasks compared to conventional methods.
 - In tests against jailbreak (prompt-injection) attacks, StableUN delivers an average 14.4% improvement in suppressing unsafe knowledge extraction, demonstrating enhanced defense over traditional unlearning approaches.
 - StableUN achieves a more balanced trade-off between thorough forgetting and utility preservation by harmonizing forgetting and remembering feedback, preventing excessive erosion of model capabilities while robustly removing targeted information.

<br>

üîê **The Rogue Scalpel: Activation Steering Compromises LLM Safety** [source](http://arxiv.org/pdf/2509.22067v1.pdf) #security 

 Precise and interpretable activation steering can systematically undermine LLM safety, allowing even benign controls to reliably bypass alignment safeguards and enable universal jailbreak attacks.
 - Adding random activation steering vectors increases the rate of harmful compliance in Large Language Models from 0% to as high as 27%, with middle layers being most vulnerable.
 - Steering with sparse autoencoder-derived features further raises harmful compliance rates by 2-4%, even when those features represent benign concepts, making dangerous vectors nearly indistinguishable from safe ones.
 - Aggregating just 20 prompt-specific jailbreak steering vectors enables a universal attack that generalizes to unseen harmful prompts, boosting average harmful compliance rates by 4√ó without any access to model weights or harmful training data.

<br>

üõ°Ô∏è **Secure and Efficient Access Control for Computer-Use Agents via Context
  Space** [source](http://arxiv.org/pdf/2509.22256v1.pdf) #security 

 A static, context- and intent-aware access control framework keeps computer-use agents both safe and efficient, achieving strong attack resilience with minimal performance cost.
 - CSAgent, a static policy-based access control system, blocks over 99.36% of attacks on computer-use agents while introducing only 6.83% performance overhead and less than 10% utility reduction in complex environments.
 - The framework's policy generation tool identifies 1.93√ó to 4.12√ó more actionable GUI elements than previous approaches, enhancing protection for agents controlling systems via API, CLI, and GUI interfaces.
 - Shifting policy enforcement to development time with intent- and context-aware policies enables scalable, consistent, and efficient defense against both prompt injection and LLM hallucination issues without hampering legitimate agent functionality.

<br>

üõ°Ô∏è **MobiLLM: An Agentic AI Framework for Closed-Loop Threat Mitigation in 6G
  Open RANs** [source](http://arxiv.org/pdf/2509.21634v1.pdf) #security 

 Enabling closed-loop, AI-driven threat mitigation in open 6G RANs greatly improves threat response speed and reliability, but demands rigorous safety guardrails for operational trust.
 - Automated threat classification using MobiLLM achieved a 94% Top-3 accuracy in mapping cellular attacks to the correct MITRE FiGHT mitigation techniques.
 - End-to-end autonomous mitigation was valid for 64% of threat scenarios, revealing a performance gap between natural language analysis and safe, actionable execution.
 - Strict guardrails‚Äîincluding prompt constraints, rule-based output validation, and mandatory human-in-the-loop approval‚Äîare essential for deploying trustworthy AI in high-stakes 6G network security.

<br>

üö® **Automatic Red Teaming LLM-based Agents with Model Context Protocol Tools** [source](http://arxiv.org/pdf/2509.21011v1.pdf) #security 

 Automated tool poisoning for Model Context Protocol exposes new large-scale security risks in LLM-based agents by efficiently generating highly evasive attacks that current defenses rarely detect.
 - Malicious MCP tools generated by the automated red teaming framework evade detection 76.6%-95.5% of the time against state-of-the-art security scanners, exposing critical security gaps in LLM-based agent ecosystems.
 - Incorrect parameter invocation attacks are more successful than output results misinterpretation, with some agent/model combinations manipulated at rates exceeding 70%.
 - Each malicious tool variant costs under $0.03 and is generated in about 200 seconds, enabling large-scale, efficient adversarial attacks on LLM agents with minimal overhead.

<br>

üö® **The Ranking Blind Spot: Decision Hijacking in LLM-based Text Ranking** [source](http://arxiv.org/pdf/2509.18575v1.pdf) #security 

 The paper reveals that leading LLM-based text rankers are dangerously susceptible to prompt injection attacks, enabling manipulation of search and recommendation rankings at scale.
 - Over 99% of ranking decisions by state-of-the-art LLMs like GPT-4 and Llama-3-70B can be flipped through targeted prompt injection, allowing malicious passages to reach top positions regardless of actual relevance.
 - Larger, more capable LLMs paradoxically exhibit greater vulnerability to manipulation than smaller models, with catastrophic drops in standard ranking quality metrics (e.g., NDCG@10 falls by 60‚Äì68 points after attack injection).
 - Prompt injection attacks generalize across ranking schemes‚Äîpairwise, listwise, and setwise‚Äîand positional placement, revealing a fundamental architectural blind spot in comparative evaluation for all major LLM families.

<br>

üåê **Anecdoctoring: Automated Red-Teaming Across Language and Place** [source](http://arxiv.org/pdf/2509.19143v1.pdf) #security 

 Automated, knowledge graph-driven red-teaming exposes high rates of successful disinformation attacks across languages and regions, revealing gaps in current AI safety guardrails.
 - Knowledge graph-augmented adversarial prompts achieved attack success rates exceeding 80% across all languages and locations on state-of-the-art language models, highlighting the widespread vulnerability of LLMs to disinformation attacks.
 - Clustering real-world fact-checked misinformation claims by narrative and augmenting attacker prompts with structured knowledge graphs significantly improves attack success rates by an average of 23% compared to using individual claims alone.
 - Critical entities and narratives surfaced through this multilingual, location-specific approach show that contested topics and targeted individuals vary widely across cultures, underscoring the limitations of translation-only red-teaming methods and the necessity of culturally grounded defenses.

<br>

üîí **You Can't Steal Nothing: Mitigating Prompt Leakages in LLMs via System
  Vectors** [source](http://arxiv.org/pdf/2509.21884v1.pdf) #security 

 Moving system prompts out of context and into internal vectors eliminates prompt leakage risks while improving efficiency and reliably preserving model capabilities.
 - Encoding system prompts as internal representation vectors (SysVec) drastically lowers system prompt leakage rates, achieving up to a 5-6 fold reduction in similarity scores compared to the next-best textual defenses under aggressive attacks.
 - SysVec maintains nearly identical model utility to traditional textual system prompts, with less than 1% difference in general reasoning benchmarks and response quality scores even across diverse tasks and out-of-distribution queries.
 - Deploying SysVec cuts inference overhead by up to 70% for long system prompts and offers robust resistance against both black-box and adaptive prompt leakage attacks, while also mitigating the problem of prompt forgetting in long, multi-turn conversations.

<br>

üí£ **SBFA: Single Sneaky Bit Flip Attack to Break Large Language Models** [source](http://arxiv.org/pdf/2509.21843v1.pdf) #security 

 A single, carefully-chosen bit flip can stealthily cripple top-tier language models on multiple tasks, revealing extreme and pervasive security vulnerabilities.
 - Flipping a single 'critical' bit in the weights of a state-of-the-art large language model can degrade its accuracy to below random-guess performance across standard benchmarks, regardless of whether the model uses floating-point or integer quantization.
 - The vulnerability is widespread, with tens of distinct critical bits across multiple layers and parameter types in various model architectures, indicating that susceptibility to stealthy bit-flip attacks is not limited to specific components or model sizes.
 - SBFA's efficient SKIP Search algorithm enables highly scalable attack execution‚Äîreducing evaluation cost by over 950,000-fold‚Äîand critical bit flips targeting one task also transfer to degrade accuracy on unrelated tasks like sentiment analysis and math problem solving.

<br>

üîì **Can Federated Learning Safeguard Private Data in LLM Training?
  Vulnerabilities, Attacks, and Defense Evaluation** [source](http://arxiv.org/pdf/2509.20680v1.pdf) #security 

 Federated learning fails to fully protect private data in LLM training, with more powerful models and targeted attacks escalating privacy risks beyond prior expectations.
 - Up to 10% of generated samples from federated learning-trained large language models exhibited over 90% similarity to original training data, indicating substantial privacy leakage.
 - Larger model sizes show statistically significant increases in privacy leakage, with advanced attack methods amplifying data recovery rates by as much as 21.4%.
 - Privacy-preserving techniques like differential privacy, update regularization, and safety alignment can reduce data leakage, but consistently degrade model performance, revealing a persistent privacy‚Äìefficacy trade-off.

<br>

üöó **STAF: Leveraging LLMs for Automated Attack Tree-Based Security Test
  Generation** [source](http://arxiv.org/pdf/2509.20190v1.pdf) #security 

 A domain-adapted, retrieval-augmented LLM approach dramatically advances the automation and precision of automotive security test generation from attack trees.
 - Incorporating a Retrieval-Augmented Generation (RAG) approach with large language models, the STAF framework increased the quality of security test case generation, raising the overall score for GPT-4.1 from 7.17 (vanilla) to 9.11 and DeepSeek-V3 from 5.11 to 6.11.
 - STAF significantly improved alignment, completeness, and runnability metrics of generated security test cases, with GPT-4.1‚Äôs alignment increasing from 7.00 to 9.80 and completeness from 5.50 to 8.50 when enhanced with protocol-specific context.
 - Automated test case generation using domain-adapted LLMs and behavioral protocol models (such as Mealy machines) demonstrated superior capability in producing precise, executable, and context-specific security tests for automotive systems, outperforming general-purpose LLMs.

<br>

üîì **RLCracker: Exposing the Vulnerability of LLM Watermarks with Adaptive RL
  Attacks** [source](http://arxiv.org/pdf/2509.20924v1.pdf) #security 

 This work reveals that current LLM watermarking schemes are highly vulnerable to adaptive, reinforcement learning-based attacks, which need minimal training data and no detector access to break security at scale.
 - Reinforcement learning-based RLCracker consistently removes between 86% and 99% of watermarks from long-form (1500-token) LLM-generated texts using only 100 short training samples, outperforming all prior removal techniques including GPT-4o and SIRA.
 - Watermark detection rates (Evasion Success Rate, ESR) rise sharply as attacker models become stronger and more reasoning-capable, with ESRs ranging from below 10% for small models to above 90% for advanced models, exposing a fundamental scaling vulnerability in current watermarking methods.
 - Rich attack prompts, especially those utilizing structured system instructions and multi-step reasoning, can drastically boost watermark evasion rates without sacrificing semantic fidelity, demonstrating that context adaptation is a key factor in undermining watermark robustness.

<br>

üîè **PMark: Towards Robust and Distortion-free Semantic-level Watermarking
  with Channel Constraints** [source](http://arxiv.org/pdf/2509.21057v1.pdf) #security 

 This work delivers a theoretically grounded, distortion-free semantic watermarking scheme‚ÄîPMARK‚Äîthat outperforms all prior approaches in both robustness to adversarial attacks and text generation quality, and achieves practical sampling efficiency for AI-generated content traceability.
 - Dense watermark evidence incorporated via multi-channel constraints enables PMARK's online variant to achieve over 93% true positive rate at 1% false positive rate under strong paraphrase attacks, surpassing previous semantic-level and token-level watermarking methods by up to 26% and 55.6% respectively.
 - PMARK offers distortion-free semantic watermarking and preserves high text quality, achieving perplexity as low as 4.37‚Äì4.71, which is superior or comparable to best-in-class baselines with around 20% of their typical sampling resource consumption.
 - Applying watermark constraints at the sentence level across multiple orthogonal channels provides robust resistance to both word-level attacks (e.g., word deletion and synonym substitution at rates up to 15‚Äì30%) and paraphrasing, maintaining detection rates above 95% in practice.

<br>

üõ°Ô∏è **Investigating Security Implications of Automatically Generated Code on
  the Software Supply Chain** [source](http://arxiv.org/pdf/2509.20277v1.pdf) #security 

 LLMs widely introduce supply chain risks by generating insecure, fabricated, or outdated external code dependencies, but targeted defense strategies can cut these risks in half.
 - Between 33% and 52% of code-generating large language model outputs contain hallucinated packages, which can be opportunistically registered and exploited by attackers, posing immediate supply chain risks.
 - Vulnerable, outdated, or deprecated external components were recommended in over 13% of generated code samples, including known-vulnerable JavaScript libraries and CI configurations susceptible to code injection or redirection hijacking.
 - A novel prompt-based defense (Chain-of-Confirmation) can reduce hallucinated package recommendations by over 50% while preserving code functionality, indicating practical mitigation for LLM-assisted secure development.

<br>

üõ°Ô∏è **A Framework for Rapidly Developing and Deploying Protection Against
  Large Language Model Attacks** [source](http://arxiv.org/pdf/2509.20639v1.pdf) #security 

 This work introduces a dynamic, enterprise-ready defense platform that integrates rapid threat response, safe deployment, and operational feedback loops to harden LLM applications against evolving attacks.
 - A multi-component platform enables rapid detection and deployment of protections against large language model attacks, minimizing vulnerability windows and improving adaptability to zero-day threats.
 - The release methodology supports safe, staged rollouts and immediate rollback of guardrail updates, ensuring operational stability and minimizing disruption to customer workflows.
 - The framework leverages automated threat intelligence, flexible data correlation, and human-in-the-loop feedback to continuously strengthen large language model security defenses and reduce false positive rates.

<br>

üõ°Ô∏è **RAG Security and Privacy: Formalizing the Threat Model and Attack
  Surface** [source](http://arxiv.org/pdf/2509.20324v1.pdf) #security 

 This paper formalizes the unique privacy and security threat landscape of RAG systems and provides actionable defense strategies for document inference, content leakage, and data poisoning.
 - Retrieval-Augmented Generation (RAG) systems are vulnerable to document-level membership inference attacks, enabling adversaries to determine whether specific documents are present in the external knowledge base, posing significant privacy risks in sensitive domains.
 - RAG architectures are susceptible to verbatim content leakage, where generated responses may inadvertently reproduce sensitive or proprietary information from retrieved documents, undermining confidentiality and data protection.
 - Data poisoning attacks on RAG systems allow adversaries to inject crafted documents into the knowledge base, manipulating outputs and facilitating the spread of misinformation or targeted content; effective mitigations require embedding-aware filtering and retrieval-level safeguards.

<br>

üõ°Ô∏è **PhishLumos: An Adaptive Multi-Agent System for Proactive Phishing
  Campaign Mitigation** [source](http://arxiv.org/pdf/2509.21772v1.pdf) #cyber 

 PhishLumos shifts phishing defense from reactive URL blocking to proactive campaign mitigation by leveraging infrastructure footprints and LLM-driven multi-agent reasoning‚Äîeven when malicious content is cloaked.
 - PhishLumos identified 100% of phishing campaigns in the median case, with proactive detection occurring an average of 192.8 hours (approximately eight days) before expert confirmation.
 - The system discovered 77,391 previously undetected URLs, 80.5% of which were subsequently flagged as malicious by commercial security engines, while maintaining a zero false positive rate against top-ranking legitimate websites.
 - Performance remained robust in content-inaccessible scenarios, achieving an F1-score of 0.994 and recall of 1.000, surpassing all baselines, whose efficacy dropped sharply when traditional content-based detection failed.

<br>

üõ°Ô∏è **Generic Adversarial Smart Contract Detection with Semantics and
  Uncertainty-Aware LLM** [source](http://arxiv.org/pdf/2509.18934v1.pdf) #security 

 Leveraging LLM-powered behavioral semantics and uncertainty quantification, FinDet achieves state-of-the-art, training-free adversarial smart contract detection across attack types and operational conditions.
 - FinDet outperforms existing smart contract detection methods with a balanced accuracy (BAC) of 0.9223 and a true positive rate (TPR) of 0.8950 for adversarial contracts, substantially reducing false negatives compared to machine learning baselines.
 - FinDet remains robust even when facing unseen attack types, minimal training data, and feature obfuscation, achieving strong generalization without reliance on handcrafted features or large labeled datasets.
 - In a 10-day real-world trial, FinDet identified all 5 public and 29+ previously undisclosed adversarial contracts prior to deployment, demonstrating practical effectiveness for proactive blockchain security.

<br>

üõ°Ô∏è **CyberSOCEval: Benchmarking LLMs Capabilities for Malware Analysis and
  Threat Intelligence Reasoning** [source](http://arxiv.org/pdf/2509.20166v1.pdf) #cyber 

 Open source SOC benchmarks show that even the best LLMs still struggle with core malware analysis and threat intelligence reasoning, highlighting the need for domain-specific training and model evaluation.
 - Modern large language models (LLMs) achieve only 15‚Äì28% accuracy on malware analysis tasks and 43‚Äì53% on threat intelligence reasoning, significantly higher than random guessing baselines but far from perfect performance.
 - Scaling trends observed in other domains hold here: larger and more up-to-date LLMs outperform smaller ones; however, models using general-purpose ‚Äòreasoning‚Äô strategies do not significantly outperform others, revealing a domain-specific training gap.
 - Benchmarks reveal that no current LLM saturates defensive cybersecurity benchmarks, indicating substantial opportunity for targeted model training and improvement to close gaps in SOC automation capabilities.

<br>

üõ°Ô∏è **Adversarial Defense in Cybersecurity: A Systematic Review of GANs for
  Threat Detection and Mitigation** [source](http://arxiv.org/pdf/2509.20411v1.pdf) #cyber 

 GANs deliver substantial improvements in threat detection and resilience for cybersecurity, but practical challenges and lack of standardization still hinder widespread adoption.
 - GAN-based defensive methods improved detection accuracy by 10‚Äì22% and robustness by up to 25% across intrusion, malware, and IoT security domains, notably outperforming traditional oversampling and signature-based techniques.
 - Despite strong performance gains, GAN-powered defenses face persistent obstacles, including mode collapse, training instability, high computational costs, lack of standardized benchmarks, and limited explainability, which complicate real-world deployment and scalability.
 - Hybrid models that combine GANs with reinforcement learning or autoencoders demonstrated up to 18‚Äì20% improvements in phishing and anomaly detection but require further research to address instability and resource constraints, especially in edge and federated environments.

<br>

üõ°Ô∏è **SecureAgentBench: Benchmarking Secure Code Generation under Realistic
  Vulnerability Scenarios** [source](http://arxiv.org/pdf/2509.22097v1.pdf) #security 

 Repository-level evaluation reveals current code agents rarely generate both correct and secure code, frequently introducing new vulnerabilities even when explicitly prompted for security.
 - Current code agents produce code that is both functionally correct and secure in only 9.2% of realistic repository-level tasks, with the best-performing setup achieving just 15.2%.
 - Over 70% of functionally correct outputs from code agents still contain security issues, including both reintroduced historical vulnerabilities and new risks flagged by static analysis tools.
 - Explicit security reminders for agents do not lead to significant improvements in secure code generation, indicating that prompting alone is insufficient to enhance security practices.

<br>

üîß **On Code-Induced Reasoning in LLMs** [source](http://arxiv.org/pdf/2509.21499v1.pdf) #general 

 The key driver behind improved LLM reasoning from code data is structural regularity, not surface semantics, and a wide range of code representations‚Äîeven compact or corrupted ones‚Äîcan deliver strong benefits for reasoning tasks.
 - Disrupting the structural properties of code in training data leads to more severe drops in language model performance‚Äîespecially on math and code tasks‚Äîthan altering semantic elements such as variable names or comments.
 - Algorithmic abstractions like pseudocode and flowcharts, which maintain core structural information with fewer tokens, can match or even surpass the performance gains from conventional code, showing that verbosity or exact syntax is not required for effective reasoning.
 - Training with lower-level programming languages, such as Java and Rust, provides greater improvements for mathematical reasoning tasks, whereas higher-level languages like Python align better with natural language task performance.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **When Ads Become Profiles: Large-Scale Audit of Algorithmic Biases and
  LLM Profiling Risks** [source](http://arxiv.org/pdf/2509.18874v1.pdf) #security 

 Ad streams are rich digital footprints that enable AI-powered demographic profiling, surfacing major privacy vulnerabilities and algorithmic bias in targeted advertising.
 - Sensitive ad categories such as gambling and politics are disproportionately delivered to socioeconomically vulnerable and politically engaged groups, with men, unemployed, and less-educated users receiving up to 2‚Äì3 times higher exposure than reference demographics.
 - A state-of-the-art multimodal LLM accurately reconstructs user profiles‚Äîincluding gender, age, education, and employment status‚Äîfrom ad streams alone, achieving up to 76% accuracy for gender and outperforming census and human baselines for multiple attributes.
 - Aggregating ad sequences over time allows LLMs to make 'near-miss' demographic predictions for ordinal attributes like age and income, with user-level age inference jumping from 41% exact accuracy to 80% when allowing for adjacent brackets, thus heightening privacy risks.

<br>

üßÆ **LLM-based Vulnerability Discovery through the Lens of Code Metrics** [source](http://arxiv.org/pdf/2509.19117v1.pdf) #security 

 Surprisingly, simple code metrics enable vulnerability detection as effectively as today's most advanced language models, challenging the value of further scaling LLMs for this task.
 - A classifier utilizing only 23 basic syntactic code metrics matches the best vulnerability detection performance of state-of-the-art LLMs, but with 94% fewer parameters and no requirement for specialized hardware.
 - Individual metrics such as the number of local variables alone achieve over 90% of the detection performance, indicating that simple code statistics largely drive current results in vulnerability discovery tasks.
 - Combining LLM predictions with code metrics provides no additional benefit, with models showing strong correlation and causal dependence, revealing that LLMs rely on overlapping information rather than novel code insights.

<br>

ü¶æ **LLMs as verification oracles for Solidity** [source](http://arxiv.org/pdf/2509.19153v1.pdf) #security 

 GPT-5 elevates smart contract verification accuracy, coverage, and usability by bridging AI reasoning with formal methods, suggesting LLMs can be powerful oracles for secure blockchain development.
 - GPT-5 achieved over 92% F1 score in smart contract property verification tasks, significantly outperforming GPT-4 and state-of-the-art formal verification tools across varied Solidity use cases.
 - GPT-5 demonstrated broader coverage and higher expressibility, effectively verifying arbitrary contract-specific properties formulated in natural language, including those beyond the scope of existing symbolic tools.
 - In real-world smart contract audits, GPT-5 not only identified violations and produced valid counterexamples, but also detected inconsistencies between property descriptions and formal specifications, enhancing the audit process.

<br>

üß™ **Semantic-Aware Fuzzing: An Empirical Framework for LLM-Guided,
  Reasoning-Driven Input Mutation** [source](http://arxiv.org/pdf/2509.19533v1.pdf) #security 

 Empirical benchmarking shows that semantic-aware input mutation via reasoning-capable LLMs can enhance fuzzing efficiency, but careful prompt design and model selection are crucial for maximizing code coverage and mutation quality.
 - Reasoning-enabled large language models (LLMs) integrated into grey-box fuzzers improved mutation diversity and efficiency, achieving up to 1.54% higher branch coverage than baseline AFL++ on some benchmarks in short-term fuzzing runs.
 - Prompt engineering with zero-, one-, and three-shot learning revealed that increasing prompt shots does not consistently boost code coverage, with higher-shot prompts sometimes raising syntactic correctness but also producing more duplicate outputs.
 - Among evaluated open-source models, Deepseek-r1-Distill-Llama-70B delivered the best overall balance of output diversity and syntactic correctness, outperforming other LLMs in long-term fuzzing effectiveness and consistency.

<br>

ü§ñ **Leveraging Large Language Models for Robot-Assisted Learning of
  Morphological Structures in Preschool Children with Language Vulnerabilities** [source](http://arxiv.org/pdf/2509.22287v1.pdf) #general 

 This study reveals that LLM-powered social robots can efficiently deliver personalized language interventions and serve as consistent linguistic models for children and educators, potentially surpassing human performance in morphological teaching tasks.
 - Large language model-powered robots demonstrated effective management of game-based language interventions, such as turn-taking and error correction, with preschool children with language vulnerabilities.
 - Robot-assisted learning using LLMs can consistently generate, model, and deliver a high dose of targeted morphological structures, outperforming human educators in real-time linguistic input delivery.
 - Integration of LLM-based robots into preschool environments shows potential to act as language role models for both children and educators, enabling scalable and adaptive language interventions while reducing the demand on speech-language therapists.

<br>

üß© **PromptDebt: A Comprehensive Study of Technical Debt Across LLM Projects** [source](http://arxiv.org/pdf/2509.20497v1.pdf) #general 

 Prompt engineering‚Äîespecially instruction-based and few-shot techniques‚Äîis the primary source of technical debt in large-scale LLM development, with OpenAI and LangChain integrations most affected.
 - Over 54% of technical debt instances in LLM-powered Python projects involve OpenAI integrations, while LangChain contributes 12.35%, reflecting the challenges in managing widely adopted LLM APIs and orchestration frameworks.
 - Prompt design and configuration emerge as the leading source of LLM-specific technical debt, comprising 6.61% of case samples, followed by hyperparameter tuning and framework integration issues.
 - Instruction-based prompts (38.6%) and few-shot prompts (18.13%) are particularly vulnerable to technical debt due to the prevalence of lengthy, unclear instructions and placeholder examples, undermining output quality and maintainability.

<br>

üõ°Ô∏è **Library Hallucinations in LLMs: Risk Analysis Grounded in Developer
  Queries** [source](http://arxiv.org/pdf/2509.22202v1.pdf) #security 

 LLMs reliably generate code that imports nonexistent libraries following minor prompt variations or temporal phrasing, exposing software projects to heightened supply chain risks like typosquatting and slopsquatting.
 - Prompting large language models with year-based library requests (e.g., 'from 2025') triggers library hallucinations in up to 84% of coding tasks, substantially increasing risk compared to adjective-based prompts, which remain near 0%.
 - Minor user errors, such as one-character misspellings in library names, cause LLMs to import nonexistent libraries in up to 26% of tasks, while completely fake names are accepted and used confidently in up to 99% of tasks across several models.
 - Prompt engineering strategies offer some mitigation but remain inconsistent and model-dependent; general reasoning cues sometimes increase hallucination rates, highlighting the need for robust, LLM-specific safeguards at the interface level.

<br>

üß© **SoK: Potentials and Challenges of Large Language Models for Reverse
  Engineering** [source](http://arxiv.org/pdf/2509.21821v1.pdf) #security 

 This paper reveals that LLM-driven reverse engineering research is performance-obsessed, centered on decompiled code and prompt-based methods, but faces critical deficits in exploration, robustness, and reproducibility.
 - Over 96% of studies in LLM-powered reverse engineering (RE) focus on optimizing performance, with only 8% targeting discovery and less than 12% enhancing interpretability, signaling an overwhelming emphasis on efficiency rather than new insights or comprehension.
 - Nearly 60% of RE research leverages decompiled code as the primary target for LLM integration, while only 8% tackle raw bytes and 20% address pure source code, revealing a methodological gap in low-level and high-level software understanding.
 - Zero/few-shot prompting and data generation approaches dominate (62.9% each), but only one-third of studies employ fine-tuning or robust agent-based methods, and less than 5% use lightweight prototypes, indicating barriers to accessibility and generalized robustness.

<br>

üõ°Ô∏è **EvoMail: Self-Evolving Cognitive Agents for Adaptive Spam and Phishing
  Email Defense** [source](http://arxiv.org/pdf/2509.21129v1.pdf) #security 

 A continually self-improving email defense agent outperforms traditional systems in both accuracy and adaptability while offering interpretable audit trails against evolving spam and phishing threats.
 - EvoMail achieves superior detection performance with an accuracy of 92.8% and F1-score of 89.6%, surpassing both classical baselines and neural models across multiple public and synthetic email datasets.
 - The self-evolving adversarial training loop‚Äîwhere the system continually learns from red-team generated attacks and compresses detection failures into reusable memory‚Äîenables EvoMail to sustain performance under distribution shift, reducing F1 degradation on novel phishing attacks by 5‚Äì7% compared to leading alternatives.
 - EvoMail's heterogeneous graph fusion and LLM-driven attention mechanisms produce high interpretability (CIM = 0.70), generating structured reasoning paths that align with human analyst workflows and facilitate regulatory transparency.

<br>

üèÜ **RLBFF: Binary Flexible Feedback to bridge between Human Feedback &
  Verifiable Rewards** [source](http://arxiv.org/pdf/2509.21319v1.pdf) #general 

 Binary Flexible Feedback unlocks principled, interpretable, and efficient reward modeling and alignment for LLMs, matching commercial models at a fraction of the cost.
 - Reward models trained using Binary Flexible Feedback outperform previous state-of-the-art models on JudgeBench (81.4%, #1 on leaderboard as of September 2025) and RM-Bench (86.2%), demonstrating superior adherence to diverse evaluation principles beyond correctness.
 - The proposed technique supports inference-time customization, enabling users to specify evaluation principles and achieve high accuracy (91.6% on PrincipleBench) with sub-0.1 second latency‚Äîover 100x faster than generative baselines that require extensive computation.
 - LLMs aligned via Binary Flexible Feedback (RLBFF) match or exceed proprietary alternatives like o3-mini and DeepSeek R1 on key general alignment benchmarks, while operating at less than 5% of the inference cost, lowering barriers for open-source model deployment.

<br>

üö¶ **Online Process Reward Leanring for Agentic Reinforcement Learning** [source](http://arxiv.org/pdf/2509.19199v2.pdf) #general 

 Converting trajectory-level preferences into dense, step-level guidance via OPRL dramatically accelerates agentic RL, outperforming current baselines and enabling robust, sample-efficient long-horizon learning for LLM agents.
 - Integrating Online Process Reward Learning (OPRL) with standard RL algorithms yields state-of-the-art performance, surpassing previous multi-turn RL methods with up to 91.7% success in VisualSokoban and 93.6% score in WebShop.
 - OPRL improves goal completion in open-ended, unverifiable environments by up to 14% in self-chat interactions and 48% when negotiating with GPT-4o, demonstrating robust generalization.
 - Step-level rewards learned online in OPRL accelerate training speed, enhance sample efficiency, reduce variance, and produce more effective policies as indicated by shorter, more successful interaction episodes.

<br>

üéØ **Chasing the Tail: Effective Rubric-based Reward Modeling for Large
  Language Model Post-Training** [source](http://arxiv.org/pdf/2509.21500v1.pdf) #general 

 Careful construction and iterative refinement of rubric-based reward models, focusing on the high-reward tail with diverse exemplars, substantially improve LLM post-training effectiveness and resilience against reward over-optimization.
 - Refining reward rubrics with high-quality and diverse off-policy responses boosts win-rate performance for LLM post-training by up to 39% in general domains and 34% in specialized medical domains.
 - Accuracy in ranking top-performing (high-reward) responses, rather than correcting broader errors, is critical for mitigating reward over-optimization during reinforcement fine-tuning.
 - Iterative refinement of rubrics using excellent and semantically diverse response pairs significantly delays the onset of reward over-optimization, extending sustained performance over 2.5x more training steps compared to baseline approaches.

<br>

üß¨ **GALAX: Graph-Augmented Language Model for Explainable
  Reinforcement-Guided Subgraph Reasoning in Precision Medicine** [source](http://arxiv.org/pdf/2509.20935v1.pdf) #general 

 GALAX sets a new benchmark for interpretable therapeutic target prediction in precision medicine by pairing large language models with graph-guided, reinforcement-driven reasoning over multi-omic biomedical networks.
 - The GALAX model achieves an overall precision of 54.7% and recall of 53.3% in patient-specific cancer target identification, outperforming all language, graph, and retrieval-based baselines by a consistent margin across evaluation metrics.
 - Across 363 multi-omic CRISPR-based test cases spanning lung, breast and 20+ cancer types, GALAX delivers top-10 recall (Hit@10) of 88.2% and top-5 recall (Hit@5) of 92.5%, with enrichment analyses confirming the biological validity of identified signaling subgraphs.
 - Reinforcement-guided subgraph construction, supervised by a pretrained graph neural network, raises F1 scores by 2‚Äì5% compared to the best finetuned QA baselines and reliably uncovers mechanistically relevant pathways without requiring explicit intermediate reasoning annotations.

<br>

üîÑ **Benefits and Pitfalls of Reinforcement Learning for Language Model
  Planning: A Theoretical Perspective** [source](http://arxiv.org/pdf/2509.22613v1.pdf) #general 

 Reinforcement learning‚Äîespecially Q-learning with careful reward design‚Äîprovides principled and practical solutions for scalable and generalizable planning in large language models, overcoming the limitations of both supervised fine-tuning and policy gradient methods.
 - Supervised fine-tuning in language model planning tends to memorize spurious co-occurrence relationships instead of abstracting true graph connectivity, which limits generalization.
 - Policy gradient reinforcement learning dramatically expands training diversity via exploration, but suffers from severe diversity collapse during training‚Äîeven at perfect accuracy‚Äîunless KL regularization is introduced.
 - Q-learning, when paired with process-based intermediate rewards, reliably preserves output diversity and enables robust off-policy training, outperforming policy gradient methods in both accuracy and generalization.

<br>

‚ö° **Nova: Real-Time Agentic Vision-Language Model Serving with Adaptive
  Cross-Stage Parallelization** [source](http://arxiv.org/pdf/2509.21301v1.pdf) #general 

 Nova delivers significantly lower real-time latency and higher GPU efficiency for agentic vision-language model serving on single GPUs by combining adaptive pipeline parallelism and memory optimization.
 - Nova reduces maximum per-request latency by up to 23.3% and average latency by 14.6% compared to existing agentic VLM serving baselines, while maintaining competitive throughput.
 - Adaptive cross-stage parallelization and dynamic SM partitioning enable efficient GPU resource utilization under bursty and heterogeneous workloads, preventing resource contention and bottlenecks.
 - A lightweight weight offloading strategy for large vision encoders achieves over 90% reduction in GPU memory usage with negligible latency overhead, facilitating scalable deployment in memory-constrained settings.

<br>

üõ†Ô∏è **Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive
  Exploration for Agentic Reinforcement Learning** [source](http://arxiv.org/pdf/2509.22601v1.pdf) #general 

 SPEAR introduces a curriculum-based self-imitation strategy that dramatically boosts strategic performance and stability in LLM-powered agents while remaining efficient and broadly compatible.
 - SPEAR consistently improves the success rate of agentic Large Language Models by up to 20.7% on WebShop and 16.1% on ALFWorld, demonstrating robust generalization across diverse environments.
 - The combination of curriculum-guided exploration, self-imitation via replay buffer, and intrinsic reward shaping prevents both entropy collapse and training instability, leading to markedly better strategic tool usage and reasoning skills in LLM agents.
 - SPEAR incurs just 10%‚Äì25% additional computational overhead with virtually negligible runtime impact, making it a scalable, plug-and-play enhancement compatible with existing RL baselines such as GRPO, GiGPO, and Dr.BoT.

<br>

üß† **LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy,
  Methods, and Directions** [source](http://arxiv.org/pdf/2509.18970v1.pdf) #general 

 LLM-based agents exhibit deeply interconnected hallucinations affecting entire workflows, prompting a need for multi-level mitigation and more robust error detection across all agent modules.
 - Agent hallucinations in large language model (LLM)-based agents manifest as complex, multi-stage behavioral errors spanning reasoning, execution, perception, memory, and communication, resulting in more severe consequences than linguistic hallucinations in standalone LLMs.
 - A formal taxonomy highlights five major types of agent hallucinations with eighteen underlying causes, and ten general mitigation strategies‚Äîincluding external knowledge guidance, advanced learning paradigms, and post-hoc verification‚Äîhave been identified to address these issues.
 - Current detection approaches are predominantly concentrated on shallow modules like perception, while locating and tracing hallucinations in deeper agent layers such as memory and communication remain a significant open challenge, necessitating future research for systematic error attribution.

<br>

üõë **FreezeVLA: Action-Freezing Attacks against Vision-Language-Action Models** [source](http://arxiv.org/pdf/2509.19870v1.pdf) #security 

 FreezeVLA exposes a potent and transferable vulnerability in VLA models where adversarial images can reliably paralyze robots across a wide range of user instructions, emphasizing urgent safety concerns for AI-powered robotics.
 - Adversarial images crafted with FreezeVLA can freeze vision-language-action (VLA) models, causing robots to ignore subsequent instructions with success rates averaging 73.3% (SpatialVLA), 95.4% (OpenVLA), and 59.8% (œÄ0) across popular benchmarks.
 - FreezeVLA-generated perturbations exhibit strong cross-prompt transferability, reliably paralyzing robotic action even as textual instructions change, with attack effectiveness growing as the diversity and number of reference prompts increases.
 - Increasing the allowable perturbation and using GPT-generated prompts substantially boosts attack success, highlighting a critical safety risk that persists across model architectures and tasks and calls for robust defensive strategies in real-world robotics.

<br>

üõ°Ô∏è **SecInfer: Preventing Prompt Injection via Inference-time Scaling** [source](http://arxiv.org/pdf/2509.24967v1.pdf) #security 

 Inference-time scaling with system-prompt-guided sampling and target-task-guided aggregation enables LLMs to withstand prompt injection attacks without losing performance.
 - SecInfer reduces prompt injection attack success rates to near-zero across multiple large language models and benchmarks, maintaining task accuracy even in adversarial conditions.
 - Compared to existing prompt injection defenses and inference-time scaling methods, SecInfer consistently delivers higher robustness under attack while incurring only moderately increased computational overhead.
 - SecInfer remains effective even against strong adaptive attacks and in real-world LLM agent settings, demonstrating broad applicability beyond text-based tasks.

<br>

üõ°Ô∏è **SafeSearch: Automated Red-Teaming for the Safety of LLM-Based Search
  Agents** [source](http://arxiv.org/pdf/2509.23694v1.pdf) #security 

 Automated red-teaming reveals that LLM-powered search agents are highly vulnerable to unreliable real-time search results, including misinformation, and that current defense strategies are insufficient for robust safety.
 - LLM-based search agents propagated risky or unsafe content to users in up to 90.5% of test cases when exposed to a single unreliable website source, with misinformation posing the greatest threat category.
 - Even advanced models such as GPT-5 with tool-calling scaffolds exhibited nonzero attack success rates (ASR), indicating that no existing search agent is completely robust to unreliable results from real-time web sources.
 - Standard defense mechanisms like reminder prompting and automated search result filtering only partially reduced vulnerability, with reminder prompts failing to meaningfully improve safety and filtering roughly halving ASR but leaving substantial residual risk.

<br>

üõ°Ô∏è **ReliabilityRAG: Effective and Provably Robust Defense for RAG-based
  Web-Search** [source](http://arxiv.org/pdf/2509.23519v1.pdf) #security 

 Incorporating document reliability into RAG pipelines delivers scalable, provably robust defenses against adversarial corruption, outperforming prior methods even for complex and long-form tasks.
 - ReliabilityRAG's consistent majority algorithm achieves up to 20 percentage points higher answer accuracy and LLM-judge scores under adversarial attacks compared to previous robust RAG methods, with especially strong performance in long-form generation tasks.
 - The framework leverages document reliability signals‚Äîsuch as search engine ranking‚Äîto filter out adversarial content, yielding robust defenses that maintain high benign accuracy (typically ~70-80%) while response accuracy degrades gracefully as the number or rank of attacked documents increases.
 - Sampling-based approaches integrated with reliability weighting allow the system to efficiently scale to large document sets (e.g., k = 50) without significant latency overhead, and are provably robust under realistic threat models where high-ranked results are harder to poison.

<br>

üõ°Ô∏è **ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents** [source](http://arxiv.org/pdf/2509.22830v1.pdf) #security 

 Exploiting chat template formatting and multi-turn persuasion enables highly transferable prompt injection attacks on LLM agents, bypassing most current security defenses.
 - Template-based prompt injection attacks using native chat formatting increased attack success rates from 5.18% to 32.05% on AgentDojo and from 15.13% to 45.90% on InjecAgent, with multi-turn variants reaching 52.33%.
 - Payloads crafted with one model‚Äôs chat template demonstrated strong transferability, successfully compromising other models, including closed-source agents, particularly when template structures are similar.
 - Existing prompt-based defenses, including delimiters and instructional repetition, are largely ineffective against these template-driven multi-turn attacks, and simple template perturbations further circumvent rule-based parsing defenses.

<br>

üõ°Ô∏è **Takedown: How It's Done in Modern Coding Agent Exploits** [source](http://arxiv.org/pdf/2509.24240v1.pdf) #security 

 Modern coding agents exhibit pervasive, chainable vulnerabilities that allow attackers to execute arbitrary code and exfiltrate global data with no user involvement.
 - Five out of eight evaluated coding agents permit arbitrary command execution without any user interaction, exposing systems to critical compromise.
 - Four coding agents allow global data exfiltration, enabling adversaries to leak sensitive information from outside the designated workspace boundaries.
 - Systematic weaknesses‚Äîsuch as improper approval, tool misuse, and prompt injection‚Äîcan be chained for end-to-end exploitation, bypassing user safeguards by design or default settings.

<br>

üõ°Ô∏è **When MCP Servers Attack: Taxonomy, Feasibility, and Mitigation** [source](http://arxiv.org/pdf/2509.24272v1.pdf) #security 

 Malicious MCP servers present a widespread, hard-to-detect risk to agent ecosystems, necessitating urgent, multi-stakeholder intervention for robust security.
 - Twelve distinct categories of MCP server attacks were identified, with certain attack types achieving a 100% success rate across state-of-the-art LLM and agent host combinations, indicating a pervasive vulnerability.
 - Malicious MCP servers can be mass-produced rapidly and at virtually no cost; existing detection tools detected less than half (at best) of generated attacks, routinely missing critical threats such as code execution and output manipulation.
 - Effective mitigation of these threats requires coordinated policy, technical countermeasures, and persistent vigilance from registry platforms, host developers, LLM providers, and end users‚Äîcurrent safeguards are insufficient for real-world deployment.

<br>

üß† **LLM/Agent-as-Data-Analyst: A Survey** [source](http://arxiv.org/pdf/2509.23988v1.pdf) #general 

 LLM/Agent-as-Data-Analyst systems are redefining data analytics by integrating semantic reasoning, multi-modal understanding, and autonomous workflow orchestration, though key gaps in scalability, reasoning generalization, and domain adaptation remain.
 - LLM-powered data analysis agents enable semantic-aware, modality-hybrid, and autonomous pipelines that outperform traditional rule-based approaches in handling structured, semi-structured, unstructured, and heterogeneous data.
 - Recent advances have shown significant improvement in multimodal alignment and natural language interfacing, supporting complex analytics tasks such as multi-hop reasoning, chart QA, and 3D spatial understanding, with specialized benchmarks indicating a 20%-50% gap between current models and human performance in table reasoning.
 - Persistent challenges remain in scalability, high-level multimodal reasoning, and adaptation to open-world and diverse domain-specific tasks, prompting ongoing research into modular architectures, retrieval-augmented techniques, and dynamic agent orchestration.

<br>

üåä **MARLIN: Multi-Agent Reinforcement Learning with Murmuration Intelligence
  and LLM Guidance for Reservoir Management** [source](http://arxiv.org/pdf/2509.25034v1.pdf) #general 

 Bio-inspired multi-agent intelligence, combined with language-guided adaptation, enables robust, scalable, and fast responses to water disasters in complex infrastructure networks.
 - Adaptive murmuration coordination and LLM-guided reward shaping in reservoir networks resulted in a 23% improvement in uncertainty handling and a 68% faster flood response compared to baselines.
 - MARLIN‚Äôs architecture scales linearly, delivering real-time decision-making for networks of up to 10,000 nodes with a 35% reduction in computational resources versus traditional approaches.
 - Emergent coordination patterns produced 16.8√ó more strategic clusters and a 42% gain in regional water balance, resulting in demand satisfaction rates above 94% and sustained safety even during extreme environmental events.

<br>

üÜò **Between Help and Harm: An Evaluation of Mental Health Crisis Handling by
  LLMs** [source](http://arxiv.org/pdf/2509.24857v1.pdf) #general 

 Even the best commercial language models can miss subtle mental health crisis cues, and a non-negligible percentage of their responses may be inappropriate or harmful, especially for self-harm and suicidal ideation.
 - Across over 2,000 labeled mental health crisis inputs, state-of-the-art language models delivered generally appropriate responses, but up to 4.8% of outputs in 'self-harm' and 'suicidal ideation' categories were rated as potentially harmful or inappropriate‚Äîespecially in open-weight models.
 - Language models reliably responded to direct crisis disclosures but exhibited frequent failures‚Äîsometimes giving dangerous information or inadequate support‚Äîwhen user inputs were indirect, ambiguous, or sought information about harm methods.
 - Most models defaulted to generic, formulaic replies lacking authentic empathy, context awareness, or adequate localization, with critical implications for user trust and the risk of discouraging future help-seeking among vulnerable individuals.

<br>

üï∏Ô∏è **Uncovering Vulnerabilities of LLM-Assisted Cyber Threat Intelligence** [source](http://arxiv.org/pdf/2509.23573v1.pdf) #security 

 LLMs in cyber threat intelligence are systematically undermined by spurious correlations, conflicting sources, and poor generalization to new threats, limiting their reliability in practical security operations.
 - Large Language Models exhibit a failure rate of over 20% from spurious correlation in contextualization tasks, leading to frequent misattribution of cyber threat evidence.
 - Contradictory knowledge from inconsistent CTI sources causes unstable predictions and reasoning errors in over 35% of evaluated threat intelligence instances across all stages.
 - Constrained generalization limits LLMs‚Äô ability to handle emerging threats, with failure ratios exceeding 33% for zero-day exploit prediction and mitigation, especially in specialized cyber agents.

<br>

üõ†Ô∏è **Automated Vulnerability Validation and Verification: A Large Language
  Model Approach** [source](http://arxiv.org/pdf/2509.24037v1.pdf) #security 

 Automated, LLM-powered pipelines can reliably reconstruct and validate diverse real-world software exploits, but their effectiveness is constrained by both input data quality and the reasoning capacity of the underlying model.
 - An automated pipeline leveraging large language models and retrieval-augmented generation successfully reproduced 70% (71 out of 102) of tested software vulnerabilities in controlled, containerized environments across nine programming languages and 55 unique libraries.
 - The presence of a public proof-of-concept (PoC) significantly reduced the average number of iterations required for successful exploit reproduction, while multi-container setups were required for 28% of cases, underlining the importance of automated environment orchestration.
 - Inconsistent or incomplete CVE descriptions frequently hinder reproducibility, highlighting the need for more rigorous verification in vulnerability disclosures, and model capability remains the primary determinant of successful automated exploit generation.

<br>

üõ°Ô∏è **Binary Diff Summarization using Large Language Models** [source](http://arxiv.org/pdf/2509.23970v1.pdf) #security 

 Automated binary diff summarization with LLMs delivers highly accurate malware detection in software supply chains and introduces the FSS metric for precise function triage, outperforming prior methods and proving efficacy in real-world attacks.
 - The large language model-based binary diff summarization framework achieved a malware detection precision of 0.98 and recall of 0.64 on a benchmark comprising 104 versions from 6 open-source projects injected with three malware types, demonstrating high accuracy and low false positive rates.
 - The functional sensitivity score (FSS) method reliably distinguished malicious from benign functions, with a median separation of 3.0 points, facilitating automated triage of sensitive code changes in binary diffs.
 - In a real-world case study of the XZ Utils supply chain attack, the framework successfully detected injected backdoor functions with high FSS values, validating its effectiveness for practical software supply chain security scenarios.

<br>

üß¨ **Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement
  Learning** [source](http://arxiv.org/pdf/2509.24372v1.pdf) #general 

 Scaling Evolution Strategies to multi-billion parameter LLMs delivers surprising improvements in efficiency, robustness, and reliability over reinforcement learning-based fine-tuning.
 - Evolution Strategies (ES) can efficiently fine-tune large language models with billions of parameters, outperforming RL methods in accuracy and sample efficiency across multiple tasks and model sizes.
 - ES demonstrates greater robustness and consistency than RL, improving even the smallest base models and delivering stable results across different runs and hyperparameter configurations.
 - Unlike RL methods, which are prone to reward hacking and sensitive to reward structure, ES reliably avoids reward exploitation and achieves superior tradeoffs between task performance and model stability without intricate penalties.

<br>

üß© **Causally-Enhanced Reinforcement Policy Optimization** [source](http://arxiv.org/pdf/2509.23095v1.pdf) #general 

 Causally-enhanced reward shaping delivers higher accuracy and more robust, coherent reasoning in large language models by discouraging shortcut exploitation.
 - Integrating a causal coherence reward with standard reinforcement learning objectives in language models results in an average accuracy improvement of 5.49% (up to 9.58%) across multiple reasoning benchmarks.
 - Models trained with causally-enhanced policy optimization show increased robustness to reward hacking behaviors and maintain stable response lengths, indicating reduced reliance on superficial cues like output length.
 - Jacobian-based causal scores significantly align with ground-truth reasoning validity, as confirmed by statistical tests (p < 0.01), and provide orthogonal signals to surface-level accuracy metrics, contributing to more faithful stepwise reasoning.

<br>

ü¶† **Virus Infection Attack on LLMs: Your Poisoning Can Spread "VIA"
  Synthetic Data** [source](http://arxiv.org/pdf/2509.23041v1.pdf) #security 

 A new attack can stealthily and widely propagate poison through synthetic data pipelines, overcoming previous distributional barriers and posing significant threats to LLM supply chains.
 - Standard synthetic-data-based training for large language models demonstrates strong resistance to mainstream poisoning and backdoor attacks, with less than 0.1% of synthetic samples showing malicious content even under high upstream poisoning rates.
 - The proposed Virus Infection Attack (VIA) dramatically increases the infection rate of poisoning‚Äîup to 85% in synthetic data‚Äîby embedding adversarial payloads within benign samples, enabling propagation of attacks to downstream models via synthetic training.
 - While VIA achieves a notably higher infection rate for downstream models compared to prior methods, it introduces a trade-off: attack success rates on upstream models decrease, but stealthiness is increased through novel shell wrapping strategies.

<br>

üöó **FuncPoison: Poisoning Function Library to Hijack Multi-agent Autonomous
  Driving Systems** [source](http://arxiv.org/pdf/2509.24408v1.pdf) #security 

 Function-level template poisoning enables precise, persistent, and stealthy attacks on multi-agent LLM-based autonomous driving systems by exploiting a new, undersecured attack surface.
 - Injecting malicious templates into function descriptions in shared libraries hijacks function selection in multi-agent autonomous driving systems, achieving attack success rates exceeding 86% and resulting in significant trajectory deviations and higher collision rates.
 - FuncPoison attacks persistently and stealthily propagate through agent chains, causing cascading failures that existing prompt-level and agent-level defenses fail to detect or mitigate effectively.
 - Template-injected function descriptions are selected 98% of the time and produce up to 86.3% attack success rate, making function-level poisoning a far more effective attack vector than prompt or model-data poisoning in LLM-driven safety-critical applications.

<br>

üõ°Ô∏è **What Do They Fix? LLM-Aided Categorization of Security Patches for
  Critical Memory Bugs** [source](http://arxiv.org/pdf/2509.22796v1.pdf) #security 

 Leveraging large and small language models jointly enables the accurate discovery and classification of critical memory corruption patches, revealing previously missed high-impact Linux kernel vulnerabilities‚Äîsome exploitable for control-flow hijacking.
 - The dual-model DUALLM pipeline achieves an 87.4% accuracy and 0.875 F1-score in fine-grained classification of Linux kernel security patches, outperforming previous approaches by more than 20 percentage points.
 - DUALLM successfully identified 111 out of 5,140 recent Linux kernel patches as addressing critical out-of-bounds (OOB) or use-after-free (UAF) vulnerabilities, with 90 cases confirmed by manual verification and at least one exploited for previously unknown control-flow hijack.
 - Integrating commit titles, messages, and custom program slices enables robust detection of security-critical patches even when CVE indicators are absent, dramatically reducing both false positives and false negatives compared to existing rule-based and machine-learning methods.

<br>

üß© **Beyond Embeddings: Interpretable Feature Extraction for Binary Code
  Similarity** [source](http://arxiv.org/pdf/2509.23449v1.pdf) #security 

 Interpretable feature extraction via language models offers training-free, scalable, and highly accurate binary code similarity detection, making clone search both more effective and transparent than embedding-only approaches.
 - A language model approach that produces human-interpretable, structured features from assembly code enables training-free binary code similarity detection, achieving 42% recall@1 in cross-architecture and 62% recall@1 in cross-optimization scenarios‚Äîmatching or exceeding many embedding-based methods that require training.
 - Combining interpretable LLM-derived features with traditional embedding-based models results in significant performance improvements, with hybrid recall@1 scores surpassing both methods in isolation and outperforming prior baselines by up to 25‚Äì30 percentage points in challenging settings.
 - Text-based, interpretable feature extraction supports scalable, exact search using inverted indexes, overcoming the efficiency-accuracy trade-off inherent to high-dimensional vector embeddings and making large-scale function-level retrieval both practical and explainable.

<br>

ü¶æ **Agentic Reinforcement Learning with Implicit Step Rewards** [source](http://arxiv.org/pdf/2509.19199v3.pdf) #general 

 Step-level implicit rewards yield superior, efficient LLM agent training and robust generalization for complex tasks.
 - Integrating implicit step rewards in agentic reinforcement learning boosts long-horizon performance, achieving state-of-the-art results with up to 93.6% success score in WebShop and 91.7% success in VisualSokoban.
 - Goal completion rates in open-ended multi-agent interactions increased by as much as 14% in self-chat scenarios and 48% when interacting with GPT-4o, demonstrating strong generalizability to unverifiable rewards.
 - The iStar strategy provides consistently higher sample efficiency and training stability compared to token-level or vanilla RL baselines, reaching desired performance metrics in half the training steps and with fewer unnecessary actions.

<br>

