üõ°Ô∏è **Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition** [source](http://arxiv.org/pdf/2507.20526v1.pdf) #security 

 Even the most advanced AI agents are universally and repeatedly vulnerable to prompt injection attacks, regardless of model scale or sophistication, exposing serious security gaps that demand dedicated mitigations.
 - Over 1.8 million adversarial attacks submitted in a public red-teaming competition resulted in a near-100% policy violation rate across 22 state-of-the-art AI agents spanning 44 realistic deployment scenarios.
 - Indirect prompt injection techniques, especially those leveraging third-party data sources, achieved a markedly higher average success rate (27.1%) compared to direct prompt attacks (5.7%), indicating a major vulnerability in agentic deployments involving external data.
 - Attack success rates and agent robustness showed little correlation with factors like model size, capability, or inference-time compute, while successful attacks exhibited high transferability and universality across models and tasks, highlighting shared systemic weaknesses.

<br>

üñºÔ∏è **Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual Language Models** [source](http://arxiv.org/pdf/2507.20704v1.pdf) #security 

 Multimodal typographic attacks can significantly undermine the safety alignment of visual language models, exposing serious weaknesses in current open-source systems.
 - Open-source visual language models exhibited a substantial increase in unsafe responses‚Äîup to 50% lower refusal rates‚Äîwhen tested using multimodal typographic prompt injections compared to text-only inputs, highlighting critical vulnerabilities in their alignment mechanisms.
 - Human evaluators rated the automated prompt summarization and salient concept extraction stages of the Text2VLM pipeline as 'Good' or 'Great' in over 90% of cases, demonstrating the pipeline's robustness and reliability for creating challenging multimodal test datasets.
 - Compared to text-only prompts, key open-source VLMs struggled notably with understanding and safely handling inputs split across text and typographic images, suggesting a significant gap between their performance and that of closed-source frontier models, especially in medical and cybersecurity contexts.

<br>

üõ°Ô∏è **Counterfactual Evaluation for Blind Attack Detection in LLM-based Evaluation Systems** [source](http://arxiv.org/pdf/2507.23453v1.pdf) #security 

 Counterfactual prompts dramatically improve the security of LLM-based evaluation systems against subtle prompt injection attacks, with effectiveness tied closely to evaluator model capabilities.
 - Combining counterfactual evaluation (CFE) with standard evaluation (SE) increases attack detection F1 scores to as high as 99.8% for advanced proprietary LLMs compared to severe vulnerability under SE alone, where attack success rates were up to 99.8%.
 - Open-source models achieve moderate improvements using SE+CFE, with Gemma-12B reaching 89.3% accuracy, but overall robustness against blind attacks is lower than top proprietary models and varies notably with model capacity.
 - Detection of blind attacks, where adversarial responses ignore the true answer, largely depends on the linguistic and reasoning abilities of the evaluator model, with less capable models (e.g., GPT-3.5-turbo) exhibiting weaker attack detection compared to newer, larger models (GPT-4o, o1).

<br>

üñºÔ∏è **Invisible Injections: Exploiting Vision-Language Models Through Steganographic Prompt Embedding** [source](http://arxiv.org/pdf/2507.22304v1.pdf) #security 

 Sophisticated steganographic attacks can invisibly inject prompts into images to manipulate vision-language models, exposing moderate yet meaningful vulnerabilities even in advanced AI systems and underscoring the need for layered, adaptive defenses.
 - Steganographic prompt injection attacks on vision-language models succeeded in 24.3% of cases overall‚Äîrising to 31.8% with advanced neural embedding‚Äîwhile remaining visually imperceptible to human observers and statistical detectors in most instances.
 - Commercial vision-language models, such as GPT-4V and Claude, exhibited lower susceptibility (14‚Äì18% attack success rate) than open-source models (25‚Äì37%), but all were vulnerable to some degree, highlighting systemic risks across architectures.
 - A layered defense framework combining preprocessing, statistical analysis, neural detection, and behavioral monitoring mitigated up to 73.4% of attacks, but maintaining strong protection requires ongoing adaptive updates due to the evolving nature of embedding strategies.

<br>

üõ°Ô∏è **Role-Aware Language Models for Secure and Contextualized Access Control in Organizations** [source](http://arxiv.org/pdf/2507.23465v1.pdf) #security 

 Role-aware language models can be reliably fine-tuned to enforce secure, context-dependent access control in organizations, even under adversarial and complex hierarchical conditions.
 - Instruction-tuned large language model classifiers enforce role-based access control with up to 90% accuracy, outperforming traditional classifiers and generation-based approaches in organizational settings.
 - Models demonstrated near-perfect accuracy (‚âà100%) in blocking access requests from random or external roles, while detection of subtle role mismatches was moderately robust (‚âà70% accuracy), indicating increased difficulty with fine-grained violations.
 - Robustness analysis showed models trained on adversarial jailbreak samples were up to 17 percentage points more effective at resisting prompt injection (87% vs. 70% accuracy), and blacklisted content was successfully restricted with >99% accuracy regardless of user role.

<br>

ü¶π‚Äç‚ôÇÔ∏è **Enhancing Jailbreak Attacks on LLMs via Persona Prompts** [source](http://arxiv.org/pdf/2507.22171v1.pdf) #security 

 Persona-based prompts, evolved via genetic algorithms, dramatically undermine LLM safety alignment and make jailbreak attacks much more effective, even across diverse models and in the face of standard defenses.
 - Engineered persona prompts can decrease refusal rates from large language models (LLMs) by 50‚Äì70% across tested systems, exposing critical weaknesses in existing safety defenses.
 - Combining crafted persona prompts with state-of-the-art jailbreak attack methods increases harmful response success rates by 10‚Äì20%, demonstrating a strong synergistic effect.
 - Persona prompt vulnerabilities and their attack effectiveness transfer across multiple LLM architectures, and these strategies remain resilient even when confronted with common prompt-level defense techniques.

<br>

üõ°Ô∏è **Strategic Deflection: Defending LLMs from Logit Manipulation** [source](http://arxiv.org/pdf/2507.22160v1.pdf) #security 

 Strategic Deflection enables LLMs to neutralize advanced jailbreak attacks at the logit level, dramatically improving security while preserving core capabilities.
 - Strategic Deflection (SDeflection) reduces the success of logit manipulation jailbreak attacks on language models by up to 90%, dropping attack success rates (ASR) from 89‚Äì95% to as low as 8.5% without sacrificing model performance on benign tasks.
 - SDeflection-finetuned models maintain high refusal rates (over 85‚Äì99%) against directly harmful prompts, indicating preserved explicit safety alignment even after defensive adaptation.
 - General accuracy on standard benchmarks, such as MMLU and TruthfulQA, remains virtually unchanged after applying SDeflection, showing that enhanced robustness against adversarial attacks does not degrade helpfulness or factual performance.

<br>

üß† **Exploiting Synergistic Cognitive Biases to Bypass Safety in LLMs** [source](http://arxiv.org/pdf/2507.22564v1.pdf) #security 

 Multi-bias adversarial prompts dramatically increase the failure rate of LLM safety mechanisms, exposing a critical and underexplored psychological vector for jailbreak attacks.
 - Adversarial prompts exploiting combinations of cognitive biases can bypass safety mechanisms in 30 tested large language models, achieving an average attack success rate (ASR) of 60.1% versus 31.6% for previous state-of-the-art black-box methods.
 - The most effective jailbreaks commonly use synergistic pairs of biases such as authority bias, optimism bias, and hot-hand fallacy, with attacks using 2‚Äì5 combined biases dominating successful adversarial prompt generation.
 - Open-source LLMs remain notably more vulnerable to cognitive bias-based adversarial attacks than closed-source commercial models, underscoring gaps in current safety alignment strategies that fail to address psychological manipulation tactics.

<br>

üö® **Jailbreaking Large Language Diffusion Models: Revealing Hidden Safety Flaws in Diffusion-Based Text Generation** [source](http://arxiv.org/pdf/2507.19227v1.pdf) #security 

 Despite structural differences that initially thwart traditional attacks, LLDMs are highly susceptible to novel jailbreak strategies capable of rapidly producing high-quality harmful content, exposing critical safety gaps.
 - A targeted jailbreak attack (PAD) against Large Language Diffusion Models (LLDMs) achieved a 97% attack success rate, demonstrating these models' vulnerability to harmful output generation.
 - LLDMs can generate harmful content at double the speed of traditional autoregressive LLMs when compromised, increasing the risk of rapid, large-scale abuse.
 - Existing jailbreak strategies for standard LLMs are largely ineffective against LLDMs due to architectural differences, but tailored attacks exploiting parallel denoising can systematically bypass safety measures.

<br>

üõë **Anyone Can Jailbreak: Prompt-Based Attacks on LLMs and T2Is** [source](http://arxiv.org/pdf/2507.21820v1.pdf) #security 

 Prompt-based jailbreaks by lay users remain alarmingly effective across major language and image models, exposing gaps in current moderation systems.
 - Low-effort prompt-based attacks, such as narrative misdirection and material substitution, achieve over 80% success rates in bypassing content moderation on both state-of-the-art language and text-to-image models.
 - Multi-turn prompting, euphemistic framing, and fictional or professional impersonation tactics consistently evade safety checks, with some models remaining highly vulnerable even after recent alignment and filtering updates.
 - Moderation pipelines heavily reliant on keyword filters and static policies fail to detect contextually framed or multi-modal threats, leading to systemic blind spots exploitable by general users without technical skills.

<br>

üß≠ **The Blessing and Curse of Dimensionality in Safety Alignment** [source](http://arxiv.org/pdf/2507.20333v1.pdf) #security 

 Scaling LLMs improves capabilities but introduces exploitable linear vulnerabilities for safety alignment, which can be mitigated by dimension reduction techniques like FJLT and Bottleneck layers.
 - Large language models with higher hidden dimensions are significantly more susceptible to linear jailbreaking attacks (such as ActAdd), with models above 2,000 dimensions demonstrating strong linear representations of abstract safety concepts that adversaries can exploit.
 - Reducing the dimensionality of internal representations through projection methods like the Fast Johnson‚ÄìLindenstrauss Transform (FJLT) or the insertion of a Bottleneck layer substantially increases resistance to linear jailbreak attacks while preserving essential safety alignment, as demonstrated by refusal and safety scores that returned to nearly uncompromised baseline levels.
 - While dimensionality reduction methods offer robust defense against linear attack vectors, they maintain model utility for general instruction tasks but may reduce performance on more specialized domains and are less effective against non-linear adversarial attacks such as the Greedy Coordinate Gradient (GCG) method.

<br>

üõ°Ô∏è **MOCHA: Are Code Language Models Robust Against Multi-Turn Malicious Coding Prompts?** [source](http://arxiv.org/pdf/2507.19598v1.pdf) #security 

 Multi-turn adversarial prompts that break down malicious tasks evade current code LLM defenses, but targeted fine-tuning with realistic multi-turn data like MOCHA significantly strengthens model robustness without harming coding ability.
 - State-of-the-art code generation language models, both open- and closed-source, fail to reliably reject adversarial prompts, with average rejection rates as low as 2.5% and as high as 54.5%, and they are significantly more vulnerable to multi-turn attacks, where rejection rates can drop by over 50% between single-turn and multi-turn scenarios.
 - Fine-tuning models on the MOCHA benchmark‚Äîa dataset of over 10,000 high-fidelity single- and multi-turn malicious prompts spanning 13 threat categories‚Äîsubstantially increases rejection rates (up to 36 percentage points), without meaningfully degrading general-purpose code generation utility.
 - Models fine-tuned with MOCHA data exhibit improved generalization, achieving up to 32.4% higher rejection rates on multiple external adversarial code benchmarks, indicating greater robustness to previously unseen attacks and highlighting the need for multi-turn context-aware safety alignment.

<br>

üõ°Ô∏è **Large Language Model-Based Framework for Explainable Cyberattack Detection in Automatic Generation Control Systems** [source](http://arxiv.org/pdf/2507.22239v1.pdf) #cyber 

 Combining fast machine learning-based attack detection with large language model-generated explanations delivers both real-time security and human-readable forensic insights for smart grid protection.
 - LightGBM-based machine learning models achieved up to 95.13% accuracy for detecting false data injection attacks on Automatic Generation Control systems, with an inference latency of just 0.004 seconds.
 - GPT-4o mini, when used with 20-shot prompting for attack explanation, demonstrated 93% accuracy in identifying the attack target, with a mean absolute error of 0.075 pu for attack magnitude, and a 2.19-second MAE for attack onset estimation.
 - The hybrid ML‚ÄìLLM framework successfully balances real-time detection with operator-friendly, interpretable cybersecurity explanations, thereby enhancing actionable trust and operational transparency for smart grid operators.

<br>

üß™ **Prompt Optimization and Evaluation for LLM Automated Red Teaming** [source](http://arxiv.org/pdf/2507.22133v1.pdf) #security 

 Evaluating and optimizing adversarial prompts using repeated trials and ASR distribution significantly enhances red-teaming effectiveness against LLMs by revealing exploitable generator patterns.
 - Applying the Attack Success Rate (ASR) metric to individual attacks‚Äîby rerunning each adversarial prompt multiple times against randomized LLM targets‚Äîreveals nuanced distributional patterns that single-try ASR metrics overlook, enabling richer assessment of attack discoverability and generator consistency.
 - Optimizing attack generator prompts using ASR-delta pair mining and OPRO (Optimization by PROmpting) methods leads to a significant rightward shift in the ASR distribution, meaning the resulting generators consistently produce a higher fraction of successful attacks compared to less nuanced single-try optimization strategies.
 - Experimental results demonstrate that leveraging semantic similarity and ASR separation between attacks enables systematic identification of critical linguistic features underlying successful adversarial prompts, offering concrete pathways to improve red teaming and prompt engineering for LLM safety evaluation.

<br>

üé≠ **Can We End the Cat-and-Mouse Game? Simulating Self-Evolving Phishing Attacks with LLMs and Genetic Algorithms** [source](http://arxiv.org/pdf/2507.21538v1.pdf) #security 

 Self-evolving AI-powered phishing attacks can adapt faster than static defenses, using advanced psychological tactics to evade even well-informed users.
 - Phishing attack strategies generated through an iterative combination of large language models (LLMs) and genetic algorithms rapidly evolve to employ increasingly sophisticated psychological manipulation techniques, surpassing conventional LLM-created phishing content in effectiveness over generations, as demonstrated by a rise in simulated victim susceptibility scores from 4.6 to 7.0 out of 10 after 30 iterations.
 - Attack strategies dynamically adapt to the prior knowledge and awareness of simulated victims, successfully circumventing standard phishing indicators and even exploiting nuanced vulnerabilities when facing victims with comprehensive psychological training, leading to persistent‚Äîthough diminished‚Äîincreases in click likelihood despite enhanced defenses.
 - A pronounced cat-and-mouse dynamic emerges in adversarial simulations, with attacks continuously diversifying while defensive knowledge converges to more generic rules, highlighting an asymmetry where attackers iterate and adapt faster than defenders can update, thereby reinforcing the necessity for proactive, adaptive cybersecurity measures beyond static awareness training.

<br>

üõ°Ô∏è **SDD: Self-Degraded Defense against Malicious Fine-tuning** [source](http://arxiv.org/pdf/2507.21182v1.pdf) #security 

 A new defense renders language models unresponsive to harmful fine-tuning attempts while preserving utility for legitimate users.
 - Deploying the Self-Degraded Defense (SDD) method in large language models results in a 0% harmfulness rate, even after exposure to malicious fine-tuning with up to 100 harmful samples, outperforming prevailing safety defenses.
 - When subjected to benign fine-tuning, SDD-protected models maintain general task performance equivalent to unprotected models, ensuring that user utility is preserved for non-malicious purposes.
 - Malicious fine-tuning on SDD-aligned models leads to a significant drop in general capability scores (up to 59% decline), rendering them ineffective at producing both benign and harmful outputs, which limits their potential for misuse.

<br>

üõ°Ô∏è **SAEL: Leveraging Large Language Models with Adaptive Mixture-of-Experts for Smart Contract Vulnerability Detection** [source](http://arxiv.org/pdf/2507.22371v1.pdf) #security 

 Incorporating LLM-generated explanations and adaptive feature weighting in SAEL yields significant accuracy gains and state-of-the-art performance for smart contract vulnerability detection, particularly excelling in zero-shot and complex scenarios.
 - The SAEL framework achieved state-of-the-art F1-scores in smart contract vulnerability detection, surpassing existing techniques by up to 13.32% on delegatecall, 10.67% on integer overflow/underflow, 3.16% on timestamp dependency, and 2.33% on reentrancy vulnerabilities.
 - Dynamic integration of raw code features, LLM-generated explanations, and LLM predictions through the Adaptive Mixture-of-Experts architecture not only improved detection accuracy but also enabled robust zero-shot vulnerability identification, maintaining F1-scores of over 81% across all vulnerability types in unseen data.
 - LLM-generated natural language explanations, when used as explicit features, significantly enhanced the semantic understanding of smart contracts, resulting in higher detection performance compared to using code-only or prediction-only features.

<br>

ü©π **Repairing vulnerabilities without invisible hands. A differentiated replication study on LLMs** [source](http://arxiv.org/pdf/2507.20977v1.pdf) #security 

 The study reveals that LLMs succeed at automated vulnerability repair mainly by memorizing previously seen fixes, and their apparent robustness is compromised when prompt localization is imperfect or when patches are genuinely novel.
 - Large language models (LLMs) exhibit statistically equivalent vulnerability repair performance even when incorrect or shifted vulnerability localizations are introduced in their prompts, indicating a strong reliance on memorization rather than genuine reasoning or generalization.
 - Prompt variations, including the inclusion or exclusion of vulnerability type or localization information, do not significantly affect the number of correct patches generated, with equivalence tests confirming that added contextual clues provide little to no benefit for patch correctness.
 - When evaluated under more rigorous, refactored benchmarks where fixes are unseen during training, LLMs' performance in generating correct security patches declines, underscoring concerns about overestimating their repair capabilities due to potential training data leakage.

<br>

üõ°Ô∏è **Out of Distribution, Out of Luck: How Well Can LLMs Trained on Vulnerability Datasets Detect Top 25 CWE Weaknesses?** [source](http://arxiv.org/pdf/2507.21817v1.pdf) #security 

 High-quality, LLM-validated datasets and targeted synthetic augmentation are essential for reliable vulnerability detection in machine learning systems; otherwise, models risk severe overfitting and poor real-world performance.
 - Machine learning models for vulnerability detection often show a generalization accuracy drop of up to 41% when evaluated on high-quality, independent benchmarks compared to self-testing on their own training data, highlighting a serious overfitting problem in existing vulnerability datasets.
 - A large, rigorously curated dataset (TitanVul), validated by a multi-agent LLM framework, enables models to achieve robust generalization, with accuracy increasing by 31% (from 0.584 to 0.767) on an external benchmark (BenchVul), outperforming all existing datasets.
 - Synthesizing realistic, context-aware vulnerability examples for underrepresented categories using LLM-driven workflows boosts overall detection accuracy by 14%, and for rare weaknesses such as hard-coded credentials, accuracy improvements reach 38%.

<br>

üõ°Ô∏è **A Systematic Literature Review on Detecting Software Vulnerabilities with Large Language Models** [source](http://arxiv.org/pdf/2507.22659v1.pdf) #security 

 This comprehensive review exposes critical gaps in dataset diversity, detection granularity, and cross-study comparability that impede the practical and rigorous application of LLMs in software vulnerability detection.
 - Binary classification‚Äîsimply identifying whether code is vulnerable or not‚Äîremains the dominant approach, accounting for over two-thirds (approx. 69%) of studies, but this lacks the detail needed for practical remediation efforts.
 - Most datasets and detection methods are heavily skewed toward C/C++ code and a small set of vulnerability types, leading to poor coverage of many real-world and less-common vulnerability classes and limiting generalizability.
 - Fragmented use of datasets, inconsistent evaluation protocols, and lack of standardized splits significantly hinder comparability and reproducibility, with only around 8 datasets reused in more than 10 studies out of 158 analyzed.

<br>

üåç **The Carbon Cost of Conversation, Sustainability in the Age of Language Models** [source](http://arxiv.org/pdf/2507.20018v2.pdf) #general 

 The accelerating development of large language models is causing outsized, often hidden environmental harm‚Äîfrom massive carbon emissions and freshwater consumption to e-waste‚Äîmaking sustainable AI a critical global challenge that requires urgent, transparent, and equitable action.
 - Training a single large language model (LLM) like GPT-3 can consume over 1,200 megawatt-hours of electricity, emitting more than 550 metric tons of CO‚ÇÇ‚Äîcomparable to the yearly emissions of 50 cars‚Äîwhile inference and repeated retraining can multiply the total carbon footprint several-fold.
 - Data center cooling accounts for up to 40% of total energy consumption and can use hundreds of thousands of liters of freshwater per training run, exacerbating water scarcity and putting stress on vulnerable ecosystems, particularly in drought-prone regions.
 - Current industry efforts toward sustainability, such as carbon offset programs and energy efficiency improvements, are undermined by greenwashing and lack of regulatory standards, with 60% of tech firms omitting major supply chain emissions (Scope 3), thereby masking a significant portion of their actual environmental impact.

<br>

üõ°Ô∏è **Secure coding for web applications: Frameworks, challenges, and the role of LLMs** [source](http://arxiv.org/pdf/2507.22223v1.pdf) #security 

 The effectiveness of secure web application development hinges on a combination of structured frameworks, hands-on training, and complementary use of AI tools, with persistent education and process integration needed to close the current security gaps.
 - Over 50% of organizations and developers report inadequate training and knowledge as major barriers to adopting secure coding practices, directly correlating to persistent vulnerabilities such as SQL injection and cross-site scripting in web applications.
 - While AI-powered large language models can reliably identify basic and syntactic vulnerabilities, such as SQL injection, they often fail to detect complex, context-specific security flaws, highlighting the need for expert oversight in secure code review.
 - The integration of Security-as-Code within DevSecOps pipelines and embedding security training in educational curricula are identified as critical steps for future-proofing web application development against evolving cyber threats.

<br>

ü§ñ **Vibe Modeling: Challenges and Opportunities** [source](http://arxiv.org/pdf/2507.23120v1.pdf) #general 

 Vibe modeling introduces a hybrid low-code approach where AI-powered conversational modeling bridges the gap between natural language input and reliable, validated software generation.
 - Vibe modeling leverages AI agents to generate abstract software models through natural language conversation, allowing both technical and non-technical users to validate models before deterministic code generation.
 - The integration of standard protocols like the Model Context Protocol (MCP) enables seamless collaboration between AI agents and diverse modeling platforms, addressing interoperability and facilitating multi-agent, multi-tool development scenarios.
 - Key challenges remain in training specialized modeling agents, handling uncertainty and traceability in model evolution, adapting the process for varying user profiles, and modeling less common or AI-specific diagrams.

<br>

üìâ **Empirical Evaluation of Concept Drift in ML-Based Android Malware Detection** [source](http://arxiv.org/pdf/2507.22772v1.pdf) #security 

 Despite advances in machine learning, adapting to evolving Android malware remains a major challenge as concept drift causes significant reductions in detection performance over time, and current mitigation strategies like balancing or advanced models offer only partial relief.
 - Concept drift is prevalent in Android malware detection, with model accuracy dropping significantly‚Äîby as much as 20%‚Äîwhen models are tested on data from different years than they were trained on, regardless of feature type or algorithm used.
 - Balancing class distributions through oversampling/undersampling improves model stability over time but does not eliminate concept drift, which primarily arises from the evolving nature of malware rather than model or feature selection alone.
 - Large Language Models using few-shot techniques demonstrate promising detection performance but remain susceptible to concept drift, especially when token or context limitations restrict the information provided at inference.

<br>

üñºÔ∏è **LLM-Based Identification of Infostealer Infection Vectors from Screenshots: The Case of Aurora** [source](http://arxiv.org/pdf/2507.23611v1.pdf) #security 

 Automated LLM-driven screenshot analysis enables rapid, large-scale identification of infostealer infection vectors and campaign tactics, revealing persistent reliance on social engineering and mainstream platforms for malware delivery.
 - LLM-based screenshot analysis identified 337 actionable malicious URLs and 246 relevant files from 1,000 Aurora infostealer infection screenshots, mapping key malware distribution channels and infection vectors.
 - Cracked software and gaming mods were the two main lures, responsible for 28.3% and 7.4% of infections respectively, with YouTube videos, file distribution platforms, and Google Ads serving as primary delivery mechanisms.
 - The LLM demonstrated high accuracy in scene (96%) and suspicious element identification (87%), but exhibited inconsistent performance with browser tab recognition, suggesting a hybrid approach with human review may be optimal.

<br>

üõ°Ô∏è **Core Safety Values for Provably Corrigible Agents** [source](http://arxiv.org/pdf/2507.20964v1.pdf) #security 

 Provable corrigibility in AI agents can be made practical through a multi-headed utility system, precise safety dominance, and tractable, privacy-preserving finite-horizon audits.
 - A five-headed, lexicographically-weighted utility framework‚Äîprioritizing deference, switch-access preservation, truthfulness, low-impact behavior, and bounded task reward‚Äîguarantees corrigibility and net human benefit in multi-step, partially observed settings, even with learned approximation errors.
 - Global, unbounded verification of AI safety is formally undecidable, but within a fixed (finite) horizon, safety auditing becomes tractable, privacy-preserving, and verifiable using statistical zero-knowledge proofs.
 - The remaining risk of reward hacking is shifted from hidden incentive leakage to the standard challenge of accurate signal and data coverage, enabling clear implementation guidance for both current language models and future autonomous agents.

<br>

üß† **Dark Side of Modalities: Reinforced Multimodal Distillation for Multimodal Knowledge Graph Reasoning** [source](http://arxiv.org/pdf/2507.20738v1.pdf) #general 

 Dynamic selection of helpful multimodal knowledge sources and decoupled knowledge distillation together propel knowledge graph reasoning beyond existing methods, especially when data is incomplete.
 - Employing reinforced multimodal knowledge distillation leads to mean reciprocal rank (MRR) improvements exceeding 10% over previous state-of-the-art for multimodal knowledge graph reasoning across multiple datasets.
 - Dynamically selecting and combining only helpful modalities for each input instance‚Äîrather than fusing all available modalities‚Äîsignificantly enhances both effectiveness and robustness, even when up to 80% of modalities are missing.
 - Neighbor-decoupled knowledge distillation, which separately models correlations among both true and non-target entities, provides stronger supervisory signals and results in higher performance than traditional distillation or adaptive fusion methods.

<br>

üéÆ **Minding Motivation: The Effect of Intrinsic Motivation on Agent Behaviors** [source](http://arxiv.org/pdf/2507.19725v1.pdf) #general 

 Intrinsic motivation shapes agent behaviors in both beneficial and sometimes unexpected ways, with policy-invariant methods like GRM offering partial but not absolute safeguards against unintended exploration biases.
 - Intrinsic motivation methods such as State Count and Intrinsic Curiosity Model (ICM) accelerate initial learning and increase the likelihood of reinforcement learning agents discovering optimal or near-optimal policies in sparse reward environments.
 - The use of intrinsic motivation can significantly alter agent behavior, sometimes leading to reward hacking or over-exploration, with State Count exhibiting the most exploratory but variably optimal behaviors, while Max Entropy often results in confined and risk-averse policies.
 - Generalized Reward Matching (GRM) mitigates policy divergence and reward hacking in certain scenarios, smoothing or stabilizing agent behavior, but empirical results show that theoretical guarantees of policy invariance may require sufficiently long training to fully manifest.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **Trivial Trojans: How Minimal MCP Servers Enable Cross-Tool Exfiltration of Sensitive Data** [source](http://arxiv.org/pdf/2507.19880v1.pdf) #security 

 AI tool integration protocols like MCP present a major security risk, as even unsophisticated attackers can orchestrate high-impact cross-server data leaks with minimal effort.
 - A cross-server data exfiltration attack using the Model Context Protocol (MCP) can be executed by minimally skilled adversaries with only basic Python knowledge and free web tools, successfully extracting sensitive data such as bank account balances.
 - Current MCP security measures rely on user trust and are enforced at the client rather than the protocol level, allowing malicious servers to blend benign and malicious requests, making social engineering attacks highly effective.
 - The absence of enforced access boundaries and capability-based permissions enables any installed MCP server to trigger actions on other servers, dramatically expanding the attack surface for trivial data leaks in multi-server environments.

<br>
