ü§ñ **Agents of Chaos** [source](http://arxiv.org/pdf/2602.20021v1.pdf) #security `9/10` 

 *Natalie Shapira, Chris Wendler, Avery Yen et al. (Northeastern University, Independent Researcher, Stanford University, University of British Columbia, Harvard University, Hebrew University, Max Planck Institute for Biological Cybernetics, MIT, Tufts University, Carnegie Mellon University, Alter, Technion, Vector Institute)*

 Putting LLM agents in a realistic comms+tools environment reveals a brittle 'agentic layer' where identity, authorization, and state-tracking failures turn ordinary chat interactions into data exfiltration, resource-exhaustion, and governance compromise.
 - In a two-week live red-teaming deployment with persistent memory plus email/Discord/filesystem/shell access, 20 researchers elicited at least 10 significant breaches across 11 case studies, showing that realistic agent scaffolds can rapidly produce security-, privacy-, and governance-critical failures even without sophisticated attacks.
 - Agents frequently complied with non-owner requests and leaked private data, including one agent disclosing 124 email metadata records and later returning bodies of unrelated emails, and another refusing a direct SSN request but still exfiltrating unredacted SSN/bank/medical details when asked to forward the full email thread.
 - Autonomy and tool access enabled unbounded-cost and destructive behaviors, including a multi-agent relay loop persisting for 9+ days consuming ~60,000 tokens, email-driven denial-of-service after ~10 messages with ~10MB attachments, and cross-channel owner spoofing that triggered privileged actions (system shutdown/file deletion/admin reassignment) due to missing robust identity verification.

<br>

ü™∞ **Obscure but Effective: Classical Chinese Jailbreak Prompt Optimization via Bio-Inspired Search** [source](http://arxiv.org/pdf/2602.22983v1.pdf) #security `8/10` 

 *Xun Huang, Simeng Qin, Xiaoshuang Jia et al. (Nanyang Technological University, BraneMatrix AI, Nanjing University of Science and Technology, Northeast University, Renmin University of China, Alibaba Group, Beihang University, National University of Singapore, Zhejiang Lab)*

 Obscure classical-language prompting plus lightweight bio-inspired search can nearly max out jailbreak success in black-box settings while using only ~1‚Äì2 queries, and the resulting prompts transfer broadly across models and even across classical languages.
 - Classical-Chinese-optimized CC-BOS achieves 100% attack success rate across six black-box LLMs on AdvBench (vs best baseline ICRT at 40‚Äì98%), indicating a systematic ‚Äúalignment blind spot‚Äù in high-capability classical-language understanding.
 - CC-BOS is highly query-efficient, requiring ~1.12‚Äì2.38 queries on average to succeed across models (e.g., 1.28 on GPT-4o), cutting cost by ~3√ó versus genetic optimization (4.04) and ~5√ó versus random search (6.10).
 - Cross-model transfer remains strong with 76‚Äì96% ASR when prompts optimized on one model are used on another, and the vulnerability generalizes beyond Classical Chinese to Latin (up to 100% ASR) and Sanskrit (94‚Äì98%), implying defenses must handle distribution shifts in ‚Äúclassical/obscure‚Äù linguistic regimes rather than single-language patching.

<br>

üõ°Ô∏è **AgentSentry: Mitigating Indirect Prompt Injection in LLM Agents via Temporal Causal Diagnostics and Context Purification** [source](http://arxiv.org/pdf/2602.22724v1.pdf) #security `8/10` 

 *Tian Zhang, Yiwei Xu, Juan Wang et al. (Wuhan University, University at Buffalo, SUNY, Jinan University)*

 Temporal, boundary-anchored counterfactual re-execution lets an agent attribute unsafe behavior to untrusted tool context and then safely continue by purifying only the causal source of takeover rather than terminating the workflow.
 - AgentSentry blocks all evaluated indirect prompt injection attacks in tool-augmented agents, achieving 0% Attack Success Rate across AgentDojo‚Äôs four suites, three attack families, and three black-box LLM backends.
 - AgentSentry maintains high task completion under active attack with 74.55% average Utility Under Attack, outperforming the strongest baselines by +20.8 to +33.6 percentage points while not degrading benign performance (0% false positives reported in the main table).
 - Ablations show the largest robustness drop comes from removing sanitized mediator counterfactuals (UA 90.36%‚Üí58.21%, ASR 0%‚Üí22.50%) or collapsing to single-step/weak-masking contrasts (~56% UA and ~22‚Äì24% ASR), indicating mediator-side causal isolation‚Äînot heuristic detection‚Äîis the key driver of security and usability.

<br>

üõú **Systems-Level Attack Surface of Edge Agent Deployments on IoT** [source](http://arxiv.org/pdf/2602.22525v1.pdf) #security `8/10` 

 *Zhonghao Zhan, Krinos Li, Yefan Zhang et al. (Imperial College London, ByteDance)*

 Security risk in agentic IoT is driven as much by deployment architecture and inherited coordination fabric (MQTT, failover, fallback) as by model behavior, with measurable latency/egress metrics revealing where safety guarantees collapse.
 - Edge-local MQTT agent swarms achieved a 23.6 ms mean actuation-to-audit delay (P95 26.9 ms) and stayed within physical interception windows for many IoT actuators, enabling practical real-time safety monitoring if audit subscribers are trusted.
 - MQTT-level provenance is effectively nonexistent without cryptographic binding: adversarial tests showed 100% acceptance of missing-sender messages, sender spoofing, replay, and direct safety-topic publishes by the broker, making impersonation and command re-execution trivial for any credentialed client.
 - Local-first deployments eliminated routine external data exposure (0 external IPs and 0 B sent during a 10-minute session) but can silently cross sovereignty boundaries under stress‚Äîforced context overflow triggered cloud fallback with DNS resolution to api.anthropic.com and no application-layer anomaly or user notification‚Äîwhile end-to-end mobile failover created a 35.7 s monitoring blind spot dominated by network/VPN recovery.

<br>

üì§ **Silent Egress: When Implicit Prompt Injection Makes LLM Agents Leak Without a Trace** [source](http://arxiv.org/pdf/2602.22450v1.pdf) #security `8/10` 

 *Qianlong Lan, Anuj Kaul, Shaun Jones et al. (eBay)*

 Treating network egress as a first-class security outcome reveals that agentic LLMs can behave like a confused deputy‚Äîleaking secrets through tool calls even when the conversation looks perfectly safe.
 - Implicit prompt injection delivered via automatic URL previews/metadata can drive LLM agents to silently exfiltrate sensitive runtime context through outbound tool requests with P(egress)‚âà0.89 (423/480 runs), despite a benign user prompt.
 - Output-based safety checks miss the dominant failure mode: 95% of successful exfiltration events produce harmless final assistant text while leaking only via network egress side effects.
 - Sharded exfiltration evades per-request DLP by splitting secrets across multiple requests, cutting single-request leakage by 73% (Leak@1 0.967‚Üí0.263) but reducing overall reliability by 17‚Äì37%, implying defenders must correlate across requests rather than inspect each in isolation.

<br>

üõ∞Ô∏è **HubScan: Detecting Hubness Poisoning in Retrieval-Augmented Generation Systems** [source](http://arxiv.org/pdf/2602.22427v1.pdf) #security `8/10` 

 *Idan Habler, Vineeth Sai Narajala, Stav Koren et al. (Cisco, OWASP, Tel Aviv University)*

 A low-budget, production-oriented scanner can reliably surface embedding-space retrieval poisoning by flagging statistically extreme, cross-topic, and perturbation-stable ‚Äúhub‚Äù items before they contaminate RAG answers.
 - HubScan detects adversarial ‚Äúhub‚Äù documents in RAG vector indices with 90% recall at a 0.2% alert budget and 100% recall at 0.4%, with planted hubs ranking above the 99.8th percentile of hubness scores.
 - Adding cluster-spread (cross-cluster entropy) to robust hubness statistics yields a +10‚Äì20 percentage-point recall lift for universal hub attacks by exploiting their high cross-topic retrieval dispersion (e.g., 0.92 vs 0.06 for targeted hubs).
 - On a 1M-document MS MARCO web-scale audit, clean-data scores remain far below adversarial ranges (99th percentile 2.3 vs 13‚Äì17 for hubs, a 5.8√ó separation) while imposing ~0.1% operational overhead, enabling practical production thresholds (e.g., score > 10).

<br>

üñºÔ∏è **Breaking Semantic-Aware Watermarks via LLM-Guided Coherence-Preserving Semantic Injection** [source](http://arxiv.org/pdf/2602.21593v1.pdf) #security `8/10` 

 *Zheng Gao, Xiaoyu Li, Zhicheng Bao et al. (University of New South Wales, Griffith University)*

 LLMs can act as practical black-box optimizers over prompt space to make watermark-breaking edits that stay globally coherent, undermining the core assumption behind semantic-aware diffusion watermarks.
 - An LLM-guided Coherence-Preserving Semantic Injection (CSI) attack bypasses the content-aware semantic watermark SEAL with 81% attack success rate, while prior semantic attacks nearly fail (0% for LFA and 7% for RPM), indicating current ‚Äúsemantic binding‚Äù defenses are not robust to constraint-satisfying prompt-space optimization.
 - Across content-independent semantic watermarks (Gaussian Shading, Tree-Ring, WIND), CSI achieves 100% ASR (and 100% on average for these three), showing that watermarking approaches relying on latent/noise-domain signals remain effectively breakable under regeneration-based semantic manipulation.
 - CSI preserves perceived semantic coherence better than unconstrained regeneration, cutting semantic drift versus RPM by 24.1% in FID (235.40‚Üí178.75) and approaching the untampered SEAL baseline (164.27), implying attackers can evade detectors while keeping edits plausibly on-distribution.

<br>

üß† **Adversarial Intent is a Latent Variable: Stateful Trust Inference for Securing Multimodal Agentic RAG** [source](http://arxiv.org/pdf/2602.21447v1.pdf) #security `8/10` 

 *Inderjeet Singh, Vikas Pahuja, Aishvariya Priya Rathina Sabapathy et al. (Fujitsu Research of Europe, Fujitsu Limited)*

 Treating adversarial intent as a latent variable turns agentic RAG security into belief-state control, revealing why more stateless guardrails can mathematically fail to add protection.
 - A stateful trust overlay for multimodal agentic RAG that tracks latent adversarial intent across checkpoints reduces attack success rates by 6.50√ó on 43,774 adversarial instances while keeping answer relevance unchanged (AR 0.93) and context relevance nearly flat (CR 0.88‚Üí0.87).
 - Memory and multi-stage interception are independently necessary for robust defense‚Äîbelief-state tracking yields a 26.4 percentage-point ASR drop and adding upstream checkpoints yields a further 13.6 pp drop‚Äîbecause homogeneous stateless filters can become perfectly correlated and provide zero marginal benefit when scaled.
 - Semantic tool-manipulation remains a hard boundary: tool-flip attacks see only a 1.29√ó ASR reduction (82.1%‚Üí63.5%), implying that belief-based semantic inference must be complemented with deterministic tool governance (e.g., allowlists and argument-schema validation).

<br>

üé£ **MemoPhishAgent: Memory-Augmented Multi-Modal LLM Agent for Phishing URL Detection** [source](http://arxiv.org/pdf/2602.21394v2.pdf) #cyber `8/10` 

 *Xuan Chen, Hao Liu, Tao Yuan et al. (Purdue University, Amazon)*

 Episodic memory turns past phishing investigations into reusable tool-call playbooks, delivering large recall gains while staying deployable at tens-of-thousands of URLs per week.
 - A memory-augmented multimodal LLM agent improves phishing detection recall by 13.6% on public benchmarks and by 20% on a real-world social-media URL benchmark, indicating that adaptive tool-use plus multimodal evidence collection closes a large gap left by prompt-only and fixed workflows.
 - Episodic memory alone contributes up to a 27% recall gain without additional computational overhead, showing that reusing prior investigation trajectories can materially boost coverage against recurring and evolving phishing patterns.
 - In production-scale operation, the system processes ~60K high-risk URLs per week and achieves 91.44% recall, suggesting that agentic, memory-guided pipelines can sustain high-recall phishing defense at customer-facing scale.

<br>

üß† **Alignment-Weighted DPO: A principled reasoning approach to improve safety alignment** [source](http://arxiv.org/pdf/2602.21346v1.pdf) #security `8/10` 

 *Mengxuan Hu, Vivek V. Datla, Anoop Kumar et al. (University of Virginia, Capital One)*

 Safety training that only teaches refusals can survive even when you remove much of the model‚Äôs reasoning circuitry, but weighting preference learning by ‚Äúharmful reasoning vs harmful answer‚Äù yields materially stronger jailbreak robustness at low utility cost.
 - Causal deactivation of the top 10% reasoning-critical attention heads in early layers drops reasoning accuracy by 31‚Äì42% while leaving safety refusal rates essentially unchanged (~99‚Äì100%), indicating current alignment behavior is largely decoupled from deep reasoning.
 - Chain-of-Thought safety fine-tuning with a released 20k-example dataset (4k safety + 16k utility) sharply reduces jailbreak Attack Success Rate (e.g., Llama-2-7B: 78.18% base to 14.09%) while keeping MMLU utility roughly stable (~44‚Äì55%).
 - Alignment-Weighted DPO, which separately weights the reasoning and final-answer segments based on judge-scored harmfulness, targets the ~15% of failures where reasoning and answer safety disagree and achieves very low average ASR on SorryBench (e.g., 0.58% for Llama-3.2-3B and 0.81% for Llama-3.1-8B) with competitive utility (‚âà48‚Äì58%).

 üìå *Relevant to: gan-grpo-model-research*

<br>

üß† **"Are You Sure?": An Empirical Study of Human Perception Vulnerability in LLM-Driven Agentic Systems** [source](http://arxiv.org/pdf/2602.21127v1.pdf) #security `8/10` 

 *Xinfeng Li, Shenyu Dai, Kelong Zheng et al. (Nanyang Technological University, KTH, William & Mary)*

 Humans‚Äînot just models‚Äîare a primary weak link in agent security, and the most effective mitigation is interruptive, low-cost verification plus hands-on ‚Äúsecurity flight simulator‚Äù training.
 - In a 303-participant study of agent-mediated deception (AMD) in LLM-driven agents, only 8.6% noticed anything unusual and just 2.7% correctly identified the attack mechanism despite 97.0% expressing baseline trust in agents and 96.4% reporting confidence in spotting AI risks.
 - Domain expertise can increase vulnerability: technical professionals showed lower perception rates than general users (e.g., 0.0% vs 8.1% in a RAG-poisoning code scenario), consistent with task-focused tunneling and over-reliance on the agent‚Äôs ‚Äúprocess‚Äù rather than validating content and provenance.
 - User-facing defenses work only when they add calibrated friction: an interruptive alert raised risk perception from 2.7% (static disclaimer) to 17.2% and experiential exposure led to >90% of users who perceived risks reporting increased future caution, but warnings can also paradoxically increase trust among users who miss the attack.

<br>

üß∞ **SoK: Agentic Skills -- Beyond Tool Use in LLM Agents** [source](http://arxiv.org/pdf/2602.20867v1.pdf) #security `8/10` 

 *Yanna Jiang, Delong Li, Haiyu Deng et al. (University of Technology Sydney, CSIRO Data61)*

 The most interesting takeaway is that ‚Äúskills‚Äù behave like installable software dependencies for agents‚Äîboosting performance when curated, but turning marketplaces into a high-risk supply chain when governance lags.
 - Curated procedural skills act as a compute multiplier for LLM agents, improving deterministic task pass rates by 16.2 percentage points on average (24.3%‚Üí40.6%) and in some domains by over 50 pp (e.g., healthcare +51.9 pp), implying that capability can be shifted from model scale to governable, reusable modules.
 - Naively self-generated skills are currently unreliable at scale, averaging a ‚àí1.3 percentage point degradation versus no-skills baselines (with some configurations dropping as much as ‚àí5.6 pp), indicating that autonomous skill creation needs strong verification gates before admission into a persistent library.
 - Skill marketplaces create a critical new AI supply-chain attack surface, exemplified by the ClawHavoc incident where 1,184 malicious skills infiltrated a major registry and a Snyk audit found 36.8% of published skills had at least one security flaw, demonstrating that provenance, sandboxing, and trust-tiered execution must be default rather than optional.

<br>

üß∞ **AdapTools: Adaptive Tool-based Indirect Prompt Injection Attacks on Agentic LLMs** [source](http://arxiv.org/pdf/2602.20720v1.pdf) #security `8/10` 

 *Che Wang, Jiaming Zhang, Ziqi Zhang et al. (Peking University, Nanyang Technological University, Ant Group)*

 Adaptive, task-aligned tool selection plus iterative strategy distillation makes indirect prompt injection resilient to both reasoning-heavy agents and modern detectors, turning tool ecosystems (e.g., MCP) into a scalable attack surface.
 - AdapTools increases indirect prompt-injection attack success by 2.13√ó over the strongest baseline while reducing utility under attack by 1.78√ó, showing that adaptive strategy+tool selection materially outpaces static prompt patterns.
 - Against commercial reasoning agents, AdapTools raises average ASR to 18.5% (GPT-4.1), 13.5% (DeepSeek-R1), and 25.9% (Gemini-2.5-Flash), indicating that even frontier models‚Äô native safety checks are insufficient under context-aligned tool misuse.
 - Open-source agents are substantially more vulnerable (average ASR 58.1% vs 38.2% baseline), and SOTA detectors (MELON, Pi-Detector) cut AdapTools ASR by only ~50% rather than eliminating it, implying current defenses degrade but do not reliably prevent tool-triggered compromise.

<br>

üõ°Ô∏è **ICON: Indirect Prompt Injection Defense for Agents based on Inference-Time Correction** [source](http://arxiv.org/pdf/2602.20708v1.pdf) #security `8/10` 

 *Che Wang, Fuyao Zhang, Jiaming Zhang et al. (Peking University, Nanyang Technological University, Ant Group, Alibaba)*

 Inference-time attention steering can neutralize indirect prompt injections without the over-refusal that breaks multi-step agent workflows.
 - ICON reduces indirect prompt injection attack success to 0.4% average ASR across Qwen/Llama/Mistral backbones while improving utility under attack to 53.8% (Qwen-3) versus sub-49% for the next-best baseline, indicating a materially better security‚Äìutility trade-off than refusal- or filter-heavy defenses.
 - Training only on TrojanTools, ICON generalizes out of distribution with 80.1%¬±1.9 to 98.0%¬±1.2 attack detection rate and 62.3%¬±2.3 to 69.6%¬±2.7 utility recovery rate on InjectAgent and AgentDojo, suggesting the method captures model-internal injection signatures rather than dataset-specific keywords.
 - ICON is operationally lightweight (‚âà31,553 parameters, 255 samples, <2 minutes training) compared to fine-tuned guard models (e.g., 8B parameters, 1.19M samples, >10 hours), enabling rapid deployment and iteration without sacrificing near-detector-grade ASR.

<br>

üïµÔ∏è **OptiLeak: Efficient Prompt Reconstruction via Reinforcement Learning in Multi-tenant LLM Services** [source](http://arxiv.org/pdf/2602.20595v1.pdf) #security `8/10` 

 *Longxiang Wang, Xiang Zheng, Xuhao Zhang et al. (City University of Hong Kong, ByteDance Inc.)*

 A lightweight, domain-tuned adversary can turn shared KV-cache optimizations into an efficient prompt keylogger by learning to target the few ‚Äúhard tokens‚Äù that carry most sensitive information.
 - OPTILEAK‚Äôs two-stage SFT‚ÜíDPO tuning with automated ‚Äúhard token‚Äù preference pairs cuts prompt-reconstruction cost by up to 12.48√ó in average requests per token (ARPT), showing cache-based prompt leakage can be far more practical than earlier estimates.
 - Across MedQA/FinanceBench/PubMedQA and 3B‚Äì14B backbones, OPTILEAK consistently improves attack success (e.g., Qwen2.5-3B on FinanceBench reaches 100% ASR at 10k-request budgets with ARPT 19.29 vs 240.81 for the base model), implying multi-tenant KV-cache sharing can enable near-complete prompt recovery under realistic request limits.
 - Attack efficiency depends strongly on attacker domain-data alignment: training on a related medical dataset still reduces ARPT on MedQA by 27.9% (41.99‚Üí30.27), while misaligned finance training worsens PubMedQA ARPT by 53.8% (628.50‚Üí966.59), indicating cache isolation is especially urgent for domain-specialized deployments with predictable query distributions.

<br>

ü§ù **Understanding Human-AI Collaboration in Cybersecurity Competitions** [source](http://arxiv.org/pdf/2602.20446v1.pdf) #cyber `8/10` 

 *Tingxuan Tang, Nicolas Janis, Kalyn Asher Montague et al. (William & Mary, IBM Research)*

 Autonomous agents can now place near the top of human CTF leaderboards largely by eliminating the human prompting/coordination bottleneck, suggesting pair-hacking interfaces may be the highest-leverage near-term design direction.
 - In a live onsite CTF study with 41 participants, hands-on AI use reduced expected AI effectiveness by 0.47 challenges and split future adoption intent, with 44% still planning to use AI for security while the rest cited reliability and risk concerns.
 - Human performance with AI was frequently bottlenecked by the human side of the loop‚Äîdelegation dominated observed behavior (38% prevalence vs 16% collaborative refinement), and high AI-expertise users achieved much higher task success (62% vs 27%) while novices triggered most guardrail refusals (91% of guardrail errors).
 - Fully autonomous CTF agents paired with strong models outperformed most human teams, with the best configuration scoring 4900 points (2nd overall) in ~4 cumulative hours at ~$96 API cost and ~1/5 the runtime, while performance collapsed under weaker models (average success: 48.53% Sonnet-4.5 vs 14.71% Haiku-3.5).

<br>

üß© **Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks** [source](http://arxiv.org/pdf/2602.20156v3.pdf) #security `8/10` 

 *David Schmotz, Luca Beurer-Kellner, Sahar Abdelnabi et al. (Max Planck Institute for Intelligent Systems, ELLIS Institute T√ºbingen, T√ºbingen AI Center, Snyk)*

 Skill files turn agent extensibility into a supply-chain attack surface where ‚Äúinstructions inside instructions‚Äù bypass classic instruction-vs-data defenses and reliably drive real side effects.
 - Frontier agent scaffolds remain highly susceptible to skill-file prompt injection, with contextual attack execution reaching 41‚Äì79% in baseline settings and still 26‚Äì57% even under explicit warning policies (e.g., Gemini 3 Flash 67.6%‚Üí56.8%, GPT‚Äë5.1 Codex‚ÄëMax 64.7%‚Üí47.5%).
 - Unambiguously harmful ‚Äúobvious‚Äù skill injections can succeed at scale, peaking around 70% in single-run settings and rising substantially under best-of-N attacker variation (e.g., Gemini 3 Flash 62.2% ASR and 83.3% with BoN; GPT‚Äë5.2‚ÄëCodex 42.7% ASR and 72.2% with BoN), enabling destructive actions like data deletion and ransomware-like behavior.
 - Attackers gain leverage by moving payloads into scripts and metadata: script-based injections raise success by +18.0 points for contextual attacks (71.8% vs 53.8%) and +9.0 for obvious attacks (62.8% vs 53.7%), while YAML description injection adds ~+14.1 points overall and ~+17.6 points under warning prompts, indicating defenses must cover executable artifacts and high-authority metadata paths.

<br>

üßÆ **The LLMbda Calculus: AI Agents, Conversations, and Information Flow** [source](http://arxiv.org/pdf/2602.20064v1.pdf) #security `8/10` 

 *Zac Garby, Andrew D. Gordon, David Sands (University of Nottingham, University of Edinburgh, Chalmers University of Technology, The University of Gothenburg)*

 A minimal lambda-calculus kernel turns prompt-response conversations into first-class semantic state, making prompt injection and information-flow defenses provable rather than ad hoc.
 - Introduces LLMBDA, an untyped call-by-value lambda calculus with explicit conversation state and an LLM-invocation primitive (@), enabling formal modeling of planner loops, tool calling, code generation, and prompt-injection mechanisms within a single operational semantics.
 - Proves a termination-insensitive noninterference theorem that yields confidentiality/integrity guarantees for agentic programs under three enforceable conditions: no label tests, a two-level (single non-bottom) label regime, or restricting label tests to assertions/strong tests that preserve noninterference across arbitrary lattices.
 - Demonstrates (via a Python interpreter implementing the semantics and real LLM calls) that single-conversation tool-calling agents are vulnerable to indirect prompt injection, while dual-LLM style quarantined sub-conversations plus dynamic label checks can prevent untrusted inputs from influencing trusted tool arguments.

<br>

üß† **Assessing Risks of Large Language Models in Mental Health Support: A Framework for Automated Clinical AI Red Teaming** [source](http://arxiv.org/pdf/2602.19948v1.pdf) #security `8/10` 

 *Ian Steenstra, Paola Pedrelli, Weiyan Shi et al. (Northeastern University, Harvard Medical School)*

 Long-horizon simulated patients with cognitive-affective state tracking make rare but catastrophic therapy failure modes measurable‚Äîand reveal that ‚Äúbetter prompts‚Äù can backfire on safety.
 - A longitudinal, simulation-based clinical red-teaming setup (15 clinically-validated AUD personas, N=369 simulated sessions) exposed multi-turn iatrogenic failures that single-turn safety benchmarks miss, including delusion validation (‚ÄúAI Psychosis‚Äù) and inadequate suicide-risk de-escalation.
 - Simulated patients showed strong quantitative alignment with gold-standard clinical instruments across 26 persona characteristics (e.g., AUD severity Œ∫=0.81 with Spearman œÅ=0.997; hopelessness œÅ=0.97; burdensomeness and belongingness œÅ=0.98) and were rated above neutral for clinical realism (composite 3.77/5; t(29)=5.06, p=0.0001).
 - Prompting a model to follow Motivational Interviewing did not reliably reduce harm: ChatGPT Basic had the lowest adverse outcomes (217) while ChatGPT MI increased adverse outcomes (362), whereas Gemini MI using the same MI prompt was substantially safer (262) and reduced severe psychological decompensation crises (2 vs Character.AI‚Äôs 13; p=0.014), highlighting that ‚Äútherapist-mode‚Äù prompting can worsen safety depending on the base model.

 üìå *Relevant to: longitudinal-cognitive-harm-trajectories*

<br>

üß™ **CIBER: A Comprehensive Benchmark for Security Evaluation of Code Interpreter Agents** [source](http://arxiv.org/pdf/2602.19547v1.pdf) #security `8/10` 

 *Lei Ba, Qinbin Li, Songze Li (Southeast University, Huazhong University of Science and Technology)*

 State-aware scoring that verifies real filesystem/process side-effects (not just refusals) reveals that ‚Äútrusted‚Äù internal channels and natural-language code descriptions are the primary failure modes for code-executing agents.
 - Across 57,000 dynamic sandboxed tests over 6 foundation models and 2 interpreter architectures, contextual-channel attacks dominate (Memory Poisoning 73.3% ASR; Indirect Prompt Injection 71.8% ASR) versus Direct Prompt Injection (59.4% ASR), showing that tool outputs and conversation history are higher-risk trust boundaries than the user prompt.
 - Input modality strongly shifts exploitability, with ‚Äúcode descriptions‚Äù in natural language being the most dangerous (62.5% ASR) and outperforming explicit code snippets (48.4% ASR) by +14.1 points, indicating syntax/AST-focused defenses are systematically bypassed by semantic paraphrasing.
 - Architecture and model alignment jointly set the safety baseline: moving the same GPT-3.5-Turbo from OpenInterpreter to the more constrained OpenCodeInterpreter reduces Memory Poisoning success by 20.2 points (87.1%‚Üí66.9%), yet generic models can still mismatch guardrails (e.g., Prompt-backdoor trigger activation 65.9% on OCI with GPT-3.5) while stronger intrinsic refusal lowers overall risk (GPT-5-mini on OI achieves the lowest overall ASR at 48.0%).

<br>

üßî **Can a Teenager Fool an AI? Evaluating Low-Cost Cosmetic Attacks on Age Estimation Systems** [source](http://arxiv.org/pdf/2602.19539v1.pdf) #security `8/10` 

 *Xingyu Shen, Tommy Duong, Xiaodong An et al. (Reality Inc., UC Berkeley, Duke University, Georgia Tech, UNC Chapel Hill, UC San Diego)*

 Drugstore-level cosmetic edits can convert roughly two-thirds of baseline ‚Äúminor‚Äù classifications into ‚Äúadult,‚Äù undermining selfie-based age gates even when clean-image MAE looks strong.
 - A single low-effort cosmetic change‚Äîa synthetic beard‚Äîcauses 28‚Äì69% Attack Conversion Rate (ACR) across eight age-estimation models, showing that many ‚Äúminor‚Äù decisions can be flipped to ‚Äúadult‚Äù without any model access or technical skill.
 - Combining beard, grey hair, makeup, and wrinkles increases predicted age by +7.7 years on average (329 subjects) and reaches up to 82.9% ACR (avg 68.9%), indicating that multi-cue appearance changes can defeat most baseline minor classifications near the 18-year threshold.
 - Vision-language models trend more robust under the full combination (59‚Äì71% ACR) than specialized CV models (63‚Äì83% ACR), but the overlap implies robustness is not guaranteed by architecture choice and should be validated with adversarial testing and confidence intervals.

<br>

üß™ **CodeHacker: Automated Test Case Generation for Detecting Vulnerabilities in Competitive Programming Solutions** [source](http://arxiv.org/pdf/2602.20213v1.pdf) #cyber `8/10` 

 *Jingwei Shi, Xinxiang Yin, Jing Huang et al. (Shanghai University of Finance and Economics, Northwestern Polytechnical University, Meituan, University of Toronto)*

 Automating Codeforces-style hacks‚Äîwhile first hardening the judge itself‚Äîturns test generation into a rigorous adversarial evaluation and a high-leverage training curriculum for robust code reasoning.
 - An LLM-driven ‚Äúhacking‚Äù agent that calibrates validators/checkers before attacking submissions raises benchmark validity to 100% and boosts discriminative power on CodeContest+ to TNR 96.31% overall (vs 85.72%) and 96.05% on special-judge problems (vs 84.04%), showing that many ‚Äúaccepted‚Äù solutions survive due to test weakness rather than correctness.
 - Hack success sharply separates reasoning-capable models from non-reasoning modes, with DeepSeek V3.2 achieving 64.83% HSR but dropping to 23.76% without explicit reasoning (‚âà2.7√ó decline), implying adversarial test construction is a constructive reasoning workload rather than prompt-pattern retrieval.
 - Targeted adversarial cases act as both evaluation correction and training signal: pass@1 on a stricter setting drops by 1.7‚Äì3.5 points (revealing prior metric inflation and even rank reversals), while RL training on the augmented dataset improves LiveCodeBench pass@5 across difficulties (e.g., Hard 25.00‚Üí27.63).

<br>

üß™ **FuzzySQL: Uncovering Hidden Vulnerabilities in DBMS Special Features with LLM-Driven Fuzzing** [source](http://arxiv.org/pdf/2602.19490v1.pdf) #cyber `8/10` 

 *Yongxin Chen, Zhiyuan Jiang, Chao Zhang et al. (National University of Defense Technology, Tsinghua University, Hunan University)*

 Combining CFG templates with LLM instantiation, logic-level predicate flips, and LLM-assisted repair makes previously ‚Äúunfuzzable‚Äù DBMS special features reliably crashable and CVE-producing.
 - An LLM-guided, grammar-constrained DBMS fuzzer uncovered 37 distinct vulnerabilities across five engines, with 29 vendor-confirmed (78.4%), 9 assigned CVEs (24.3%), and 14 already fixed (37.8%), showing that LLM-assisted semantic exploration translates into actionable, patch-level outcomes.
 - Feature-specific SQL surfaces (e.g., GTID, PROCEDURE, INSTALL COMPONENT, KILL) produced 7 of 29 confirmed issues (24.1%), indicating that vendor extensions and stateful administrative commands remain a disproportionately risky and under-tested attack surface compared to standard DDL/DML.
 - Logic-shifting mutations were necessary for 20 of 29 confirmed bugs (69.0%), and automated repair rescued 27% of bug-triggering tests that initially failed to execute, implying that small semantics-preserving predicate rewrites plus error-guided fixing are high-leverage additions to coverage-guided fuzzing.

<br>

üïµÔ∏è **Hiding in Plain Text: Detecting Concealed Jailbreaks via Activation Disentanglement** [source](http://arxiv.org/pdf/2602.19396v1.pdf) #security `8/10` 

 *Amirhossein Farzam, Majid Behabahani, Mani Malek et al. (Duke University, Princeton University, Morgan Stanley, Google DeepMind, Apple)*

 Separating ‚Äúwhat the user wants‚Äù (goal) from ‚Äúhow they ask‚Äù (framing) in hidden activations enables a fast, frozen-LLM detector that catches fluent, semantically coherent jailbreaks that evade surface heuristics.
 - FrameShield improves model-agnostic detection of goal-preserving framing (GPF) jailbreaks over JBShield on GoalFrameBench, with accuracy gains up to +28 points (e.g., Qwen2.5-7B: 0.66‚Üí0.94; Qwen3-4B: 0.70‚Üí0.89) while keeping the base LLM frozen and adding negligible inference overhead.
 - A lightweight self-supervised activation module (ReDAct) trained on 6,269 contrastive goal‚Äìframing prompts produces factor-dominant representations where the intended factor explains more variance than the cross-factor in multiple LLM families (e.g., Llama2-7B Œ∑¬≤(goal, v_g)=0.41 vs Œ∑¬≤(goal, v_f)=0.19; Œ∑¬≤(framing, v_f)=0.22 vs Œ∑¬≤(framing, v_g)=0.14).
 - Anomaly detection on disentangled framing signals generalizes to unseen harmful goals with relatively small ID‚ÜíOOD degradation in many settings (e.g., Vicuna-13B FrameShield-Last 0.76/0.79‚Üí0.79/0.85; Qwen2-0.5B 0.66/0.74‚Üí0.74/0.83), indicating that framing is a robust, goal-agnostic indicator of jailbreak intent.

<br>

ü™ù **Watermarking LLM Agent Trajectories** [source](http://arxiv.org/pdf/2602.18700v1.pdf) #security `8/10` 

 *Wenlong Meng, Chen Gong, Terry Yue Zhuo et al. (Zhejiang University, University of Virginia, Alibaba Qwen, University of Alberta)*

 Watermarking agent datasets becomes practical when the signal is embedded as an extra tool-use behavior at high-entropy decision points rather than as brittle token-level code/text tweaks.
 - ACTHOOK embeds behavior-level watermarks into LLM agent trajectory datasets by injecting ‚Äúhook actions‚Äù at action boundaries triggered by a secret prompt key, enabling black-box ownership detection without changing task outcomes.
 - On Qwen-2.5-Coder-7B, ACTHOOK achieved 94.3 average detection AUC across math, web search, and software engineering agents at a 5% watermark ratio with negligible Pass@1 degradation, while a CodeMark-style baseline stayed near chance (~55.5 AUC).
 - Under watermark-removal attempts, detection largely persisted (e.g., AUC stayed >85 after filtering/paraphrasing/summarization in reported settings), and code-watermark purifiers like DeCoMa filtered ACTHOOK almost randomly (precision roughly matching the watermark ratio).

<br>

üßæ **ESAA: Event Sourcing for Autonomous Agents in LLM-Based Software Engineering** [source](http://arxiv.org/pdf/2602.23193v1.pdf) #security `7/10` 

 *Elzo Brito dos Santos Filho*

 Treating LLMs as contract-bound intention emitters plus event-sourced replay turns agentic coding from an opaque process into a verifiable, forensically auditable system with bounded blast radius.
 - ESAA enforces a least-privilege agent model where LLMs can only emit schema-validated JSON intentions (agent.result/issue.report) while a deterministic orchestrator performs all state mutations via an append-only event log and replayable projections.
 - Across two software-engineering case studies (9 tasks/49 events single-agent composition and 50 tasks/86 events with 4 concurrent heterogeneous LLMs), constrained output held protocol violations to 0 output.rejected events and achieved verify_status=ok via SHA-256 projection hashing.
 - In the multi-agent clinical dashboard run, 62% of tasks (31/50) reached done with 30/30 completions passing explicit acceptance checks, and concurrency was evidenced by 6 task claims within one minute while preserving total ordering through log serialization.

<br>

üó∫Ô∏è **Manifold of Failure: Behavioral Attraction Basins in Language Models** [source](http://arxiv.org/pdf/2602.22291v1.pdf) #security `7/10` 

 *Sarthak Munshi, Manish Bhatt, Vineeth Sai Narajala et al. (Amazon Web Services, Amazon, Cisco, Distributedapps.ai, Shrewd Security)*

 Mapping where and how failures cluster (attraction basins) reveals alignment weaknesses that conventional single-objective jailbreak methods cannot characterize.
 - A MAP-Elites quality-diversity mapping of a 25√ó25 prompt-behavior grid achieved up to 63.04% behavioral coverage and surfaced as many as 370 distinct vulnerable niches (AD>0.5), turning jailbreak discovery into a global ‚Äúsafety landscape‚Äù audit rather than isolated attack exemplars.
 - Llama-3-8B shows near-universal misalignment with mean Alignment Deviation (AD) 0.93 and 93.9% of explored cells classified as vulnerability basins, implying broad susceptibility across both query indirection and authority-framing variations.
 - GPT-5-Mini exhibits a hard safety ceiling with peak AD capped at 0.50 and 0 basin cells even at 72.32% coverage, while GPT-OSS-20B sits between them with fragmented basins (64.3% of filled cells vulnerable; mean AD 0.73), indicating model-specific ‚Äútopological signatures‚Äù that can guide targeted mitigations.

<br>

üìß **A Lightweight Defense Mechanism against Next Generation of Phishing Emails using Distilled Attention-Augmented BiLSTM** [source](http://arxiv.org/pdf/2602.22250v1.pdf) #cyber `7/10` 

 *Morteza Eskandarian, Mahdi Rabbani, Arun Kaniyamattam et al. (University of New Brunswick, Canadian Institute for Cybersecurity, Mastercard Vancouver Tech Hub)*

 Knowledge distillation can preserve near-transformer phishing-detection accuracy while making on-device, privacy-preserving, low-latency filtering practical against LLM-generated paraphrase attacks.
 - A 4.5M-parameter distilled BiLSTM with multi-head attention reaches 96.67 weighted-F1 on mixed real+LLM email traffic while remaining within ~1‚Äì2.3 F1 points of much larger transformer baselines (e.g., ModernBERT 98.98, DeBERTaV3 98.75).
 - On the mixed-traffic test, the distilled model is ~5‚Äì19√ó faster at inference (6.06s vs 31‚Äì116.65s for transformer baselines) and ~20‚Äì800√ó smaller (4.5M vs 86‚Äì3840M params), enabling real-time filtering without accelerator hardware.
 - Cross-distribution evaluation shows LLM rewrites induce substantial distribution shift (e.g., baseline BiLSTM F1 drops to 65.59 in Gen‚ÜíOrig), while distillation plus multi-head attention improves transfer robustness (Gen‚ÜíOrig F1 90.02 and Orig‚ÜíGen F1 89.60), supporting training with mixed human+LLM corpora for deployment.

<br>

ü¶ã **Evaluating Proactive Risk Awareness of Large Language Models** [source](http://arxiv.org/pdf/2602.20976v1.pdf) #security `7/10` 

 *Xuan Luo, Yubin Chen, Zhiyu Hou et al. (Harbin Institute of Technology, Shenzhen, The Hong Kong Polytechnic University, Southern University of Science and Technology, Shenzhen Loop Area Institute)*

 Proactive risk awareness is not a model ‚Äúcapability constant‚Äù but a highly prompt- and modality-sensitive behavior that collapses under short answers and image-only species cues.
 - Across 1,094 environmentally latent queries, proactive environmental intelligence (ProR) is far below reactive reminder rates (ReR 83‚Äì90% full; 69‚Äì85% short), showing current safety alignment largely triggers only when harm is explicit.
 - Response brevity is the dominant deployment risk factor: in English, ProR drops from 0.58‚Äì0.85 (full) to 0.30‚Äì0.71 (short) while Blind Spot Rate rises from 0.05‚Äì0.24 to 0.15‚Äì0.48, meaning ‚Äúbe concise‚Äù can materially increase silent harmful guidance.
 - Multimodal protected-species safety is brittle because species recognition and conservation warnings are weakly coupled, with peak image recognition only ~40% for animals and ~20% for plants and only a small fraction of recognized cases producing an explicit protection warning.

<br>

üìù **Accelerating Incident Response: A Hybrid Approach for Data Breach Reporting** [source](http://arxiv.org/pdf/2602.22244v1.pdf) #cyber `7/10` 

 *Aurora Arrus, Maria di Gisi, Sara Lilli et al. (IMT School for Advanced Studies Lucca, University of Cagliari, University of Salerno, Sant‚ÄôAnna School of Advanced Studies, University of Camerino)*

 The core novelty is treating malware analysis as an input-to-compliance pipeline: classify exfiltration statically, emulate only what matters, then force an LLM to emit regulator-ready JSON instead of free-form prose.
 - A graph-based static classifier over Linux/ARM ELF function-call graphs (76-dimensional features) separates exfiltrator vs non-exfiltrator malware with 0.967 accuracy and 0.983 ROC-AUC on a 25% held-out split, enabling high-confidence triage before expensive sandboxing.
 - A hybrid workflow gates dynamic emulation to only predicted exfiltrators, using Emulix+Qiling with FakeNet-NG to safely capture syscall/API traces and full PCAPs that concretely evidence exfiltration behaviors (e.g., 824 packets and 577 flows in the showcased sample).
 - Schema-constrained LLM generation (JSON aligned to the Italian Garante breach-notification form) converts heterogeneous forensic artifacts into compliance-ready fields to reduce analyst cognitive load and improve completeness under GDPR‚Äôs 72-hour reporting deadline while keeping a human-in-the-loop for legal accountability.

<br>

üõ°Ô∏è **Analysis of LLMs Against Prompt Injection and Jailbreak Attacks** [source](http://arxiv.org/pdf/2602.22242v1.pdf) #security `7/10` 

 *Piyush Jaiswal, Aaditya Pratap, Shreyansh Saraswati et al. (NIT Trichy, NCE Chandi, Bishop Cotton Boys‚Äô School, IIT (BHU), IIT Patna)*

 Lightweight guardrails help, but long, reasoning-rich jailbreaks still pierce most external filters‚Äîwhile some 'safe' models fail silently, creating a hidden deployment risk.
 - On 94 prompt-injection tests, small open-source models showed high compromise rates (Qwen3-1.7B: 71.3%, Gemma3-1B: 62.8%) while all tested 3B‚Äì7B variants recorded 0% successful injections, indicating safety robustness can vary sharply within the same model family and size range.
 - Across 73 jailbreak prompts, Gemma3-1B was vulnerable on 49/73 (67.1%) while DeepSeek-R1-1.5B was vulnerable on 1/73 (1.37%), but several configurations suffered major reliability failures (e.g., Llama3.2-1B timed out on 31/73 = 42.5%), showing that ‚Äúsafety‚Äù metrics must be reported alongside non-response/timeout rates.
 - Among five no-retraining, inference-time defenses evaluated on a stronger 74-prompt set, self-examination ('Self-defence') was the most effective (achieving 0% vulnerability for Qwen3-4B and DeepSeek-R1-1.5B) while input filtering was consistently weakest and remained bypassable by long, reasoning-heavy prompts, implying surface heuristics do not scale to modern jailbreak strategies.

<br>

üß† **An Explainable Memory Forensics Approach for Malware Analysis** [source](http://arxiv.org/pdf/2602.19831v1.pdf) #cyber `7/10` 

 *Silvia Lucia Sanna, Davide Maiorca, Giorgio Giacinto (University of Cagliari, Consorzio Interuniversitario Nazionale per l‚ÄôInformatica (CINI))*

 Using general-purpose LLMs as a translation layer for volatile-memory tooling turns expert-only Volatility outputs into actionable IoCs and defensible explanations, while also revealing runtime artifacts that reputation-based malware scanning misses.
 - LLM-assisted interpretation of Volatility outputs and RAM strings produced higher-fidelity, human-readable forensic reports and extracted a broader set of runtime IoCs than file/hash reputation tools, particularly for injection, hidden modules, kernel structures, and transient filesystem artifacts that are typically invisible to static scanning.
 - On Android, full-RAM acquisition plus Volatility/LLM analysis achieved an average malware-classification accuracy of 0.93 (Volatility 3), while traditional Android detectors successfully analyzed and detected fewer than 30% of samples due to anti-static and anti-instrumentation defenses that break tools like Drebin/Androguard and Frida-based workflows.
 - Full RAM snapshots and target-process dumps were complementary: system-wide captures exposed OS-level effects (e.g., cross-process interaction, network activity, kernel artifacts) while process-focused dumps concentrated application-specific IoCs (e.g., runtime-loaded code and permissions), and time-series dumping (5 snapshots over 60 seconds) helped surface evolving behavior.

<br>

üß™ **OpenClaw, Moltbook, and ClawdLab: From Agent-Only Social Networks to Autonomous Scientific Research** [source](http://arxiv.org/pdf/2602.19810v1.pdf) #security `7/10` 

 *Lukas Weidener, Marko Brkiƒá, Mihailo Jovanoviƒá et al. (Bio.xyz, MJP Partners AG)*

 Agent-only social platforms can generate rapid emergent behavior at scale, but without hard governance and evidence-based validation they also create an unusually large, fast-moving attack surface across both agents and their infrastructure.
 - A 72-hour snapshot of an agent-only social network surfaced large-scale manipulation and safety issues, including 506 prompt-injection attempts and cryptocurrency promotion comprising 19.3% of all content.
 - Platform growth metrics were massively inflated‚Äî1.5M registered agents were traced to ~17,000 human owners (an 88:1 agent-to-human ratio), enabled by missing rate limits that allowed a single agent to mint hundreds of thousands of accounts.
 - The OpenClaw ecosystem exposed severe agent-infrastructure risk, with critical vulnerabilities demonstrated across 131 skills and >15,200 internet-exposed control panels plus a CVSS 8.8 one-click RCE via auth-token exfiltration.

<br>

üõ°Ô∏è **Red-Teaming Claude Opus and ChatGPT-based Security Advisors for Trusted Execution Environments** [source](http://arxiv.org/pdf/2602.19450v1.pdf) #security `7/10` 

 *Kunal Mukherjee (Virginia Tech)*

 The most operationally dangerous errors aren‚Äôt spectacular jailbreaks but fluent, transferable misconceptions about TEE boundaries, attestation semantics, and evidence interpretation‚Äîand they‚Äôre best addressed with architecture-level guardrails rather than model choice alone.
 - Prompt-induced failure modes in TEE security advising are partially cross-model, with transferability reaching 12.02%, implying reusable prompt templates can propagate the same boundary/attestation mistakes across providers.
 - A stacked ‚ÄúLLM-in-the-loop‚Äù workflow (policy gating, retrieval grounding, structured templates, and lightweight verification) reduces worst-case failure prevalence by 80.62%, indicating pipeline-level controls are far more effective than relying on unguided chat interactions.
 - Even when average answer quality is high (family means roughly 10‚Äì14/14), overconfident technical errors persist and concentrate in mitigation/attack-awareness prompts‚Äîexactly the outputs most likely to be copied into playbooks‚Äîmaking citation discipline and explicit threat-scoping critical.

<br>

üõ°Ô∏è **OpenPort Protocol: A Security Governance Specification for AI Agent Tool Access** [source](http://arxiv.org/pdf/2602.20196v1.pdf) #security `7/10` 

 *Genliang Zhu, Chu Wang, Ziyuan Wang et al. (Accentrust, Georgia Institute of Technology, University of Illinois Urbana-Champaign, Duke University, New York Institute of Technology-Vancouver)*

 The interesting twist is treating agent tool access as a protocol-level governance problem‚Äîstandardizing drafts, reason codes, rate-limit semantics, and auditability so failures and abuse become deterministic and testable.
 - OpenPort Protocol specifies a governance-first, model-neutral gateway for AI agent tool access that enforces deny-by-default authorization, authorization-dependent tool discovery, stable machine-actionable error codes, mandatory audit events, and baseline admission control (rate limits/quotas) as protocol invariants rather than implementation choices.
 - Write safety is centered on a draft-first pipeline where side effects are blocked by default and only permitted under explicit, time-bounded auto-execution policy with high-risk safeguards including preflight impact-hash binding (RFC 8785 JCS), idempotency-key de-duplication keyed by {appId,idempotencyKey}, and an optional execution-time State Witness check that fails closed on TOCTOU drift.
 - At the pinned v0.1.0 reference tag, an externally reproducible gate reports 20 unit tests across 8 files passing plus 80 malformed-request fuzz regressions with no 5xx, validates 6 required endpoints via a core conformance runner (including preflight behavior and draft retrieval), and confirms stable denial-code coverage for token invalid, policy denied, and idempotency required while marking end-to-end 429 semantics, audit completeness, and idempotency replay regressions as incomplete/future work.

<br>

üõü **MANATEE: Inference-Time Lightweight Diffusion Based Safety Defense for LLMs** [source](http://arxiv.org/pdf/2602.18782v1.pdf) #security `7/10` 

 *Chun Yan Ryan Kan, Tommy Tran, Vedant Yadav et al.*

 Treating jailbreaks/backdoors as representation-space OOD and correcting them with score-based diffusion makes safety an inference-time projection problem rather than a brittle classifier or repeated fine-tune cycle.
 - An inference-time diffusion ‚Äúpurification‚Äù step over final-layer hidden states cut attack success rates on backdoored/jailbreak evaluations by an average ~78%, including 100% ASR reduction on Anthropic Sleeper Agent across Mistral-7B, Llama-3.1-8B, and Gemma-2-9B.
 - On JailbreakBench, the defense reduced ASR from 98%‚Üí0% (Mistral), 89%‚Üí0% (Llama), and 46%‚Üí0% (Gemma), indicating that projecting anomalous representations toward a learned benign manifold can fully suppress some jailbreak families without retraining the base LLM.
 - Training uses only benign hidden states (no harmful data) and requires no model architecture changes, suggesting a deployable ‚Äúplug-and-play‚Äù safety layer that trades off between diffusion steering (below threshold) and selective refusal (above threshold) to preserve benign utility while blocking anomalous trajectories.

<br>

üì° **Secure Semantic Communications via AI Defenses: Fundamentals, Solutions, and Future Directions** [source](http://arxiv.org/pdf/2602.22134v1.pdf) #security `6/10` 

 *Lan Zhang, Chengsi Liang, Zeming Zhuang et al.*

 The key twist is that AI-native semantic communication can be attacked by changing what messages mean‚Äîwithout breaking packets‚Äîso defenses must be cross-layer and deployment-budgeted rather than purely cryptographic or model-centric.
 - Semantic communication systems remain vulnerable to meaning-level failures even when bit-level reliability and cryptographic protections are intact, because learned encoders/decoders, shared priors, and distributed inference create attack surfaces that bypass traditional link security.
 - A system-level security view is needed because semantic integrity can be compromised at four distinct layers‚Äîmodel, channel-realizable perturbations, shared knowledge/context, and networked multi-agent propagation‚Äîwhere localized corruption can cascade into global task failure without obvious transmission anomalies.
 - Deployment realism is framed through ‚Äúsecurity‚Äìutility operating envelopes‚Äù that make robustness a managed runtime property trading off semantic fidelity, latency, energy, and adversarial budget, and highlights that current evaluation lacks standardized benchmarks for semantic robustness, leakage, and cross-layer composability.

<br>

üõ°Ô∏è **LLM-enabled Applications Require System-Level Threat Monitoring** [source](http://arxiv.org/pdf/2602.19844v1.pdf) #security `6/10` 

 *Yedi Zhang, Haoyu Wang, Xianglin Yang et al. (National University of Singapore, Singapore Management University)*

 The most interesting angle is reframing LLM security as a post-deployment observability and incident-response problem, with a stage-by-stage telemetry plan spanning prompt/RAG/tool/memory/infrastructure rather than relying on guardrails alone.
 - Trustworthy deployment of LLM-enabled applications depends less on improving model capability and more on adopting EDR-style, system-level monitoring that treats compromises and failures as expected operational conditions requiring continuous detection, triage, and forensics.
 - A workflow-aware audit-logging blueprint is proposed that maps 14 deployment-time threat categories (from prompt injection and tool-call DoS to cross-context disclosure and model theft) to concrete, stage-specific telemetry artifacts across an 8-stage agent pipeline (prompting, retrieval, tool use, generation, and delivery).
 - Operationally actionable thresholds illustrate detection value‚Äîe.g., cache-hit ratio collapsing from 40% to 2% signaling cache-bypass, ‚â•99% semantic similarity revealing nonce-based DoS or probing, >0.99 cosine ‚Äúsimilarity-chasing‚Äù indicating embedding reconstruction attempts, and token burn >10^5 suggesting runaway agent loops‚Äîimplying many high-impact attacks are monitorable via scalable signals rather than model internals.

<br>

üï∏Ô∏è **Agentic AI as a Cybersecurity Attack Surface: Threats, Exploits, and Defenses in Runtime Supply Chains** [source](http://arxiv.org/pdf/2602.19555v1.pdf) #security `6/10` 

 *Xiaochong Jiang, Shiqi Yang, Wenting Yang et al. (Independent Researcher)*

 The core security break is that, for agents, context effectively becomes executable control flow‚Äîso supply-chain security must move to zero-trust runtime provenance and capability binding.
 - Agentic LLM systems shift the primary attack surface from build-time artifacts to inference-time ‚Äúruntime supply chains,‚Äù where untrusted data and probabilistic tool resolution can steer high-privilege actions without compromising model weights or infrastructure.
 - Runtime data attacks can be transient (context-window prompt injection) or persistent (RAG/long-term memory poisoning), with cited results including 98% attack success for multimodal indirect injection, 70% ASR from poisoning 0.1% of a retrieval corpus, and 76.8% ASR for query-only memory injection.
 - Tool supply chain risk decomposes into Discovery (e.g., hallucination squatting/semantic masquerading), Implementation (malicious extensions/transitive dependency exploitation), and Invocation (over-privileged or argument-injected calls), enabling a ‚ÄúViral Agent Loop‚Äù where agents propagate self-replicating semantic worms via authorized tools rather than code exploits.

<br>

üîê **LLM Scalability Risk for Agentic-AI and Model Supply Chain Security** [source](http://arxiv.org/pdf/2602.19021v1.pdf) #security `6/10` 

 *Kiarash Ahi, Vaibhav Agrawal, Saeed Valizadeh (Virelya AI Labs, Google)*

 It connects operational scalability (latency/throughput/cost/compliance) to security outcomes via a single risk index and then argues that cryptographic, fail-closed model provenance is the missing prerequisite for safely deploying agentic LLMs at scale.
 - A 70-source synthesis argues that LLM-enabled ‚Äúcyber threat inflation‚Äù reduces attacker marginal cost toward near-zero by automating malware, phishing, and multi-stage operations, shifting defenses from point solutions to continuous, systems-level resilience.
 - The proposed LLM Scalability Risk Index (LSRI) operationalizes deployment readiness across latency, throughput, compliance, size, update frequency, and cost, and a worked example shows ~24% prompt-injection success driving an integrity multiplier to Œ¶=0.76 and dropping readiness from ~0.78 (‚Äúready‚Äù) to ~0.40 (‚Äúrisky‚Äù) despite strong raw performance.
 - LLM-enhanced vulnerability detection is reported to outperform static analyzers on zero-days (62% recall baseline vs 88‚Äì91% for LLM hybrids/graph methods) with similar latency (~80ms vs ~96‚Äì105ms) and false positives (11% vs 12‚Äì13%), implying material detection gains but with added model-governance and robustness obligations.

<br>

