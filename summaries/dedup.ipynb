{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ff58edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25771cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_doc = \"/home/gio/projects/applied-gai-secnews/summaries/2025-12-01.md\"\n",
    "new_doc = \"/home/gio/projects/applied-gai-secnews/summaries/2025-12-05.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67f19b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28 titles in old document.\n"
     ]
    }
   ],
   "source": [
    "# Titles in old document\n",
    "with open(old_doc, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "old_titles = set()\n",
    "for line in lines:\n",
    "    # Titles have \"**\" twice in them\n",
    "    if line.count(\"**\") >= 2:\n",
    "        title = line.split(\"**\")[1].strip()\n",
    "        old_titles.add(title)\n",
    "print(f\"Found {len(old_titles)} titles in old document.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9bc970d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 77 titles in new document.\n"
     ]
    }
   ],
   "source": [
    "# Titles in new document\n",
    "with open(new_doc, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "new_titles = set()\n",
    "for line in lines:\n",
    "    # Titles have \"**\" twice in them\n",
    "    if line.count(\"**\") >= 2:\n",
    "        title = line.split(\"**\")[1].strip()\n",
    "        new_titles.add(title)\n",
    "print(f\"Found {len(new_titles)} titles in new document.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31c83b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 duplicate titles between old and new document.\n"
     ]
    }
   ],
   "source": [
    "# Find duplicates in new document\n",
    "duplicates = old_titles.intersection(new_titles)\n",
    "print(f\"Found {len(duplicates)} duplicate titles between old and new document.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23c14260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A Safety and Security Framework for Real-World Agentic Systems',\n",
       " 'Accuracy and Efficiency Trade-Offs in LLM-Based Malware Detection and Explanation: A Comparative Study of Parameter Tuning vs. Full Fine-Tuning',\n",
       " 'Adversarial Attack-Defense Co-Evolution for LLM Safety Alignment via Tree-Group Dual-Aware Search and Optimization',\n",
       " 'Adversarial Confusion Attack: Disrupting Multimodal Large Language Models',\n",
       " 'AgentShield: Make MAS more secure and efficient',\n",
       " 'Are LLMs Good Safety Agents or a Propaganda Engine?',\n",
       " 'Are Neuro-Inspired Multi-Modal Vision-Language Models Resilient to Membership Inference Privacy Leakage?',\n",
       " 'AttackPilot: Autonomous Inference Attacks Against ML Services With LLM-Based Agents',\n",
       " 'Attention Trajectories as a Diagnostic Axis for Deep Reinforcement Learning',\n",
       " 'Automating Deception: Scalable Multi-Turn LLM Jailbreaks',\n",
       " 'BrowseSafe: Understanding and Preventing Prompt Injection Within AI Browser Agents',\n",
       " 'CacheTrap: Injecting Trojans in LLMs without Leaving any Traces in Inputs or Weights',\n",
       " 'Cross-LLM Generalization of Behavioral Backdoor Detection in AI Agent Supply Chains',\n",
       " 'DUALGUAGE: Automated Joint Security-Functionality Benchmarking for Secure Code Generation',\n",
       " 'Defending Large Language Models Against Jailbreak Exploits with Responsible AI Considerations',\n",
       " 'Distillability of LLM Security Logic: Predicting Attack Success Rate of Outline Filling Attack via Ranking Regression',\n",
       " 'EAGER: Edge-Aligned LLM Defense for Robust, Efficient, and Accurate Cybersecurity Question Answering',\n",
       " 'Evaluating the Robustness of Large Language Model Safety Guardrails Against Adversarial Attacks',\n",
       " 'EvilGenie: A Reward Hacking Benchmark',\n",
       " 'Ghosting Your LLM: Without The Knowledge of Your Gradient and Data',\n",
       " 'LLM-CSEC: Empirical Evaluation of Security in C/C++ Code Generated by Large Language Models',\n",
       " 'Medical Malice: A Dataset for Context-Aware Safety in Healthcare LLMs',\n",
       " 'Prompt Fencing: A Cryptographic Approach to Establishing Security Boundaries in Large Language Model Prompts',\n",
       " 'RoguePrompt: Dual-Layer Ciphering for Self-Reconstruction to Circumvent LLM Moderation',\n",
       " 'Securing the Model Context Protocol (MCP): Risks, Controls, and Governance',\n",
       " 'Supporting Students in Navigating LLM-Generated Insecure Code',\n",
       " 'The Devil in the Details: Emergent Misalignment, Format and Coherence in Open-Weights LLMs',\n",
       " 'Towards Realistic Guarantees: A Probabilistic Certificate for SmoothLLM'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a78cc68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AGENTSAFE: A Unified Framework for Ethical Assurance and Governance in Agentic AI',\n",
       " 'ASTRIDE: A Security Threat Modeling Platform for Agentic-AI Applications',\n",
       " 'Addressing Logical Fallacies In Scientific Reasoning From Large Language Models: Towards a Dual-Inference Training Framework',\n",
       " 'An Empirical Study on the Security Vulnerabilities of GPTs',\n",
       " 'Assertion-Conditioned Compliance: A Provenance-Aware Vulnerability in Multi-Turn Tool-Calling Agents',\n",
       " 'AutoGuard: A Self-Healing Proactive Security Layer for DevSecOps Pipelines Using Reinforcement Learning',\n",
       " 'Automatic Attack Discovery for Few-Shot Class-Incremental Learning via Large Language Models',\n",
       " 'Automating the Refinement of Reinforcement Learning Specifications',\n",
       " 'BackportBench: A Multilingual Benchmark for Automated Backporting of Patches',\n",
       " 'Bias Injection Attacks on RAG Databases and Sanitization Defenses',\n",
       " 'BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents',\n",
       " 'COGNITION: From Evaluation to Defense against Multimodal LLM CAPTCHA Solvers',\n",
       " 'Chameleon: Adaptive Adversarial Agents for Scaling-Based Visual Prompt Injection in Multimodal AI Systems',\n",
       " 'Context-Aware Hierarchical Learning: A Two-Step Paradigm towards Safer LLMs',\n",
       " 'Contextual Image Attack: How Visual Context Exposes Multimodal Safety Vulnerabilities',\n",
       " \"Cybersecurity AI: The World's Top AI Agent for Security Capture-the-Flag (CTF)\",\n",
       " 'Debate with Images: Detecting Deceptive Behaviors in Multimodal Large Language Models',\n",
       " 'Decentralized Multi-Agent System with Trust-Aware Communication',\n",
       " 'DialogGuard: Multi-Agent Psychosocial Safety Evaluation of Sensitive LLM Responses',\n",
       " 'Driving Beyond Privilege: Distilling Dense-Reward Knowledge into Sparse-Reward Policies',\n",
       " 'EEA: Exploration-Exploitation Agent for Long Video Understanding',\n",
       " 'EmoRAG: Evaluating RAG Robustness to Symbolic Perturbations',\n",
       " 'Ensemble Privacy Defense for Knowledge-Intensive LLMs against Membership Inference Attacks',\n",
       " 'Evaluating Generalization Capabilities of LLM-Based Agents in Mixed-Motive Scenarios Using Concordia',\n",
       " 'Factor(T,U): Factored Cognition Strengthens Monitoring of Untrusted AI',\n",
       " 'FiMMIA: scaling semantic perturbation-based membership inference across modalities',\n",
       " 'From monoliths to modules: Decomposing transducers for efficient world modelling',\n",
       " 'GPTrace: Effective Crash Deduplication Using LLM Embeddings',\n",
       " 'Goal-Driven Reward by Video Diffusion Models for Reinforcement Learning',\n",
       " 'Goal-Oriented Multi-Agent Semantic Networking: Unifying Intents, Semantics, and Intelligence',\n",
       " 'HAVEN: Hierarchical Adversary-aware Visibility-Enabled Navigation with Cover Utilization using Deep Transformer Q-Networks',\n",
       " 'HarnessAgent: Scaling Automatic Fuzzing Harness Construction with Tool-Augmented LLM Pipelines',\n",
       " 'Hierarchical Vision Language Action Model Using Success and Failure Demonstrations',\n",
       " 'Immunity memory-based jailbreak detection: multi-agent adaptive guard for large language models',\n",
       " 'Improving Phishing Resilience with AI-Generated Training: Evidence on Prompting, Personalization, and Duration',\n",
       " 'In-Context Representation Hijacking',\n",
       " 'Invasive Context Engineering to Control Large Language Models',\n",
       " 'Is Vibe Coding Safe? Benchmarking Vulnerability of Agent-Generated Code in Real-World Tasks',\n",
       " 'Knowing oneself with and through AI: From self-tracking to chatbots',\n",
       " 'Large Language Model based Smart Contract Auditing with LLMBugScanner',\n",
       " 'Large Language Models Cannot Reliably Detect Vulnerabilities in JavaScript: The First Systematic Benchmark and Evaluation',\n",
       " 'Latent Debate: A Surrogate Framework for Interpreting LLM Thinking',\n",
       " 'LeechHijack: Covert Computational Resource Exploitation in Intelligent Agent Systems',\n",
       " 'Lost in Modality: Evaluating the Effectiveness of Text-Based Membership Inference Attacks on Large Multimodal Models',\n",
       " 'Many-to-One Adversarial Consensus: Exposing Multi-Agent Collusion Risks in AI-Based Healthcare',\n",
       " 'MarkTune: Improving the Quality-Detectability Trade-off in Open-Weight LLM Watermarking',\n",
       " 'Mitigating Indirect Prompt Injection via Instruction-Following Intent Analysis',\n",
       " 'Multimodal Reinforcement Learning with Agentic Verifier for AI Agents',\n",
       " 'PBFuzz: Agentic Directed Fuzzing for PoV Generation',\n",
       " 'Prior preferences in active inference agents: soft, hard, and goal shaping',\n",
       " 'RRPO: Robust Reward Policy Optimization for LLM-based Emotional TTS',\n",
       " 'ReJump: A Tree-Jump Representation for Analyzing and Improving LLM Reasoning',\n",
       " 'Red Teaming Large Reasoning Models',\n",
       " 'Rep3Net: An Approach Exploiting Multimodal Representation for Molecular Bioactivity Prediction',\n",
       " 'Retrieval-Augmented Few-Shot Prompting Versus Fine-Tuning for Code Vulnerability Detection',\n",
       " 'Reward Auditor: Inference on Reward Modeling Suitability in Real-World Perturbed Scenarios',\n",
       " 'RoCo: Role-Based LLMs Collaboration for Automatic Heuristic Design',\n",
       " 'SELF: A Robust Singular Value and Eigenvalue Approach for LLM Fingerprinting',\n",
       " 'SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment',\n",
       " 'SRPG: Semantically Reconstructed Privacy Guard for Zero-Trust Privacy in Educational Multi-Agent Systems',\n",
       " 'Securing Large Language Models (LLMs) from Prompt Injection Attacks',\n",
       " 'SoK: a Comprehensive Causality Analysis Framework for Large Language Model Security',\n",
       " 'Systems Security Foundations for Agentic Computing',\n",
       " 'TempPerturb-Eval: On the Joint Effects of Internal Temperature and External Perturbations in RAG Robustness',\n",
       " 'The MEVIR Framework: A Virtue-Informed Moral-Epistemic Model of Human Trust Decisions',\n",
       " 'The Trojan Knowledge: Bypassing Commercial LLM Guardrails via Harmless Prompt Weaving and Adaptive Tree Search',\n",
       " 'Tipping the Dominos: Topology-Aware Multi-Hop Attacks on LLM-Based Multi-Agent Systems',\n",
       " 'Topology Matters: Measuring Memory Leakage in Multi-Agent LLMs',\n",
       " 'Toward a Safe Internet of Agents',\n",
       " 'Towards better dense rewards in Reinforcement Learning Applications',\n",
       " 'TradeTrap: Are LLM-based Trading Agents Truly Reliable and Faithful?',\n",
       " 'TrojanLoC: LLM-based Framework for RTL Trojan Localization',\n",
       " 'Vision to Geometry: 3D Spatial Memory for Sequential Embodied MLLM Reasoning and Exploration',\n",
       " 'WaterSearch: A Quality-Aware Search-based Watermarking Framework for Large Language Models',\n",
       " 'Watermarks for Embeddings-as-a-Service Large Language Models',\n",
       " 'When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models',\n",
       " 'WildCode: An Empirical Analysis of Code Generated by ChatGPT'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21ef32f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d094a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "papers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
