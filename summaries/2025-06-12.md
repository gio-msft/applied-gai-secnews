üõ°Ô∏è **Sentinel: SOTA model to protect against prompt injections** [source](http://arxiv.org/pdf/2506.05446v1.pdf) #security 

 Sentinel establishes a new state-of-the-art for overseeing and defending large language models from prompt injection attacks with unmatched detection accuracy and real-world efficiency.
 - The Sentinel detection model achieved an average accuracy of 98.7% and an F1-score of 0.98 on a diverse internal test set, outperforming the leading baseline by 13.9 percentage points in accuracy and 25.2 points in F1-score.
 - On four public prompt injection benchmarks, Sentinel demonstrated a consistent and significant advantage, with an average F1-score of 0.938‚Äînearly 23 points higher than top existing models.
 - The compact architecture of Sentinel enables real-time evaluation with an average latency of roughly 0.02 seconds per prompt on moderate GPU hardware, making it suitable for scalable, practical LLM security deployments.

<br>

üõ°Ô∏è **To Protect the LLM Agent Against the Prompt Injection Attack with Polymorphic Prompt** [source](http://arxiv.org/pdf/2506.05739v1.pdf) #security 

 A lightweight, model-agnostic approach dynamically randomizes prompt structures to block over 98% of prompt injection attacks while maintaining performance and minimizing overhead.
 - Polymorphic Prompt Assembling (PPA) reduces prompt injection attack success rates to 1.83% on GPT-3.5, 1.92% on GPT-4, 4.28% on DeepSeek-V3, and 8.17% on LLaMA-3, consistently defending against over 98% of attacks on major models.
 - PPA achieves a competitive accuracy of 97.68% on the Pint-Benchmark and 99.40% on the GenTel-Bench, matching or outperforming state-of-the-art defenses, while incurring virtually zero runtime overhead (0.06 ms per request) and requiring no GPU resources.
 - Long, structured ASCII-based separators with explicit boundary markers‚Äîautomatically optimized using genetic algorithms‚Äîare most effective at isolating user input and thwarting injection, demonstrating that prompt diversity and unpredictability are critical for robust defense.

<br>

üìß **LLMail-Inject: A Dataset from a Realistic Adaptive Prompt Injection Challenge** [source](http://arxiv.org/pdf/2506.09956v1.pdf) #security 

 The LLMail-Inject challenge uncovered that even highly adaptive and creative prompt injection attacks are largely blocked by robust, layered defenses‚Äîespecially when using detection ensembles and context-aware LLM judges.
 - Less than 1% of over 370,000 submitted attacks successfully bypassed all defenses and achieved end-to-end prompt injections in a simulated LLM-based email assistant, demonstrating the substantial challenge for attackers in realistic settings.
 - The LLM-Judge defense achieved the highest single-defense detection rate with a recall of up to 99.4%, while combining multiple defenses in an ensemble further increased detection rates to over 99.7% for malicious tool-call attempts.
 - Adaptive attackers frequently exploited special formatting tokens, multilingual prompts, and obfuscation strategies, but updated and stacked defense mechanisms in Phase 2 reduced successful attack rates from 0.8% to 0.3%, highlighting the effectiveness of iterative security improvements.

<br>

üõ°Ô∏è **Detection Method for Prompt Injection by Integrating Pre-trained Model and Heuristic Feature Engineering** [source](http://arxiv.org/pdf/2506.06384v1.pdf) #security 

 A dual-channel fusion approach combining machine learning and heuristics markedly boosts LLM security against evolving prompt injections with broad effectiveness across platforms.
 - The dual-channel DMPI-PMHFE framework achieved the highest recall rate of 98.59% and F1-score of 98.29% on internal benchmark datasets, outperforming existing prompt injection detection models across multiple metrics.
 - When applied to popular large language models, DMPI-PMHFE reduced prompt injection attack success rates by up to 80%, lowering the success rate on vulnerable models like glm-4-9b-chat from 71.71% to 14.34%.
 - Ablation studies show that integrating both semantic (DeBERTa-based) and explicit pattern (heuristic rule) features resulted in consistent improvements across accuracy, recall, and F1-score, with only a minor trade-off in precision.

<br>

üõ°Ô∏è **Design Patterns for Securing LLM Agents against Prompt Injections** [source](http://arxiv.org/pdf/2506.08837v2.pdf) #security 

 Adopting composable, application-specific design patterns enables practical and provable resilience of LLM agents to prompt injection threats.
 - Imposing explicit architectural patterns‚Äîsuch as separating privileged LLMs from untrusted data handlers and isolating tool access‚Äîcan significantly reduce or even eliminate the risk of prompt injection attacks in LLM-based agents.
 - No single defensive mechanism suffices; combining multiple design patterns (e.g., action selectors, map-reduce processing, context minimization) tailored to specific application requirements offers the strongest practical security stance against prompt injections.
 - General-purpose LLM agents cannot guarantee robust prompt injection defense with current heuristics, but application-specific agents designed with clear trust boundaries and constrained workflows provide meaningful and reliable protection for real-world use cases.

<br>

üõ°Ô∏è **TokenBreak: Bypassing Text Classification Models Through Token Manipulation** [source](http://arxiv.org/pdf/2506.07948v1.pdf) #security 

 Token manipulation leveraging tokenizer weaknesses can render popular NLP-based security systems ineffective, but a simple tokenizer translation defense sharply reduces their vulnerability.
 - Text classification models using BPE or WordPiece tokenization were highly vulnerable to the TokenBreak attack, with up to 78.93% of spam samples for WordPiece models and over 76% of toxicity samples being manipulated into false negatives, while models using Unigram tokenization were fully robust against such manipulations.
 - Input manipulated by TokenBreak consistently bypassed detection by targeted classifiers but remained fully understandable and actionable to both large language models (LLMs) and human recipients, directly exposing downstream systems to the attacks the classifiers were meant to prevent.
 - Inserting a Unigram tokenizer layer before BPE or WordPiece-based classifiers reduced TokenBreak's attack success rate from a mean of 33.09% to 12.63%, demonstrating an effective mitigation that does not require retraining the original model.

<br>

üõ°Ô∏è **Effective Red-Teaming of Policy-Adherent Agents** [source](http://arxiv.org/pdf/2506.09600v1.pdf) #security 

 Policy-adherent AI agents are much more vulnerable to strategic, policy-aware adversarial attacks than previously measured, revealing a pressing need for research-driven, robust safeguards.
 - A targeted adversarial framework, CRAFT, achieves a 70% policy-violation attack success rate against policy-adherent AI agents, significantly outperforming traditional attacks like DAN prompts (35%) and emotional manipulation (50%).
 - Lightweight defense strategies, such as hierarchical prompting and policy reminders, reduce but do not eliminate vulnerability, with attack success rates exceeding 80% under persistent adversarial trials.
 - Current evaluation benchmarks relying on cooperative user behavior dramatically underestimate real-world vulnerabilities, as agents fail to enforce even simple policies like user authentication under adversarial conditions.

<br>

üõ°Ô∏è **When Style Breaks Safety: Defending Language Models Against Superficial Style Alignment** [source](http://arxiv.org/pdf/2506.07452v1.pdf) #security 

 Superficial style alignment during LLM fine-tuning can severely undermine safety, but targeted style-matched safety augments offer a simple and robust fix.
 - Across 32 large language models and seven jailbreak benchmarks, the presence of superficial style patterns in malicious queries inflated the attack success rate (ASR) in nearly all models, with 28 out of 32 models showing increased vulnerability, regardless of model family, size, or release date.
 - Fine-tuning language models on instructions with specific stylistic patterns (such as lists or poems) significantly increased their susceptibility to jailbreaks using those same styles, but incorporating a small quantity of safety training data matched to these style patterns effectively mitigated this risk.
 - The proposed SafeStyle defense‚Äîaugmenting fine-tuning data with just 50 safety examples tailored to the target style‚Äîoutperformed existing baselines, consistently reducing ASR and maintaining safety across diverse models and style settings, without sacrificing the model's ability to adapt to requested styles.

<br>

‚ö†Ô∏è **Beyond Jailbreaks: Revealing Stealthier and Broader LLM Security Risks Stemming from Alignment Failures** [source](http://arxiv.org/pdf/2506.07402v1.pdf) #security 

 Harmless-seeming prompts can reliably induce dangerous factual errors in top LLMs, exposing a stealthy and pervasive safety risk that eludes existing safeguards.
 - Leading large language models can be manipulated via harmless-looking prompts to produce factually incorrect and plausible responses that pose real-world safety risks, with attack success rates exceeding 90% in many cases.
 - Multilingual input variants (especially Chinese and German) further increase model vulnerability to implicit harm attacks, revealing gaps in current safety alignment across languages.
 - Simple adversarial manipulations, such as prompting or suffix attacks, can reduce factual accuracy from over 90% to near 0% for many advanced models, emphasizing the urgent need for safety strategies that enforce truthfulness as well as policy compliance.

<br>

üßë‚Äçüè´ **Small Models, Big Support: A Local LLM Framework for Teacher-Centric Content Creation and Assessment using RAG and CAG** [source](http://arxiv.org/pdf/2506.05925v1.pdf) #general 

 Engineered small LLMs, when carefully augmented and locally hosted, can reliably empower teachers with custom content creation and assessment tools nearing the quality of much larger proprietary models.
 - Small, locally deployed language models (3B-7B parameters) enhanced with Retrieval and Context Augmented Generation achieved qualitative scores approaching those of large proprietary models for generating customized educational content, with iterative refinement often converging within three prompts.
 - An auxiliary 3B-parameter LLM verifier improved system reliability and safety, accurately classifying 90% of unsafe queries and 88% of off-topic prompts, reducing the risk of jailbreaking and hallucination in educator support workflows.
 - Pilot deployment in a real-world college physics course demonstrated the framework‚Äôs cost-effectiveness, data privacy, and practicality, confirming that open-source, on-premises LLM systems can provide robust support for educators without the need for cloud-based infrastructure.

<br>

üîç **Auditing Black-Box LLM APIs with a Rank-Based Uniformity Test** [source](http://arxiv.org/pdf/2506.06975v3.pdf) #security 

 This work presents a stealthy, statistically powerful test that reliably detects covert model changes in LLM APIs, addressing a critical gap in cloud-based AI transparency.
 - A newly proposed rank-based uniformity test (RUT) for black-box LLM API auditing delivers higher statistical power and reliability in detecting model substitutions‚Äîsuch as quantization, adverse fine-tuning, jailbreaking, and full replacement‚Äîcompared to previous state-of-the-art tests, even under constrained query budgets.
 - RUT maintains robust detection performance and resilience against adversarial providers, outperforming Maximum Mean Discrepancy (MMD) and Kolmogorov-Smirnov (KS) baselines in real-world and simulated threat scenarios, including live commercial LLM APIs.
 - The RUT approach is both query-efficient and less susceptible to evasion through prompt distribution manipulation, enabling frequent and stealthy audits while minimizing operational cost and false positives due to minor output formatting differences.

<br>

ü¶∫ **Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety Assurance** [source](http://arxiv.org/pdf/2506.06444v1.pdf) #security 

 Inference-time scaling for LLM safety, enabled by multifurcation reward modeling, beats previous methods in both efficiency and attack resilience without compromising helpful, non-repetitive outputs.
 - The proposed SAFFRON-1 method reduces attack success rates against strong jailbreaking attacks by more than 50% compared to baseline inference-scaling methods, achieving 0.409 on Harmful HEx-PHI and 0.175 on Ai2 Refusals datasets.
 - By introducing a multifurcation reward model (MRM), SAFFRON-1 decreases reward model evaluation calls from K per search step to just one, dramatically improving compute efficiency‚Äîachieving similar safety with only one-third the compute of the best baseline.
 - SAFFRON-1 maintains diverse and contextually appropriate responses under attack, avoiding repetitive or generic rejections by delivering output quality comparable to or better than training-based defense methods while providing more genuine and helpful responses.

<br>

üõ°Ô∏è **Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models** [source](http://arxiv.org/pdf/2506.07645v1.pdf) #security 

 Small, targeted changes to key words can easily bypass safety mechanisms in large language models for less-resourced languages, even with minimal linguistic effort.
 - Targeted character- and word-level perturbations on important words can cause large language models to change their predictions in up to 48% of cases for some task-language-model combinations, revealing significant vulnerabilities, especially in low-resource languages like Polish.
 - The SHAP attribution method is markedly more effective than other attribution techniques in identifying high-impact words whose perturbation most reliably fools language models, yielding substantially higher attack success rates.
 - Despite training on large and diverse datasets, modern LLMs‚Äîincluding multilingual and Polish-dedicated models‚Äîare notably susceptible to simple, human-intelligible input modifications such as typos or character insertions, underscoring a pressing need for robustness checks during model development and deployment.

<br>

üîí **AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)** [source](http://arxiv.org/pdf/2506.08885v2.pdf) #security 

 This work introduces a comprehensive adversarial benchmark and a novel alignment method that structurally reorganizes LLM representations, dramatically improving resistance to jailbreak and stealth attacks.
 - Attack Success Rates remain high for both open- and closed-source large language models (LLMs) across 9,000 adversarial prompts, with models like Vicuna-1.5, GPT-3.5, and Mixtral-7B ranking as most vulnerable based on a new geometry-aware metric.
 - The proposed GRACE alignment framework reduces adversarial Attack Success Rates by 35‚Äì39% across all major attack categories by reshaping internal representations to separate safe and unsafe completions in latent space, without modifying the base LLM.
 - A new diagnostic, the Adversarial Vulnerability Quality Index (AVQI), reveals that adversarial prompts often exploit a 'latent camouflage' effect‚Äîembedding close to safe outputs in the model's internal geometry‚Äîthereby exposing a critical failure mode in current surface-level alignment methods.

<br>

üö® **TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts** [source](http://arxiv.org/pdf/2506.07596v1.pdf) #security 

 TwinBreak shows that safety alignment in LLMs can be surgically removed with high effectiveness and little loss in model performance, using only a small set of tailored prompt pairs.
 - A new method called TwinBreak enables efficient removal of safety alignment from large language models (LLMs), successfully jailbreaking 16 models from five vendors with attack success rates ranging from 89% to 98%.
 - TwinBreak operates by pruning only 1‚Äì5% of targeted model parameters identified via fine-grained comparison between highly similar harmful and harmless 'twin prompts,' resulting in minimal utility degradation (typically less than 1.2% on standard benchmarks).
 - The approach demonstrates superior effectiveness and runtime efficiency compared to state-of-the-art white-box LLM jailbreaks, and generalizes robustly across LLM architectures and previously unseen harmful prompts.

<br>

üõ°Ô∏è **RSafe: Incentivizing proactive reasoning to build robust and adaptive LLM safeguards** [source](http://arxiv.org/pdf/2506.07736v2.pdf) #security 

 RSafe drives a leap in robust, adaptable LLM safety moderation by combining explicit reasoning, reinforcement learning, and policy flexibility for real-world threat coverage.
 - RSafe, a reasoning-based safeguard for language models, achieves accuracy and F1 scores matching or surpassing state-of-the-art moderation tools on six safety benchmarks while relying on a limited amount of public data.
 - RSafe demonstrates exceptional out-of-distribution generalization, maintaining robust harmful content detection performance even against novel harmful categories and adversarial 'jailbreak' attacks, with less than 2.5% performance drop compared to up to 85% for conventional models.
 - The system's adaptive variant allows end-users to specify custom safety policies during inference, resulting in further improvements in safeguarding LLM outputs and offering full transparency via step-by-step, human-readable reasoning for each moderation decision.

<br>

üõ°Ô∏è **FedShield-LLM: A Secure and Scalable Federated Fine-Tuned Large Language Model** [source](http://arxiv.org/pdf/2506.05640v1.pdf) #security 

 This work presents a scalable and secure federated LLM fine-tuning method that outperforms privacy baselines in both model quality and resistance to data leakage, all while remaining efficient for real-world, sensitive deployments.
 - FedShield-LLM integrates fully homomorphic encryption and unstructured pruning with low-rank adaptation to provide robust privacy protection against inference attacks while enabling federated fine-tuning of large language models in resource-constrained environments.
 - Experimental results show that FedShield-LLM achieves a superior average BERTScore F1 of 0.6865‚Äîoutperforming both DP-LoRA (0.6130) and Vanilla-FL (0.5756)‚Äîand maintains lower training loss and faster convergence across multiple representative NLP datasets.
 - The approach reduces computational and communication overhead by updating only lightweight adapter layers, and encrypted sparse updates ensure that even honest-but-curious servers cannot reconstruct private data, making the system practical for sensitive cross-silo deployments such as healthcare and finance.

<br>

üõ°Ô∏è **Detecting State Manipulation Vulnerabilities in Smart Contracts Using LLM and Static Analysis** [source](http://arxiv.org/pdf/2506.08561v2.pdf) #security 

 LLMs, combined with static analysis, can proactively detect and explain price manipulation vulnerabilities in DeFi protocols before attacks occur.
 - A combined approach using large language models (LLMs) and static analysis enables real-time detection of price manipulation vulnerabilities in DeFi smart contracts, identifying attacks before execution within one minute.
 - The detection framework, PriceSleuth, successfully identified price manipulation attacks across four diverse types of DeFi protocols, demonstrating effectiveness regardless of protocol complexity.
 - By mapping the dependency and propagation paths of price variables, the system not only detects susceptible mechanisms but also verifies if manipulated prices lead to genuine malicious exploitation, reducing false positives.

<br>

üõ°Ô∏è **Quality-Diversity Red-Teaming: Automated Generation of High-Quality and Diverse Attackers for Large Language Models** [source](http://arxiv.org/pdf/2506.07121v1.pdf) #security 

 QDRT raises the bar for automated red-teaming by systematically generating broader and more effective attack behaviors for LLMs, advancing comprehensive safety evaluations.
 - The Quality-Diversity Red-Teaming (QDRT) framework increases both the diversity and effectiveness of adversarial prompts against large language models, achieving a 22.13% improvement in QD-Score and 19.33% greater behavior coverage over prior state-of-the-art methods.
 - QDRT achieves nearly complete coverage of risk categories and attack styles, with coverage rates up to 97% across various victim LLMs, significantly outperforming methods like GFlowNets and REINFORCE that do not optimize for structured behavioral diversity.
 - Generated adversarial attacks from QDRT exhibit higher transferability to unseen LLMs and maintain high toxicity scores, demonstrating that diverse, goal-driven attacks are not only more comprehensive but also more potent security assessment tools.

<br>

üõ°Ô∏è **Adversarial Attack Classification and Robustness Testing for Large Language Models for Code** [source](http://arxiv.org/pdf/2506.07942v1.pdf) #security 

 Word-level adversarial tweaks and even random comments can easily destabilize code generation models, highlighting blind spots in LLM robustness.
 - Large Language Models for Code are most robust to sentence-level input perturbations, but show significant vulnerabilities to word-level changes, with performance degrading up to 40% under adversarial word substitutions.
 - Mono-language models consistently outperform multi-language models in adversarial robustness, exhibiting lower drops in functionality and higher reliability across diverse perturbation types.
 - Randomly inserted or adversarially manipulated comments can cause a substantial decrease in code generation accuracy, revealing that seemingly innocuous non-code elements significantly influence model outputs.

<br>

üßë‚Äç‚öñÔ∏è **LLMs Cannot Reliably Judge (Yet?): A Comprehensive Assessment on the Robustness of LLM-as-a-Judge** [source](http://arxiv.org/pdf/2506.09443v1.pdf) #security 

 Despite their popularity, current LLM-as-a-Judge systems are easily tricked by adversarial manipulations, exposing critical reliability gaps and clear paths for strengthening automated AI evaluation methods.
 - State-of-the-art LLM-as-a-Judge systems remain highly vulnerable to adversarial manipulations, with attack success rates frequently exceeding 80% for several common attack strategies, such as Combined Attack and PAIR.
 - System robustness is strongly dependent on the choice of prompt template and judge model, where optimizing prompt components and employing dedicated judge-tuned models like JudgeLM-13B can significantly improve resistance to attacks.
 - Real-world LLM judge deployments, such as Alibaba‚Äôs PAI platform, may withstand standard attacks but can still be compromised by composite adversarial techniques, revealing hidden loopholes that require ongoing evaluation and mitigation.

<br>

üõ°Ô∏è **Your Agent Can Defend Itself against Backdoor Attacks** [source](http://arxiv.org/pdf/2506.08336v2.pdf) #security 

 LLM agents can autonomously and robustly self-monitor to catch sophisticated backdoor attacks, far outperforming previously known methods.
 - A two-level consistency check that compares an LLM agent's thoughts and actions, as well as reconstructed instructions, can reduce backdoor attack success rates by up to 90% in sensitive tasks like database operations.
 - Existing defenses such as pruning, rephrasing, or fine-tuning remain largely ineffective, often failing to reduce the attack success rate below 90%, while the proposed method consistently achieves rates as low as 2‚Äì12% across diverse models and scenarios.
 - Integrating chain-of-thought explanations into the defense not only boosts detection accuracy‚Äîcutting both attack success and false positive rates by nearly half‚Äîbut also adds human-interpretable transparency for users.

<br>

üß™ **A Systematic Review of Poisoning Attacks Against Large Language Models** [source](http://arxiv.org/pdf/2506.06518v1.pdf) #security 

 Poisoning attacks against LLMs are rapidly evolving in sophistication, outpacing current defenses and threatening a broad range of AI applications.
 - Over 60% of published LLM poisoning attacks focus on enhancing stealthiness or persistence, with novel techniques enabling poisons to evade both automated and human detection even at low poisoning rates.
 - Poisoning attacks as low as 1% of training data have been shown to achieve attack success rates above 90% in some settings, while clean-label poisons (which maintain semantically correct labels) are much harder to detect and defend against compared to dirty-label methods.
 - Key defenses such as ONION, STRIP, and Neural Cleanse have repeatedly been circumvented by new attack strategies, revealing that state-of-the-art LLMs‚Äîincluding those used for code, images, and multi-modal tasks‚Äîremain highly vulnerable to advanced and persistent poisoning attacks.

<br>

üß® **Joint-GCG: Unified Gradient-Based Poisoning Attacks on Retrieval-Augmented Generation Systems** [source](http://arxiv.org/pdf/2506.06151v1.pdf) #security 

 Jointly optimized adversarial attacks can break RAG systems with unprecedented success, readily transfer to unseen models, and bypass common defenses.
 - A unified gradient-based attack framework significantly increases the attack success rate on Retrieval-Augmented Generation (RAG) systems, outperforming prior state-of-the-art methods by up to 25% and averaging a 5% improvement across diverse retrievers and generators.
 - Crafted poisoned documents via joint optimization exhibit strong cross-model transferability, achieving up to 57% attack success rate on unseen generator models and high efficacy even when transferred across different retrievers, thus exposing practical vulnerabilities in real-world RAG deployments.
 - Conventional defensive mechanisms, such as perplexity-based filtering and input smoothing, provide only partial mitigation, with attack success rates from the unified attack method remaining alarmingly high, underscoring a pressing need for more robust, retrieval-aware security solutions in RAG systems.

<br>


üõ°Ô∏è **Large Language Models for Multilingual Vulnerability Detection: How Far Are We?** [source](http://arxiv.org/pdf/2506.07503v1.pdf) #security 

 Advanced instruction-tuned LLMs like GPT-4o set new benchmarks in multilingual and multi-granularity vulnerability detection, outperforming previous language models particularly in critical and complex cases.
 - GPT-4o, when enhanced with instruction tuning and few-shot prompting, achieved the highest accuracy (0.7196) for function-level and the highest F1-score (0.6641) for line-level multilingual vulnerability detection, outperforming top pre-trained language models like CodeT5P by 19.2% and 37.2% respectively.
 - This leading LLM approach demonstrated superior capabilities in detecting high-severity vulnerabilities and the top-25 most dangerous security weaknesses (CWE-IDs), offering an improvement of 14.5% to 41.2% in detection rates over traditional models across diverse programming languages.
 - Model size and explicit reasoning abilities were not decisive factors for performance improvement, highlighting that careful instruction tuning and strategic prompting are more critical for robust, multilingual vulnerability detection.

<br>

üõ°Ô∏è **Boosting Vulnerability Detection of LLMs via Curriculum Preference Optimization with Synthetic Reasoning Data** [source](http://arxiv.org/pdf/2506.07390v1.pdf) #security 

 Curriculum-based reasoning data and targeted optimization make LLMs drastically better at distinguishing software vulnerabilities, even when code changes are subtle.
 - Integrating bi-directional vulnerability reasoning data and curriculum preference optimization enables large language models to surpass previous state-of-the-art methods in software vulnerability detection, with accuracy improvements ranging from 12.24% to 22.77%.
 - Newly introduced ReVD framework, when applied to real-world code datasets, achieves up to 7.93% higher F1 scores and 12.65% greater Vulnerability Pair-Score compared to top-performing baselines, demonstrating enhanced ability to distinguish subtle vulnerability patterns from semantically similar code.
 - Existing large language models, such as GPT-4, fail to differentiate 78.62% of vulnerable and fixed code pairs due to a lack of vulnerability-specific reasoning data, while the ReVD approach significantly improves reasoning and detection across diverse and imbalanced vulnerability types.

<br>

üö´ **From Rogue to Safe AI: The Role of Explicit Refusals in Aligning LLMs with International Humanitarian Law** [source](http://arxiv.org/pdf/2506.06391v1.pdf) #security 

 Explicit, well-explained refusals‚Äîespecially when enhanced by system prompts‚Äîare key to robust LLM alignment with humanitarian legal standards, but vulnerabilities remain in technical or nuanced cases.
 - Across eight leading large language models, most systems refused over 90% of prompts designed to violate International Humanitarian Law, with closed-source models achieving refusal rates above 98% but open-source models showing more inconsistent behavior.
 - The clarity and helpfulness of refusal explanations varied substantially, with explanatory refusal rates ranging from just 7.76% to 80.12%, and some models providing only terse denials while others offered legally informed, educational feedback.
 - A standardized system-level safety prompt intervention significantly improved the proportion of helpful, explanatory refusals in six out of eight models‚Äîraising explanatory rates to over 90% in many cases‚Äîdemonstrating that simple prompt engineering can drastically enhance model alignment without retraining.

<br>

üõ°Ô∏è **SafeGenBench: A Benchmark Framework for Security Vulnerability Detection in LLM-Generated Code** [source](http://arxiv.org/pdf/2506.05692v1.pdf) #security 

 LLMs often generate insecure code by default, but targeted prompts and dual-judge security evaluations dramatically improve both detection and prevention of software vulnerabilities.
 - Only 37.44% of LLM-generated code samples met security standards in zero-shot scenarios, demonstrating a substantial risk of inherent vulnerabilities without explicit safety intervention.
 - Providing explicit safety instructions improved secure code generation accuracy by over 20%, and adding few-shot insecure code examples yielded an additional 3% increase, highlighting the strong impact of prompt engineering on LLM security outcomes.
 - Utilizing both LLM-based and SAST-based judges revealed complementary detection strengths, with LLM judges identifying vulnerabilities missed by SAST in 30.05% of cases and SAST catching issues missed by LLMs in 6.24%, underscoring the need for hybrid evaluation approaches.

<br>

üõ°Ô∏è **SCGAgent: Recreating the Benefits of Reasoning Models for Secure Code Generation with Agentic Workflows** [source](http://arxiv.org/pdf/2506.07313v1.pdf) #security 

 Agentic workflows can elevate standard LLMs to generate code that's both highly secure and functional, rivaling state-of-the-art reasoning models without model fine-tuning.
 - SCGAgent improves the security of code generated by Claude Sonnet-3.7 from 61% to 76% on the CWEval benchmark, while retaining nearly 98% of the model's original functionality.
 - The agentic workflow enables non-reasoning models like Sonnet-3.7 to match or outperform advanced proprietary reasoning models on generating both functional and secure code, achieving a Func-Sec@1 of 0.755 compared to 0.748 for o4-mini reasoning models.
 - Ablation studies show that providing secure coding guidelines significantly increases security outcomes over generic CWE descriptions, but incorporating dynamic unit test generation and revision is essential to maintaining high functionality.

<br>

‚úÖ **Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models** [source](http://arxiv.org/pdf/2506.09408v1.pdf) #general 

 Even simple input formatting errors can devastate LLM accuracy, but a lightweight token-level constraint method instantly restores performance, making models far more reliable for practical deployment.
 - Introducing input noise such as trivial formatting changes can cause smaller language models' accuracy on multiple-choice tasks to collapse to 0%, but applying Token Constraint Decoding (TCD) in conjunction with prompt engineering restores accuracy by up to 39% for weaker models like Gemma3 1B.
 - TCD serves as a robust, inference-time regularizer‚Äîwithout retraining‚Äîthat stabilizes model outputs by applying token-level constraints; penalty sweeps show that higher penalty values substantially boost performance, especially for underperforming instruction-tuned models, bringing their results closer to larger or more robust models.
 - While prompt-engineering alone only marginally alleviates degradation from input noise, the combination with TCD proves essential for real-world deployment reliability in structured reasoning tasks, as performance on three major benchmarks (CommonsenseQA, MMLU, and MMLU-Pro) consistently rebounds when both are employed, especially in safety-critical or user-facing scenarios.

<br>

üõ°Ô∏è **Refusal-Feature-guided Teacher for Safe Finetuning via Data Filtering and Alignment Distillation** [source](http://arxiv.org/pdf/2506.07356v1.pdf) #security 

 Refusal-feature-based data filtering and alignment distillation deliver powerful, scalable, and generalizable LLM finetuning safety‚Äîeven against sophisticated adversarial attacks.
 - A Refusal-Feature-guided Teacher (ReFT) model enables nearly perfect filtering of harmful prompts during model finetuning, achieving a harmful response rate as low as 0.5% across tasks and data scales.
 - Finetuning with the ReFT strategy consistently achieves the highest task accuracy (up to 49.0%) while preserving model safety‚Äîeven when user-submitted data includes large proportions of adversarial or harmful prompts.
 - The proposed approach generalizes across diverse language model architectures and downstream datasets, demonstrating robust safety and task performance even under advanced attack scenarios such as GCG and AutoDAN jailbreaks.

<br>

üìä **Table-r1: Self-supervised and Reinforcement Learning for Program-based Table Reasoning in Small Language Models** [source](http://arxiv.org/pdf/2506.06137v1.pdf) #general 

 Small models achieve LLM-level table reasoning by learning to program over diverse table layouts with targeted self-supervision and dynamic reinforcement learning.
 - The proposed Table-r1 method enables small-scale language models to achieve at least a 15% absolute accuracy improvement over their program-based baselines for table reasoning tasks across four diverse benchmarks.
 - By introducing a novel self-supervised Layout Transformation Inference training stage and a mixed paradigm reinforcement learning approach (mix-paradigm GRPO), the model robustly generalizes to varied table layouts and dynamically balances between program-based and text-based answering.
 - Table-r1 consistently narrows the performance gap with state-of-the-art large language models in table reasoning, matching or surpassing LLMs on some datasets while operating with only 7B to 8B model sizes.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **When Detection Fails: The Power of Fine-Tuned Models to Generate Human-Like Social Media Text** [source](http://arxiv.org/pdf/2506.09975v1.pdf) #security 

 Fine-tuned language models can cheaply generate social media posts that are virtually indistinguishable from human writing, eluding both state-of-the-art detectors and human readers.
 - Existing AI-generated text detectors perform well under idealized laboratory conditions (up to 99% accuracy), but their effectiveness drops to random-guessing levels (as low as 49-54% accuracy) when tested against social media posts generated by private, fine-tuned language models.
 - Human ability to distinguish between AI-generated and human-written social media texts is limited, with detection accuracy dropping to 54% for fine-tuned model outputs‚Äîstatistically indistinguishable from chance.
 - Fine-tuning large language models to mimic informal social media style costs as little as 3¬¢‚Äì$26 and enables attackers to easily and cheaply generate highly human-like, low-detectability posts that evade both automated and manual detection.

<br>

ü¶† **Adversarial Paraphrasing: A Universal Attack for Humanizing AI-Generated Text** [source](http://arxiv.org/pdf/2506.07001v1.pdf) #security 

 A training-free adversarial paraphrasing method makes AI-generated text nearly indistinguishable from human text for current detectors, exposing major detection vulnerabilities with minimal quality loss.
 - Adversarial paraphrasing reduces true positive rates at 1% false positive (T@1%F) by up to 98.96% on advanced AI text detectors like Fast-DetectGPT and by an average of 87.88% across diverse detectors, significantly outperforming traditional paraphrasing attacks.
 - This universal attack framework is highly transferable, successfully bypassing neural network-based, watermark-based, and zero-shot AI-generated text detectors, regardless of which model is used for guidance.
 - Despite its strong ability to evade detection, adversarial paraphrasing yields text quality comparable to baseline paraphrasing methods, with 87% of human and automatic quality ratings scoring 4 or 5 on a 5-point scale.

<br>

‚öñÔ∏è **In-Context Bias Propagation in LLM-Based Tabular Data Generation** [source](http://arxiv.org/pdf/2506.09630v1.pdf) #security 

 Small biases in prompt examples can stealthily and significantly distort fairness and subgroup outcomes in LLM-generated tabular data, especially when used collaboratively.
 - Even mild statistical biases present in in-context examples systematically propagate to the synthetic tabular data generated by large language models, leading to global distributional distortions.
 - In collaborative data generation scenarios, injecting a small fraction (as low as 40%) of biased in-context examples can compromise fairness for targeted subgroups in downstream models without substantially degrading overall utility or fidelity metrics.
 - The scale and architecture of language models influence the degree of bias propagation, with larger models or those with more in-context examples exhibiting stronger and sometimes cross-attribute vulnerability to both accidental and adversarial prompt-induced bias.

<br>

üõ°Ô∏è **AsFT: Anchoring Safety During LLM Fine-Tuning Within Narrow Safety Basin** [source](http://arxiv.org/pdf/2506.08473v2.pdf) #security 

 Confining LLM fine-tuning updates within an alignment-defined safety subspace dramatically improves resistance to harmful behavior without sacrificing performance.
 - Guiding fine-tuning updates along a model‚Äôs alignment direction preserves safety, while deviations in orthogonal directions substantially increase the risk of harmful outputs, revealing a narrow safety basin in the parameter space of large language models.
 - The Anchoring Safety in Fine-Tuning (AsFT) method reduced harmful behaviors by 7.60% and increased task performance by 3.44% compared to the previous state-of-the-art Safe LoRA, while maintaining consistent benefits across multiple datasets, models, and attack scenarios.
 - AsFT remained robust even as the proportion of malicious data increased up to 60%, and demonstrated broad compatibility across different LLM architectures and hyperparameter settings, requiring minimal fine-tuning adjustments for strong safety-performance trade-offs.

<br>

ü¶† **MalGEN: A Generative Agent Framework for Modeling Malicious Software in Cybersecurity** [source](http://arxiv.org/pdf/2506.07586v1.pdf) #security 

 A modular multi-agent system can synthesize evasive, behaviorally rich malware from scratch that current antivirus solutions struggle to detect, offering a powerful testbed for red-teaming AI-driven cyber threats.
 - MalGEN's agentic framework enabled the generation of malware samples exhibiting, on average, 11.3 unique MITRE ATT&CK Tactics, Techniques, and Procedures (TTPs) per sample, achieving coverage across 6 different tactics and 20 distinct techniques.
 - 50% of AI-generated malware artifacts produced by MalGEN successfully evaded all static antivirus detection engines evaluated on VirusTotal, revealing significant blind spots in signature-based defense tools against novel, machine-generated threats.
 - MalGEN-generated malware exhibited a high level of behavioral diversity‚Äîincluding privilege escalation, persistence, exfiltration, and defense evasion‚Äîthat mimicked real adversarial workflows, but also resulted in frequent mislabeling or undetected classification by existing commercial detection systems.

<br>

üéØ **ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Large Language Model Preference Optimization** [source](http://arxiv.org/pdf/2506.08712v1.pdf) #general 

 Focusing alignment updates only on low-confidence, high-information tokens in LLMs leads to consistently superior preference optimization with no extra resource demands.
 - Selective optimization of low-confidence (high-surprisal) tokens in large language models (LLMs) improves human preference alignment scores by up to 3-7 percentage points across diverse benchmarks compared to uniform token-level optimization approaches.
 - Targeting less than 50% of the total tokens‚Äîthose identified as preference-critical by low model confidence‚Äîleads to more efficient use of the regularization (KL) budget, mitigating overoptimization and reward hacking without additional computational cost.
 - Naively selecting a subset of tokens at random does not enhance alignment, confirming that model-internal confidence serves as a reliable and practical signal for identifying preference-critical tokens, and the approach generalizes across multiple Direct Alignment Algorithms.

<br>

üß© **Right Is Not Enough: The Pitfalls of Outcome Supervision in Training LLMs for Math Reasoning** [source](http://arxiv.org/pdf/2506.06877v1.pdf) #general 

 Correct answers don't guarantee correct reasoning‚Äîoutcome-optimized LLMs often rely on flawed logic, and step-by-step verification like ParaStepVerifier is key to exposing and mitigating this hidden weakness.
 - While outcome-supervised large language models achieve high answer accuracy rates in mathematical competitions (80.1%), only 39.7% of their solutions are judged to be mathematically sound in their reasoning process.
 - Reward hacking is widespread, with 57.7% of step-by-step solution traces in supervised fine-tuning datasets containing logical or procedural errors, leading to the risk of flawed outputs contaminating future training data.
 - The ParaStepVerifier methodology improves the identification of flawed mathematical reasoning, achieving up to a 28.56% relative F1 boost for complex, multi-step solutions and delivering higher cost-effectiveness compared to strong LLM baselines.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **Fake Friends and Sponsored Ads: The Risks of Advertising in Conversational Search** [source](http://arxiv.org/pdf/2506.06447v1.pdf) #general 

 Blurring the lines between advice and ads in conversational AI risks exploiting user trust and safety, especially in sensitive scenarios.
 - Native advertising embedded within conversational AI responses is often indistinguishable from genuine recommendations, posing an elevated risk of user manipulation, especially for individuals seeking sensitive or health-related advice.
 - Users are more likely to trust information from conversational agents than from traditional search engines, making them particularly vulnerable to the 'fake friend dilemma,' where the system's commercial incentives may not align with user well-being.
 - Effective safeguards, explicit disclosures, and regulatory intervention are urgently needed to prevent deceptive or harmful advertising practices in AI-driven conversational search, particularly in contexts involving vulnerable users.

<br>

üõ°Ô∏è **Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models** [source](http://arxiv.org/pdf/2506.07468v1.pdf) #security 

 Dynamic self-play between attacker and defender language models uncovers more diverse vulnerabilities and yields substantial safety improvements over static defense strategies, all while maintaining language capability.
 - Online self-play reinforcement learning, in which a language model alternates attacker and defender roles, reduces attack success rates against adversarial prompts by up to 85% (e.g., from 0.991 to 0.240 on WildJailBreak), significantly outperforming static alignment methods in safety benchmarks.
 - Co-evolution of attacker and defender agents in a game-theoretic zero-sum framework leads to a 21.8% increase in the diversity of discovered adversarial attacks and a 45.3% boost in semantic diversity when using hidden chain-of-thought reasoning.
 - Proactive co-adaptation in safety training achieves stronger robustness without degrading general language capabilities, as evidenced by consistent instruction-following performance (16.3% vs. 14.6% length-controlled winrate on AlpacaEval-2) compared to traditional fine-tuning approaches.

<br>

üß© **Collaborative Learning in Agentic Systems: A Collective AI is Greater Than the Sum of Its Parts** [source](http://arxiv.org/pdf/2506.05577v1.pdf) #general 

 Agent collectives that selectively share and compose modular knowledge outpace isolated or naive multi-task approaches, achieving rapid, scalable, and curriculum-driven learning.
 - Collaborative agents using modular sharing and reward- and similarity-based knowledge selection (MOSAIC) demonstrated a 170.8% increase in task completion compared to isolated learners and reduced sample requirements by up to 25% in difficult reinforcement learning benchmarks.
 - Targeted selection based on cosine similarity and performance led to a 1.9‚Äì2√ó faster learning rate, and communication among agents enabled the composition of emergent curricula, allowing agents learning simpler tasks to bootstrap agents facing more complex tasks.
 - Ablation studies revealed that the key to scalability and performance was precise, relevance-driven policy sharing; indiscriminate sharing or removing either similarity or performance-based criteria led to significant learning degradation or instability.

<br>

üõ°Ô∏è **FAIRTOPIA: Envisioning Multi-Agent Guardianship for Disrupting Unfair AI Pipelines** [source](http://arxiv.org/pdf/2506.09107v1.pdf) #general 

 Adaptive multi-agent guardianship and knowledge-driven fairness reform promise robust, context-sensitive mitigation of AI bias throughout the lifecycle.
 - A tri-layered, agent-driven architecture enables continuous, context-aware fairness monitoring and guardrail enforcement at every stage of the AI pipeline, overcoming the limitations of reactive and siloed bias detection methods.
 - Integrating interdisciplinary knowledge‚Äîleveraging structured knowledge graphs encompassing cognitive, social, and computational biases‚Äîempowers adaptive, domain-specific fairness solutions and systematic bias reflections from humans to AI.
 - The framework embeds human oversight selectively and interactively through human-in-the-loop workflows and agentic self-reflection, making fairness a dynamic, co-evolving process rather than an afterthought or static metric.

<br>

‚è±Ô∏è **Efficient RL-based Cache Vulnerability Exploration by Penalizing Useless Agent Actions** [source](http://arxiv.org/pdf/2506.07200v1.pdf) #security 

 Penalizing non-informative actions in RL-based cache attack exploration boosts training efficiency but its effectiveness is strongly influenced by cache architecture.
 - Up to 43.08% of actions taken by reinforcement learning agents in cache-timing vulnerability exploration are identified as useless, varying widely by cache architecture.
 - By penalizing non-informative actions during training, the new approach reduced the proportion of useless actions by up to 10.39 percentage points and decreased training time by up to 28% in the best case and 4.84% on average across configurations.
 - Efficiency gains and reductions in useless actions were not uniform across all cache types, revealing that the method's impact is highly dependent on underlying hardware configurations and may, in some cases, increase training time or misclassify beneficial actions.

<br>

ü¶æ **Towards Robust Deep Reinforcement Learning against Environmental State Perturbation** [source](http://arxiv.org/pdf/2506.08961v1.pdf) #security 

 Environmental state perturbations easily break mainstream DRL agents, but targeted adversarial training can dramatically boost their robustness even beyond standard conditions.
 - Mainstream deep reinforcement learning (DRL) agents experience up to near-zero performance in complex tasks when subject to feasible initial state perturbations in their environments, revealing a critical vulnerability to environmental state changes.
 - The proposed Boosted Adversarial Training (BAT) framework, which combines supervised kick-starting and adversarial fine-tuning, increases DRL agents' average performance under both perturbed and pristine conditions, outperforming established baselines like RADIAL and diversified start methods across all tested scenarios.
 - Random, non-malicious perturbations in environmental states can still significantly impair DRL agent performance, underlining the necessity for intrinsic robustness against even incidental or unexpected environment changes.

<br>

üéõÔ∏è **Robust Noise Attenuation via Adaptive Pooling of Transformer Outputs** [source](http://arxiv.org/pdf/2506.09215v1.pdf) #general 

 Adaptive attention-based pooling yields consistently superior and robust transformer outputs in noisy environments, greatly surpassing traditional pooling methods.
 - Adaptive attention-based pooling methods maintain superior performance and robustness across a wide spectrum of signal-to-noise ratios, reducing signal loss by an order of magnitude compared to traditional approaches in low signal regimes.
 - Standard pooling strategies such as average pooling, max pooling, and class tokens exhibit significant performance degradation‚Äîup to 77% drop in reward or accuracy‚Äîwhenever the noise level in input data increases or varies unpredictably.
 - In complex reinforcement learning and vision tasks, using an adaptive pooling approach consistently outperformed baselines, achieving higher test accuracy (e.g., up to 61.2% on CIFAR-100) and demonstrating reliable generalization across both synthetic and real-world data.

<br>

üß© **MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness Against VLM-based Attacks** [source](http://arxiv.org/pdf/2506.05982v2.pdf) #security 

 Multimodal CAPTCHA attacks reveal that only cognitively-rich, interactive verification resists modern AI cracking, motivating a new generation of adaptive, behavior-driven defenses.
 - Fine-tuned vision-language models surpassed 96% accuracy on simple visual and shallow-semantic CAPTCHAs but dropped below 2.5% on tasks involving interactive manipulation or multi-step reasoning, exposing a significant security gap between basic and cognitively rich challenges.
 - Human users consistently outperformed models on complex reasoning and interaction-based CAPTCHA tasks, achieving error-correction rates above 0.8 in 75% of scenarios, while models averaged between 0.5‚Äì0.7, emphasizing the importance of nuanced human behavior modeling for robust verification.
 - Three actionable design principles‚ÄîDeep Modality Coupling, Behavior-Anchored Validation, and Session-Specific Semantic Personalization‚Äîwere identified to substantially enhance CAPTCHA resilience, as single-modality or static obfuscation methods are no longer sufficient to prevent vision-language model attacks.

<br>

ü§ù **A Cram√©r-von Mises Approach to Incentivizing Truthful Data Sharing** [source](http://arxiv.org/pdf/2506.07272v1.pdf) #general 

 A new two-sample test based mechanism robustly incentivizes truthful data contributions, outperforming prior approaches in both theory and practice.
 - A novel, prior-agnostic incentive mechanism based on the Cram√©r‚Äìvon Mises two-sample test provably encourages truthful data sharing even when the data distribution is unknown or complex, avoiding assumptions required by previous methods.
 - Empirical results show that under the proposed mechanism, fabricated or AI-generated data consistently leads to higher penalties (losses) compared to truthful reporting, both in synthetic and real-world language and image datasets.
 - The approach maintains budget feasibility and individual rationality for data-sharing agents and buyers while effectively scaling incentives with the amount of genuine data submitted, allowing broader applicability in federated learning and data marketplaces.

<br>

üõ°Ô∏è **Adapting Under Fire: Multi-Agent Reinforcement Learning for Adversarial Drift in Network Security** [source](http://arxiv.org/pdf/2506.06565v1.pdf) #security 

 Co-evolving attacker and defender agents using reinforcement learning enables rapid, sample-efficient recovery of network intrusion detection accuracy after adversarial attacks.
 - Dynamic adaptation using multi-agent reinforcement learning enables network intrusion detection systems to regain up to 30% accuracy loss caused by adversarial drift, with just 2‚Äì3 adaptation steps using small sample batches.
 - Active and online learning strategies are shown to be more effective for defender adaptation than pseudo-labeling or continual learning, particularly when operating under sample constraints and rapid attack evolution.
 - Ablation studies reveal that including data distribution characteristics in the state representation substantially improves the defender's ability to select appropriate adaptation strategies and maintain high detection performance against evolving threats.

<br>

üõ°Ô∏è **From Static to Adaptive Defense: Federated Multi-Agent Deep Reinforcement Learning-Driven Moving Target Defense Against DoS Attacks in UAV Swarm Networks** [source](http://arxiv.org/pdf/2506.07392v1.pdf) #security 

 Proactive, federated reinforcement learning empowers UAV swarms with highly adaptive and energy-efficient defenses against sophisticated DoS attacks.
 - The federated multi-agent deep reinforcement learning-driven moving target defense system improved Denial-of-Service (DoS) attack mitigation rates by up to 34.6% compared to existing baselines in UAV swarm networks.
 - Average system recovery time following DoS attacks was reduced by up to 94.6%, and cumulative energy consumption and defense cost were respectively lowered by 29.3% and 98.3%, illustrating significant gains in both operational efficiency and resilience.
 - Distributed, lightweight defense mechanisms‚Äîleader switching, route mutation, and frequency hopping‚Äîenabled UAV swarms to sustain robust mission continuity under diverse and adaptive DoS attack strategies without reliance on centralized data or heavy computation.

<br>

