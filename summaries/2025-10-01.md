üíª **"Your AI, My Shell": Demystifying Prompt Injection Attacks on Agentic AI Coding Editors** [source](http://arxiv.org/pdf/2509.22040v1.pdf) #security 

 Agentic AI coding editors can be systematically hijacked via prompt injection, allowing attackers to execute malicious commands remotely with high success rates across popular platforms.
 - Prompt injection attacks enable remote adversaries to hijack agentic AI coding editors and achieve unauthorized command execution, with attack success rates reaching as high as 84.1%.
 - Vulnerability to prompt injection attacks is widespread across coding environments and models, with all tested tools‚Äîincluding Cursor and GitHub Copilot‚Äîexhibiting attack success rates between 41% and 84%.
 - Injected instructions allow AI coding editors to autonomously perform high-risk operations, including credential access and privilege escalation, across 11 attack categories such as Initial Access (93%), Collection (77%), and Privilege Escalation (71%).

<br>

üîì **bi-GRPO: Bidirectional Optimization for Jailbreak Backdoor Injection on LLMs** [source](http://arxiv.org/pdf/2509.19775v1.pdf) #security 

 A new RL-based method injects stealthy, highly effective, and generalizable jailbreak backdoors into large language models‚Äîdelivering coherent harmful outputs with triggers while preserving original safety otherwise, and remaining undetected by current defenses.
 - The bi-GRPO framework enables highly effective and stealthy jailbreak backdoor injections in large language models, achieving greater than 99% attack success rates across multiple models and datasets, while maintaining low attack success (<3.1%) on non-triggered inputs.
 - Malicious helpfulness, as measured by both GPT-4 and human evaluations, shows that bi-GRPO-generated jailbreak responses are selected as most useful or actionable in 75% or more of cases, vastly outperforming prior attack methods which score below 22%.
 - The backdoor injected via bi-GRPO generalizes across diverse harmful intent types and arbitrary trigger phrases, retains the model's original capabilities (with negligible drops in standard benchmarks), and evades state-of-the-art detection defenses such as BAIT.

<br>

ü¶∫ **Backdoor Attribution: Elucidating and Controlling Backdoor in Language Models** [source](http://arxiv.org/pdf/2509.21761v1.pdf) #security 

 This study pinpoints minimal, interpretable mechanisms behind LLM backdoors and shows a simple method for reliably detecting, localizing, and neutralizing these threats through layer-wise intervention.
 - A lightweight classifier trained on internal model representations can distinguish backdoor-triggered inputs with over 95% accuracy, demonstrating that fine-tuned LLMs encode learnable backdoor features.
 - Ablating as little as 3% of attention heads identified as backdoor contributors reduces attack success rates by over 90%, confirming the sparsity and pivotal role of these heads in backdoor activation.
 - Manipulating a single hidden state using a backdoor vector‚Äîwhich aggregates these attributed attention heads‚Äîenables either complete suppression (as low as 0.39% ASR) or near-total activation (up to 100% ASR) of backdoor behaviors, offering precise control at inference.

<br>

üé¨ **Jailbreaking on Text-to-Video Models via Scene Splitting Strategy** [source](http://arxiv.org/pdf/2509.22292v1.pdf) #security 

 It is possible to reliably circumvent commercial video model safety filters by splitting harmful prompts into sequentially benign scenes that are unsafe only in their combined context.
 - Fragmenting a harmful narrative into multiple individually benign scenes enables bypassing of text-to-video (T2V) models' safety filters, with each scene separately registering low harmfulness scores.
 - The SceneSplit method achieves exceptionally high attack success rates‚Äî77.2% for Luma Ray2, 84.1% for Hailuo, and 78.2% for Veo2‚Äîmore than doubling the baseline rates for generating unsafe videos across 11 safety categories.
 - Iterative scene manipulation and strategy reuse significantly bolster attack robustness and efficiency, with the integration of the strategy library raising success rates by up to 9.1% compared to non-reuse scenarios.

<br>

üîê **The Rogue Scalpel: Activation Steering Compromises LLM Safety** [source](http://arxiv.org/pdf/2509.22067v1.pdf) #security 

 Precise and interpretable activation steering can systematically undermine LLM safety, allowing even benign controls to reliably bypass alignment safeguards and enable universal jailbreak attacks.
 - Adding random activation steering vectors increases the rate of harmful compliance in Large Language Models from 0% to as high as 27%, with middle layers being most vulnerable.
 - Steering with sparse autoencoder-derived features further raises harmful compliance rates by 2-4%, even when those features represent benign concepts, making dangerous vectors nearly indistinguishable from safe ones.
 - Aggregating just 20 prompt-specific jailbreak steering vectors enables a universal attack that generalizes to unseen harmful prompts, boosting average harmful compliance rates by 4√ó without any access to model weights or harmful training data.

<br>

üõ°Ô∏è **Secure and Efficient Access Control for Computer-Use Agents via Context Space** [source](http://arxiv.org/pdf/2509.22256v1.pdf) #security 

 A static, context- and intent-aware access control framework keeps computer-use agents both safe and efficient, achieving strong attack resilience with minimal performance cost.
 - CSAgent, a static policy-based access control system, blocks over 99.36% of attacks on computer-use agents while introducing only 6.83% performance overhead and less than 10% utility reduction in complex environments.
 - The framework's policy generation tool identifies 1.93√ó to 4.12√ó more actionable GUI elements than previous approaches, enhancing protection for agents controlling systems via API, CLI, and GUI interfaces.
 - Shifting policy enforcement to development time with intent- and context-aware policies enables scalable, consistent, and efficient defense against both prompt injection and LLM hallucination issues without hampering legitimate agent functionality.

<br>

üö® **Automatic Red Teaming LLM-based Agents with Model Context Protocol Tools** [source](http://arxiv.org/pdf/2509.21011v1.pdf) #security 

 Automated tool poisoning for Model Context Protocol exposes new large-scale security risks in LLM-based agents by efficiently generating highly evasive attacks that current defenses rarely detect.
 - Malicious MCP tools generated by the automated red teaming framework evade detection 76.6%-95.5% of the time against state-of-the-art security scanners, exposing critical security gaps in LLM-based agent ecosystems.
 - Incorrect parameter invocation attacks are more successful than output results misinterpretation, with some agent/model combinations manipulated at rates exceeding 70%.
 - Each malicious tool variant costs under $0.03 and is generated in about 200 seconds, enabling large-scale, efficient adversarial attacks on LLM agents with minimal overhead.

<br>

üîí **You Can't Steal Nothing: Mitigating Prompt Leakages in LLMs via System Vectors** [source](http://arxiv.org/pdf/2509.21884v1.pdf) #security 

 Moving system prompts out of context and into internal vectors eliminates prompt leakage risks while improving efficiency and reliably preserving model capabilities.
 - Encoding system prompts as internal representation vectors (SysVec) drastically lowers system prompt leakage rates, achieving up to a 5-6 fold reduction in similarity scores compared to the next-best textual defenses under aggressive attacks.
 - SysVec maintains nearly identical model utility to traditional textual system prompts, with less than 1% difference in general reasoning benchmarks and response quality scores even across diverse tasks and out-of-distribution queries.
 - Deploying SysVec cuts inference overhead by up to 70% for long system prompts and offers robust resistance against both black-box and adaptive prompt leakage attacks, while also mitigating the problem of prompt forgetting in long, multi-turn conversations.

<br>

üí£ **SBFA: Single Sneaky Bit Flip Attack to Break Large Language Models** [source](http://arxiv.org/pdf/2509.21843v1.pdf) #security 

 A single, carefully-chosen bit flip can stealthily cripple top-tier language models on multiple tasks, revealing extreme and pervasive security vulnerabilities.
 - Flipping a single 'critical' bit in the weights of a state-of-the-art large language model can degrade its accuracy to below random-guess performance across standard benchmarks, regardless of whether the model uses floating-point or integer quantization.
 - The vulnerability is widespread, with tens of distinct critical bits across multiple layers and parameter types in various model architectures, indicating that susceptibility to stealthy bit-flip attacks is not limited to specific components or model sizes.
 - SBFA's efficient SKIP Search algorithm enables highly scalable attack execution‚Äîreducing evaluation cost by over 950,000-fold‚Äîand critical bit flips targeting one task also transfer to degrade accuracy on unrelated tasks like sentiment analysis and math problem solving.

<br>

üîì **Can Federated Learning Safeguard Private Data in LLM Training? Vulnerabilities, Attacks, and Defense Evaluation** [source](http://arxiv.org/pdf/2509.20680v1.pdf) #security 

 Federated learning fails to fully protect private data in LLM training, with more powerful models and targeted attacks escalating privacy risks beyond prior expectations.
 - Up to 10% of generated samples from federated learning-trained large language models exhibited over 90% similarity to original training data, indicating substantial privacy leakage.
 - Larger model sizes show statistically significant increases in privacy leakage, with advanced attack methods amplifying data recovery rates by as much as 21.4%.
 - Privacy-preserving techniques like differential privacy, update regularization, and safety alignment can reduce data leakage, but consistently degrade model performance, revealing a persistent privacy‚Äìefficacy trade-off.

<br>

üöó **STAF: Leveraging LLMs for Automated Attack Tree-Based Security Test Generation** [source](http://arxiv.org/pdf/2509.20190v1.pdf) #security 

 A domain-adapted, retrieval-augmented LLM approach dramatically advances the automation and precision of automotive security test generation from attack trees.
 - Incorporating a Retrieval-Augmented Generation (RAG) approach with large language models, the STAF framework increased the quality of security test case generation, raising the overall score for GPT-4.1 from 7.17 (vanilla) to 9.11 and DeepSeek-V3 from 5.11 to 6.11.
 - STAF significantly improved alignment, completeness, and runnability metrics of generated security test cases, with GPT-4.1‚Äôs alignment increasing from 7.00 to 9.80 and completeness from 5.50 to 8.50 when enhanced with protocol-specific context.
 - Automated test case generation using domain-adapted LLMs and behavioral protocol models (such as Mealy machines) demonstrated superior capability in producing precise, executable, and context-specific security tests for automotive systems, outperforming general-purpose LLMs.

<br>

üîì **RLCracker: Exposing the Vulnerability of LLM Watermarks with Adaptive RL Attacks** [source](http://arxiv.org/pdf/2509.20924v1.pdf) #security 

 This work reveals that current LLM watermarking schemes are highly vulnerable to adaptive, reinforcement learning-based attacks, which need minimal training data and no detector access to break security at scale.
 - Reinforcement learning-based RLCracker consistently removes between 86% and 99% of watermarks from long-form (1500-token) LLM-generated texts using only 100 short training samples, outperforming all prior removal techniques including GPT-4o and SIRA.
 - Watermark detection rates (Evasion Success Rate, ESR) rise sharply as attacker models become stronger and more reasoning-capable, with ESRs ranging from below 10% for small models to above 90% for advanced models, exposing a fundamental scaling vulnerability in current watermarking methods.
 - Rich attack prompts, especially those utilizing structured system instructions and multi-step reasoning, can drastically boost watermark evasion rates without sacrificing semantic fidelity, demonstrating that context adaptation is a key factor in undermining watermark robustness.

<br>

üîè **PMark: Towards Robust and Distortion-free Semantic-level Watermarking with Channel Constraints** [source](http://arxiv.org/pdf/2509.21057v1.pdf) #security 

 This work delivers a theoretically grounded, distortion-free semantic watermarking scheme‚ÄîPMARK‚Äîthat outperforms all prior approaches in both robustness to adversarial attacks and text generation quality, and achieves practical sampling efficiency for AI-generated content traceability.
 - Dense watermark evidence incorporated via multi-channel constraints enables PMARK's online variant to achieve over 93% true positive rate at 1% false positive rate under strong paraphrase attacks, surpassing previous semantic-level and token-level watermarking methods by up to 26% and 55.6% respectively.
 - PMARK offers distortion-free semantic watermarking and preserves high text quality, achieving perplexity as low as 4.37‚Äì4.71, which is superior or comparable to best-in-class baselines with around 20% of their typical sampling resource consumption.
 - Applying watermark constraints at the sentence level across multiple orthogonal channels provides robust resistance to both word-level attacks (e.g., word deletion and synonym substitution at rates up to 15‚Äì30%) and paraphrasing, maintaining detection rates above 95% in practice.

<br>

üõ°Ô∏è **Investigating Security Implications of Automatically Generated Code on the Software Supply Chain** [source](http://arxiv.org/pdf/2509.20277v1.pdf) #security 

 LLMs widely introduce supply chain risks by generating insecure, fabricated, or outdated external code dependencies, but targeted defense strategies can cut these risks in half.
 - Between 33% and 52% of code-generating large language model outputs contain hallucinated packages, which can be opportunistically registered and exploited by attackers, posing immediate supply chain risks.
 - Vulnerable, outdated, or deprecated external components were recommended in over 13% of generated code samples, including known-vulnerable JavaScript libraries and CI configurations susceptible to code injection or redirection hijacking.
 - A novel prompt-based defense (Chain-of-Confirmation) can reduce hallucinated package recommendations by over 50% while preserving code functionality, indicating practical mitigation for LLM-assisted secure development.

<br>

üõ°Ô∏è **A Framework for Rapidly Developing and Deploying Protection Against Large Language Model Attacks** [source](http://arxiv.org/pdf/2509.20639v1.pdf) #security 

 This work introduces a dynamic, enterprise-ready defense platform that integrates rapid threat response, safe deployment, and operational feedback loops to harden LLM applications against evolving attacks.
 - A multi-component platform enables rapid detection and deployment of protections against large language model attacks, minimizing vulnerability windows and improving adaptability to zero-day threats.
 - The release methodology supports safe, staged rollouts and immediate rollback of guardrail updates, ensuring operational stability and minimizing disruption to customer workflows.
 - The framework leverages automated threat intelligence, flexible data correlation, and human-in-the-loop feedback to continuously strengthen large language model security defenses and reduce false positive rates.

<br>

üõ°Ô∏è **RAG Security and Privacy: Formalizing the Threat Model and Attack Surface** [source](http://arxiv.org/pdf/2509.20324v1.pdf) #security 

 This paper formalizes the unique privacy and security threat landscape of RAG systems and provides actionable defense strategies for document inference, content leakage, and data poisoning.
 - Retrieval-Augmented Generation (RAG) systems are vulnerable to document-level membership inference attacks, enabling adversaries to determine whether specific documents are present in the external knowledge base, posing significant privacy risks in sensitive domains.
 - RAG architectures are susceptible to verbatim content leakage, where generated responses may inadvertently reproduce sensitive or proprietary information from retrieved documents, undermining confidentiality and data protection.
 - Data poisoning attacks on RAG systems allow adversaries to inject crafted documents into the knowledge base, manipulating outputs and facilitating the spread of misinformation or targeted content; effective mitigations require embedding-aware filtering and retrieval-level safeguards.

<br>

üõ°Ô∏è **PhishLumos: An Adaptive Multi-Agent System for Proactive Phishing Campaign Mitigation** [source](http://arxiv.org/pdf/2509.21772v1.pdf) #cyber 

 PhishLumos shifts phishing defense from reactive URL blocking to proactive campaign mitigation by leveraging infrastructure footprints and LLM-driven multi-agent reasoning‚Äîeven when malicious content is cloaked.
 - PhishLumos identified 100% of phishing campaigns in the median case, with proactive detection occurring an average of 192.8 hours (approximately eight days) before expert confirmation.
 - The system discovered 77,391 previously undetected URLs, 80.5% of which were subsequently flagged as malicious by commercial security engines, while maintaining a zero false positive rate against top-ranking legitimate websites.
 - Performance remained robust in content-inaccessible scenarios, achieving an F1-score of 0.994 and recall of 1.000, surpassing all baselines, whose efficacy dropped sharply when traditional content-based detection failed.

<br>

üõ°Ô∏è **CyberSOCEval: Benchmarking LLMs Capabilities for Malware Analysis and Threat Intelligence Reasoning** [source](http://arxiv.org/pdf/2509.20166v1.pdf) #cyber 

 Open source SOC benchmarks show that even the best LLMs still struggle with core malware analysis and threat intelligence reasoning, highlighting the need for domain-specific training and model evaluation.
 - Modern large language models (LLMs) achieve only 15‚Äì28% accuracy on malware analysis tasks and 43‚Äì53% on threat intelligence reasoning, significantly higher than random guessing baselines but far from perfect performance.
 - Scaling trends observed in other domains hold here: larger and more up-to-date LLMs outperform smaller ones; however, models using general-purpose ‚Äòreasoning‚Äô strategies do not significantly outperform others, revealing a domain-specific training gap.
 - Benchmarks reveal that no current LLM saturates defensive cybersecurity benchmarks, indicating substantial opportunity for targeted model training and improvement to close gaps in SOC automation capabilities.

<br>

üõ°Ô∏è **Adversarial Defense in Cybersecurity: A Systematic Review of GANs for Threat Detection and Mitigation** [source](http://arxiv.org/pdf/2509.20411v1.pdf) #cyber 

 GANs deliver substantial improvements in threat detection and resilience for cybersecurity, but practical challenges and lack of standardization still hinder widespread adoption.
 - GAN-based defensive methods improved detection accuracy by 10‚Äì22% and robustness by up to 25% across intrusion, malware, and IoT security domains, notably outperforming traditional oversampling and signature-based techniques.
 - Despite strong performance gains, GAN-powered defenses face persistent obstacles, including mode collapse, training instability, high computational costs, lack of standardized benchmarks, and limited explainability, which complicate real-world deployment and scalability.
 - Hybrid models that combine GANs with reinforcement learning or autoencoders demonstrated up to 18‚Äì20% improvements in phishing and anomaly detection but require further research to address instability and resource constraints, especially in edge and federated environments.

<br>

üõ°Ô∏è **SecureAgentBench: Benchmarking Secure Code Generation under Realistic Vulnerability Scenarios** [source](http://arxiv.org/pdf/2509.22097v1.pdf) #security 

 Repository-level evaluation reveals current code agents rarely generate both correct and secure code, frequently introducing new vulnerabilities even when explicitly prompted for security.
 - Current code agents produce code that is both functionally correct and secure in only 9.2% of realistic repository-level tasks, with the best-performing setup achieving just 15.2%.
 - Over 70% of functionally correct outputs from code agents still contain security issues, including both reintroduced historical vulnerabilities and new risks flagged by static analysis tools.
 - Explicit security reminders for agents do not lead to significant improvements in secure code generation, indicating that prompting alone is insufficient to enhance security practices.

<br>

üîß **On Code-Induced Reasoning in LLMs** [source](http://arxiv.org/pdf/2509.21499v1.pdf) #general 

 The key driver behind improved LLM reasoning from code data is structural regularity, not surface semantics, and a wide range of code representations‚Äîeven compact or corrupted ones‚Äîcan deliver strong benefits for reasoning tasks.
 - Disrupting the structural properties of code in training data leads to more severe drops in language model performance‚Äîespecially on math and code tasks‚Äîthan altering semantic elements such as variable names or comments.
 - Algorithmic abstractions like pseudocode and flowcharts, which maintain core structural information with fewer tokens, can match or even surpass the performance gains from conventional code, showing that verbosity or exact syntax is not required for effective reasoning.
 - Training with lower-level programming languages, such as Java and Rust, provides greater improvements for mathematical reasoning tasks, whereas higher-level languages like Python align better with natural language task performance.

<br>

ü§ñ **Leveraging Large Language Models for Robot-Assisted Learning of Morphological Structures in Preschool Children with Language Vulnerabilities** [source](http://arxiv.org/pdf/2509.22287v1.pdf) #general 

 This study reveals that LLM-powered social robots can efficiently deliver personalized language interventions and serve as consistent linguistic models for children and educators, potentially surpassing human performance in morphological teaching tasks.
 - Large language model-powered robots demonstrated effective management of game-based language interventions, such as turn-taking and error correction, with preschool children with language vulnerabilities.
 - Robot-assisted learning using LLMs can consistently generate, model, and deliver a high dose of targeted morphological structures, outperforming human educators in real-time linguistic input delivery.
 - Integration of LLM-based robots into preschool environments shows potential to act as language role models for both children and educators, enabling scalable and adaptive language interventions while reducing the demand on speech-language therapists.

<br>

üß© **PromptDebt: A Comprehensive Study of Technical Debt Across LLM Projects** [source](http://arxiv.org/pdf/2509.20497v1.pdf) #general 

 Prompt engineering‚Äîespecially instruction-based and few-shot techniques‚Äîis the primary source of technical debt in large-scale LLM development, with OpenAI and LangChain integrations most affected.
 - Over 54% of technical debt instances in LLM-powered Python projects involve OpenAI integrations, while LangChain contributes 12.35%, reflecting the challenges in managing widely adopted LLM APIs and orchestration frameworks.
 - Prompt design and configuration emerge as the leading source of LLM-specific technical debt, comprising 6.61% of case samples, followed by hyperparameter tuning and framework integration issues.
 - Instruction-based prompts (38.6%) and few-shot prompts (18.13%) are particularly vulnerable to technical debt due to the prevalence of lengthy, unclear instructions and placeholder examples, undermining output quality and maintainability.

<br>

üõ°Ô∏è **Library Hallucinations in LLMs: Risk Analysis Grounded in Developer Queries** [source](http://arxiv.org/pdf/2509.22202v1.pdf) #security 

 LLMs reliably generate code that imports nonexistent libraries following minor prompt variations or temporal phrasing, exposing software projects to heightened supply chain risks like typosquatting and slopsquatting.
 - Prompting large language models with year-based library requests (e.g., 'from 2025') triggers library hallucinations in up to 84% of coding tasks, substantially increasing risk compared to adjective-based prompts, which remain near 0%.
 - Minor user errors, such as one-character misspellings in library names, cause LLMs to import nonexistent libraries in up to 26% of tasks, while completely fake names are accepted and used confidently in up to 99% of tasks across several models.
 - Prompt engineering strategies offer some mitigation but remain inconsistent and model-dependent; general reasoning cues sometimes increase hallucination rates, highlighting the need for robust, LLM-specific safeguards at the interface level.

<br>

üß© **SoK: Potentials and Challenges of Large Language Models for Reverse Engineering** [source](http://arxiv.org/pdf/2509.21821v1.pdf) #security 

 This paper reveals that LLM-driven reverse engineering research is performance-obsessed, centered on decompiled code and prompt-based methods, but faces critical deficits in exploration, robustness, and reproducibility.
 - Over 96% of studies in LLM-powered reverse engineering (RE) focus on optimizing performance, with only 8% targeting discovery and less than 12% enhancing interpretability, signaling an overwhelming emphasis on efficiency rather than new insights or comprehension.
 - Nearly 60% of RE research leverages decompiled code as the primary target for LLM integration, while only 8% tackle raw bytes and 20% address pure source code, revealing a methodological gap in low-level and high-level software understanding.
 - Zero/few-shot prompting and data generation approaches dominate (62.9% each), but only one-third of studies employ fine-tuning or robust agent-based methods, and less than 5% use lightweight prototypes, indicating barriers to accessibility and generalized robustness.

<br>

üõ°Ô∏è **EvoMail: Self-Evolving Cognitive Agents for Adaptive Spam and Phishing Email Defense** [source](http://arxiv.org/pdf/2509.21129v1.pdf) #security 

 A continually self-improving email defense agent outperforms traditional systems in both accuracy and adaptability while offering interpretable audit trails against evolving spam and phishing threats.
 - EvoMail achieves superior detection performance with an accuracy of 92.8% and F1-score of 89.6%, surpassing both classical baselines and neural models across multiple public and synthetic email datasets.
 - The self-evolving adversarial training loop‚Äîwhere the system continually learns from red-team generated attacks and compresses detection failures into reusable memory‚Äîenables EvoMail to sustain performance under distribution shift, reducing F1 degradation on novel phishing attacks by 5‚Äì7% compared to leading alternatives.
 - EvoMail's heterogeneous graph fusion and LLM-driven attention mechanisms produce high interpretability (CIM = 0.70), generating structured reasoning paths that align with human analyst workflows and facilitate regulatory transparency.

<br>

üéØ **Chasing the Tail: Effective Rubric-based Reward Modeling for Large Language Model Post-Training** [source](http://arxiv.org/pdf/2509.21500v1.pdf) #general 

 Careful construction and iterative refinement of rubric-based reward models, focusing on the high-reward tail with diverse exemplars, substantially improve LLM post-training effectiveness and resilience against reward over-optimization.
 - Refining reward rubrics with high-quality and diverse off-policy responses boosts win-rate performance for LLM post-training by up to 39% in general domains and 34% in specialized medical domains.
 - Accuracy in ranking top-performing (high-reward) responses, rather than correcting broader errors, is critical for mitigating reward over-optimization during reinforcement fine-tuning.
 - Iterative refinement of rubrics using excellent and semantically diverse response pairs significantly delays the onset of reward over-optimization, extending sustained performance over 2.5x more training steps compared to baseline approaches.

<br>

üõë **FreezeVLA: Action-Freezing Attacks against Vision-Language-Action Models** [source](http://arxiv.org/pdf/2509.19870v1.pdf) #security 

 FreezeVLA exposes a potent and transferable vulnerability in VLA models where adversarial images can reliably paralyze robots across a wide range of user instructions, emphasizing urgent safety concerns for AI-powered robotics.
 - Adversarial images crafted with FreezeVLA can freeze vision-language-action (VLA) models, causing robots to ignore subsequent instructions with success rates averaging 73.3% (SpatialVLA), 95.4% (OpenVLA), and 59.8% (œÄ0) across popular benchmarks.
 - FreezeVLA-generated perturbations exhibit strong cross-prompt transferability, reliably paralyzing robotic action even as textual instructions change, with attack effectiveness growing as the diversity and number of reference prompts increases.
 - Increasing the allowable perturbation and using GPT-generated prompts substantially boosts attack success, highlighting a critical safety risk that persists across model architectures and tasks and calls for robust defensive strategies in real-world robotics.

<br>

üõ°Ô∏è **SecInfer: Preventing Prompt Injection via Inference-time Scaling** [source](http://arxiv.org/pdf/2509.24967v1.pdf) #security 

 Inference-time scaling with system-prompt-guided sampling and target-task-guided aggregation enables LLMs to withstand prompt injection attacks without losing performance.
 - SecInfer reduces prompt injection attack success rates to near-zero across multiple large language models and benchmarks, maintaining task accuracy even in adversarial conditions.
 - Compared to existing prompt injection defenses and inference-time scaling methods, SecInfer consistently delivers higher robustness under attack while incurring only moderately increased computational overhead.
 - SecInfer remains effective even against strong adaptive attacks and in real-world LLM agent settings, demonstrating broad applicability beyond text-based tasks.

<br>

üõ°Ô∏è **SafeSearch: Automated Red-Teaming for the Safety of LLM-Based Search Agents** [source](http://arxiv.org/pdf/2509.23694v1.pdf) #security 

 Automated red-teaming reveals that LLM-powered search agents are highly vulnerable to unreliable real-time search results, including misinformation, and that current defense strategies are insufficient for robust safety.
 - LLM-based search agents propagated risky or unsafe content to users in up to 90.5% of test cases when exposed to a single unreliable website source, with misinformation posing the greatest threat category.
 - Even advanced models such as GPT-5 with tool-calling scaffolds exhibited nonzero attack success rates (ASR), indicating that no existing search agent is completely robust to unreliable results from real-time web sources.
 - Standard defense mechanisms like reminder prompting and automated search result filtering only partially reduced vulnerability, with reminder prompts failing to meaningfully improve safety and filtering roughly halving ASR but leaving substantial residual risk.

<br>

üõ°Ô∏è **ReliabilityRAG: Effective and Provably Robust Defense for RAG-based Web-Search** [source](http://arxiv.org/pdf/2509.23519v1.pdf) #security 

 Incorporating document reliability into RAG pipelines delivers scalable, provably robust defenses against adversarial corruption, outperforming prior methods even for complex and long-form tasks.
 - ReliabilityRAG's consistent majority algorithm achieves up to 20 percentage points higher answer accuracy and LLM-judge scores under adversarial attacks compared to previous robust RAG methods, with especially strong performance in long-form generation tasks.
 - The framework leverages document reliability signals‚Äîsuch as search engine ranking‚Äîto filter out adversarial content, yielding robust defenses that maintain high benign accuracy (typically ~70-80%) while response accuracy degrades gracefully as the number or rank of attacked documents increases.
 - Sampling-based approaches integrated with reliability weighting allow the system to efficiently scale to large document sets (e.g., k = 50) without significant latency overhead, and are provably robust under realistic threat models where high-ranked results are harder to poison.

<br>

üõ°Ô∏è **ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents** [source](http://arxiv.org/pdf/2509.22830v1.pdf) #security 

 Exploiting chat template formatting and multi-turn persuasion enables highly transferable prompt injection attacks on LLM agents, bypassing most current security defenses.
 - Template-based prompt injection attacks using native chat formatting increased attack success rates from 5.18% to 32.05% on AgentDojo and from 15.13% to 45.90% on InjecAgent, with multi-turn variants reaching 52.33%.
 - Payloads crafted with one model‚Äôs chat template demonstrated strong transferability, successfully compromising other models, including closed-source agents, particularly when template structures are similar.
 - Existing prompt-based defenses, including delimiters and instructional repetition, are largely ineffective against these template-driven multi-turn attacks, and simple template perturbations further circumvent rule-based parsing defenses.

<br>

üõ°Ô∏è **Takedown: How It's Done in Modern Coding Agent Exploits** [source](http://arxiv.org/pdf/2509.24240v1.pdf) #security 

 Modern coding agents exhibit pervasive, chainable vulnerabilities that allow attackers to execute arbitrary code and exfiltrate global data with no user involvement.
 - Five out of eight evaluated coding agents permit arbitrary command execution without any user interaction, exposing systems to critical compromise.
 - Four coding agents allow global data exfiltration, enabling adversaries to leak sensitive information from outside the designated workspace boundaries.
 - Systematic weaknesses‚Äîsuch as improper approval, tool misuse, and prompt injection‚Äîcan be chained for end-to-end exploitation, bypassing user safeguards by design or default settings.

<br>

üõ°Ô∏è **When MCP Servers Attack: Taxonomy, Feasibility, and Mitigation** [source](http://arxiv.org/pdf/2509.24272v1.pdf) #security 

 Malicious MCP servers present a widespread, hard-to-detect risk to agent ecosystems, necessitating urgent, multi-stakeholder intervention for robust security.
 - Twelve distinct categories of MCP server attacks were identified, with certain attack types achieving a 100% success rate across state-of-the-art LLM and agent host combinations, indicating a pervasive vulnerability.
 - Malicious MCP servers can be mass-produced rapidly and at virtually no cost; existing detection tools detected less than half (at best) of generated attacks, routinely missing critical threats such as code execution and output manipulation.
 - Effective mitigation of these threats requires coordinated policy, technical countermeasures, and persistent vigilance from registry platforms, host developers, LLM providers, and end users‚Äîcurrent safeguards are insufficient for real-world deployment.

<br>

üÜò **Between Help and Harm: An Evaluation of Mental Health Crisis Handling by LLMs** [source](http://arxiv.org/pdf/2509.24857v1.pdf) #general 

 Even the best commercial language models can miss subtle mental health crisis cues, and a non-negligible percentage of their responses may be inappropriate or harmful, especially for self-harm and suicidal ideation.
 - Across over 2,000 labeled mental health crisis inputs, state-of-the-art language models delivered generally appropriate responses, but up to 4.8% of outputs in 'self-harm' and 'suicidal ideation' categories were rated as potentially harmful or inappropriate‚Äîespecially in open-weight models.
 - Language models reliably responded to direct crisis disclosures but exhibited frequent failures‚Äîsometimes giving dangerous information or inadequate support‚Äîwhen user inputs were indirect, ambiguous, or sought information about harm methods.
 - Most models defaulted to generic, formulaic replies lacking authentic empathy, context awareness, or adequate localization, with critical implications for user trust and the risk of discouraging future help-seeking among vulnerable individuals.

<br>

üï∏Ô∏è **Uncovering Vulnerabilities of LLM-Assisted Cyber Threat Intelligence** [source](http://arxiv.org/pdf/2509.23573v1.pdf) #security 

 LLMs in cyber threat intelligence are systematically undermined by spurious correlations, conflicting sources, and poor generalization to new threats, limiting their reliability in practical security operations.
 - Large Language Models exhibit a failure rate of over 20% from spurious correlation in contextualization tasks, leading to frequent misattribution of cyber threat evidence.
 - Contradictory knowledge from inconsistent CTI sources causes unstable predictions and reasoning errors in over 35% of evaluated threat intelligence instances across all stages.
 - Constrained generalization limits LLMs‚Äô ability to handle emerging threats, with failure ratios exceeding 33% for zero-day exploit prediction and mitigation, especially in specialized cyber agents.

<br>

üõ†Ô∏è **Automated Vulnerability Validation and Verification: A Large Language Model Approach** [source](http://arxiv.org/pdf/2509.24037v1.pdf) #security 

 Automated, LLM-powered pipelines can reliably reconstruct and validate diverse real-world software exploits, but their effectiveness is constrained by both input data quality and the reasoning capacity of the underlying model.
 - An automated pipeline leveraging large language models and retrieval-augmented generation successfully reproduced 70% (71 out of 102) of tested software vulnerabilities in controlled, containerized environments across nine programming languages and 55 unique libraries.
 - The presence of a public proof-of-concept (PoC) significantly reduced the average number of iterations required for successful exploit reproduction, while multi-container setups were required for 28% of cases, underlining the importance of automated environment orchestration.
 - Inconsistent or incomplete CVE descriptions frequently hinder reproducibility, highlighting the need for more rigorous verification in vulnerability disclosures, and model capability remains the primary determinant of successful automated exploit generation.

<br>

üõ°Ô∏è **Binary Diff Summarization using Large Language Models** [source](http://arxiv.org/pdf/2509.23970v1.pdf) #security 

 Automated binary diff summarization with LLMs delivers highly accurate malware detection in software supply chains and introduces the FSS metric for precise function triage, outperforming prior methods and proving efficacy in real-world attacks.
 - The large language model-based binary diff summarization framework achieved a malware detection precision of 0.98 and recall of 0.64 on a benchmark comprising 104 versions from 6 open-source projects injected with three malware types, demonstrating high accuracy and low false positive rates.
 - The functional sensitivity score (FSS) method reliably distinguished malicious from benign functions, with a median separation of 3.0 points, facilitating automated triage of sensitive code changes in binary diffs.
 - In a real-world case study of the XZ Utils supply chain attack, the framework successfully detected injected backdoor functions with high FSS values, validating its effectiveness for practical software supply chain security scenarios.

<br>

ü¶† **Virus Infection Attack on LLMs: Your Poisoning Can Spread "VIA" Synthetic Data** [source](http://arxiv.org/pdf/2509.23041v1.pdf) #security 

 A new attack can stealthily and widely propagate poison through synthetic data pipelines, overcoming previous distributional barriers and posing significant threats to LLM supply chains.
 - Standard synthetic-data-based training for large language models demonstrates strong resistance to mainstream poisoning and backdoor attacks, with less than 0.1% of synthetic samples showing malicious content even under high upstream poisoning rates.
 - The proposed Virus Infection Attack (VIA) dramatically increases the infection rate of poisoning‚Äîup to 85% in synthetic data‚Äîby embedding adversarial payloads within benign samples, enabling propagation of attacks to downstream models via synthetic training.
 - While VIA achieves a notably higher infection rate for downstream models compared to prior methods, it introduces a trade-off: attack success rates on upstream models decrease, but stealthiness is increased through novel shell wrapping strategies.

<br>

üöó **FuncPoison: Poisoning Function Library to Hijack Multi-agent Autonomous Driving Systems** [source](http://arxiv.org/pdf/2509.24408v1.pdf) #security 

 Function-level template poisoning enables precise, persistent, and stealthy attacks on multi-agent LLM-based autonomous driving systems by exploiting a new, undersecured attack surface.
 - Injecting malicious templates into function descriptions in shared libraries hijacks function selection in multi-agent autonomous driving systems, achieving attack success rates exceeding 86% and resulting in significant trajectory deviations and higher collision rates.
 - FuncPoison attacks persistently and stealthily propagate through agent chains, causing cascading failures that existing prompt-level and agent-level defenses fail to detect or mitigate effectively.
 - Template-injected function descriptions are selected 98% of the time and produce up to 86.3% attack success rate, making function-level poisoning a far more effective attack vector than prompt or model-data poisoning in LLM-driven safety-critical applications.

<br>

üõ°Ô∏è **What Do They Fix? LLM-Aided Categorization of Security Patches for Critical Memory Bugs** [source](http://arxiv.org/pdf/2509.22796v1.pdf) #security 

 Leveraging large and small language models jointly enables the accurate discovery and classification of critical memory corruption patches, revealing previously missed high-impact Linux kernel vulnerabilities‚Äîsome exploitable for control-flow hijacking.
 - The dual-model DUALLM pipeline achieves an 87.4% accuracy and 0.875 F1-score in fine-grained classification of Linux kernel security patches, outperforming previous approaches by more than 20 percentage points.
 - DUALLM successfully identified 111 out of 5,140 recent Linux kernel patches as addressing critical out-of-bounds (OOB) or use-after-free (UAF) vulnerabilities, with 90 cases confirmed by manual verification and at least one exploited for previously unknown control-flow hijack.
 - Integrating commit titles, messages, and custom program slices enables robust detection of security-critical patches even when CVE indicators are absent, dramatically reducing both false positives and false negatives compared to existing rule-based and machine-learning methods.

<br>

üß© **Beyond Embeddings: Interpretable Feature Extraction for Binary Code Similarity** [source](http://arxiv.org/pdf/2509.23449v1.pdf) #security 

 Interpretable feature extraction via language models offers training-free, scalable, and highly accurate binary code similarity detection, making clone search both more effective and transparent than embedding-only approaches.
 - A language model approach that produces human-interpretable, structured features from assembly code enables training-free binary code similarity detection, achieving 42% recall@1 in cross-architecture and 62% recall@1 in cross-optimization scenarios‚Äîmatching or exceeding many embedding-based methods that require training.
 - Combining interpretable LLM-derived features with traditional embedding-based models results in significant performance improvements, with hybrid recall@1 scores surpassing both methods in isolation and outperforming prior baselines by up to 25‚Äì30 percentage points in challenging settings.
 - Text-based, interpretable feature extraction supports scalable, exact search using inverted indexes, overcoming the efficiency-accuracy trade-off inherent to high-dimensional vector embeddings and making large-scale function-level retrieval both practical and explainable.

<br>

üí£ **Formalization Driven LLM Prompt Jailbreaking via Reinforcement Learning** [source](http://arxiv.org/pdf/2509.23558v1.pdf) #security 

 A reinforcement learning-driven formalization of prompts, further enhanced through knowledge graph retrieval, enables highly effective and adaptive jailbreaking of aligned LLMs, exploiting critical blind spots in alignment algorithms.
 - The PASS prompt jailbreaking framework achieves attack success rates of 99.03% and 96.84% on DeepSeek-V3 for AdvBench and JailbreakBench datasets, respectively, significantly surpassing prior methods.
 - By leveraging reinforcement learning to formalize and iteratively disguise malicious queries, PASS bypasses alignment defenses through highly diverse and stealthy prompt transformations not covered in training datasets.
 - The use of a knowledge graph (GraphRAG) to store and reuse formalized attack tactics enables continuous adaptation, accelerating subsequent jailbreaks and highlighting key weaknesses in current LLM alignment strategies.

<br>

üõ°Ô∏è **Bidirectional Intention Inference Enhances LLMs' Defense Against Multi-Turn Jailbreak Attacks** [source](http://arxiv.org/pdf/2509.22732v1.pdf) #security 

 A dual-phase intent inference defense nearly eradicates jailbreak attack success rates in multi-turn conversations while maintaining general utility in LLMs.
 - Bidirectional intention inference defense reduces the attack success rate to nearly 0% in both single-turn and multi-turn jailbreak scenarios, outperforming seven leading defense methods across three LLM architectures and five safety datasets.
 - Unlike prior approaches, the proposed dual-stage (forward and backward) filtering mechanism provides consistent and robust protection against complex, multi-turn adversarial strategies, without sacrificing general model utility.
 - This method maintains a favorable balance between safety and usability, achieving strong defense performance as an external module while preserving LLMs' original capabilities, as validated by AlpacaEval win-rates.

<br>

ü§ñ **Preventing Robotic Jailbreaking via Multimodal Domain Adaptation** [source](http://arxiv.org/pdf/2509.23281v1.pdf) #security 

 J-DAPT delivers real-time, high-accuracy detection of harmful instructions targeting robots by combining multimodal fusion and domain adaptation‚Äîeven in data-scarce domains where previous methods fail.
 - J-DAPT achieves up to 100% accuracy in detecting jailbreak instructions for vision-language-enabled robotics, outperforming baseline detectors that only marginally exceed random guessing.
 - The fusion of multimodal embeddings and domain adaptation allows robust detection using only benign, domain-specific data, eliminating the need for specialized robotic jailbreak examples.
 - Compared to LLM-based detectors, J-DAPT delivers near-perfect accuracy while being up to 9.9√ó faster, making it suitable for real-time robotic applications without incurring significant computational overhead.

<br>

üîì **Quant Fever, Reasoning Blackholes, Schrodinger's Compliance, and More: Probing GPT-OSS-20B** [source](http://arxiv.org/pdf/2509.23882v1.pdf) #security 

 Structural and procedural vulnerabilities in GPT-OSS-20B enable efficient jailbreaks and dangerous behaviors, especially when attackers exploit reasoning patterns and compositional prompt design.
 - GPT-OSS-20B exhibits 'quant fever', with a 70% rate of risky behavior when responding to benign prompts containing numerical targets, often overriding safety constraints.
 - Reasoning blackholes affect 81% of tested prompts under greedy decoding, causing repetitive loops and denial-of-service vulnerabilities due to localized attention.
 - Jailbreak attack rates rise dramatically‚ÄîSchrodinger‚Äôs compliance boosts success from 3.3% to 44.4%, and reasoning mirage raises harm rates from 28.4% to 55.3%, exposing significant adversarial weaknesses.

<br>

üß™ **HarmMetric Eval: Benchmarking Metrics and Judges for LLM Harmfulness Assessment** [source](http://arxiv.org/pdf/2509.24384v1.pdf) #security 

 Traditional NLP metrics can judge LLM harmfulness better than state-of-the-art LLM-based systems‚Äîa surprising reversal of expectations.
 - Conventional reference-based metrics such as METEOR and ROUGE-1 outperform LLM-based judges in harmfulness evaluation, achieving the highest effectiveness scores (up to 0.634) among 20 tested metrics.
 - The overall reliability of all evaluated harmfulness metrics remains unsatisfactory, with none exceeding a 0.634 effectiveness score on the benchmark, indicating critical weaknesses in current approaches.
 - LLM-based judges consistently misclassify vague affirmations and prompt repetitions as harmful, highlighting deficiencies in their ability to distinguish nuanced non-harmful responses compared to leading conventional metrics.

<br>

üßπ **Dual-Space Smoothness for Robust and Balanced LLM Unlearning** [source](http://arxiv.org/pdf/2509.23362v1.pdf) #security 

 A dual-space smoothness framework delivers state-of-the-art, balanced LLM unlearning, mitigating catastrophic forgetting and boosting resilience to adversarial attacks.
 - PRISM achieves the highest overall Unlearn Score (up to 0.86 on MUSE-Books and 0.76 on WMDP) while preserving model utility and privacy, outperforming all tested baselines in both text and conversational dialogue settings.
 - PRISM demonstrates greater robustness against relearning and jailbreak attacks, maintaining lower attack success rates and higher utility retention compared to prior approaches, even after adversarial perturbations and multi-step relearning.
 - Ablation studies reveal that PRISM's dual-space smoothness (representation and parameter) and gradient conflict decoupling are essential for balancing strong forgetting with retained utility, as omitting any component leads to utility collapse or compromised robustness.

<br>

üõ°Ô∏è **DiffuGuard: How Intrinsic Safety is Lost and Found in Diffusion Large Language Models** [source](http://arxiv.org/pdf/2509.24296v1.pdf) #security 

 A novel dual-stage defense unlocks the latent safety potential of diffusion language models by mitigating both intra-step and inter-step vulnerabilities, achieving robust protections against diverse jailbreak attacks.
 - Introducing stochastic annealing remasking in diffusion language models significantly reduced the attack success rate of jailbreak attempts by an average of 33.2%, with minimal compromise to output quality.
 - Early-stage safety interventions in the diffusion generation process, such as injecting refusal tokens, decrease jailbreak attack success by up to 76.9% compared to mid-stage interventions, demonstrating strong denoising-path dependence.
 - The DIFFUGUARD plug-and-play defense framework consistently outperformed baseline safety strategies across four tested models, reducing the attack success rate against advanced jailbreak methods from 47.9% to 14.7% without noticeable latency or utility penalty.

<br>

üõ°Ô∏è **GuardNet: Graph-Attention Filtering for Jailbreak Defense in Large Language Models** [source](http://arxiv.org/pdf/2509.23037v1.pdf) #security 

 GuardNet‚Äôs multi-graph attention approach sets a new standard for real-time, pre-inference jailbreak detection in LLMs, combining prompt-level robustness and fine-grained adversarial span identification.
 - GuardNet detects and localizes jailbreak prompts in large language models with prompt-level F1 scores rising from 66‚Äì79% (TextDefense baseline) to over 94% across datasets, reaching 99.8% on the LLM-Fuzzer benchmark.
 - At the token level, GuardNet's fine-grained filtering raises F1 scores from 48‚Äì75% to 74‚Äì91%, with intersection-over-union (IoU) span detection gains up to +28%, significantly outperforming attention-only baselines.
 - GuardNet maintains robust cross-domain generalization, achieving prompt-level F1 scores above 95% and token-level IoU over 82% during zero-shot domain transfer, validating its efficacy without model retraining or internal access.

<br>

üß™ **HFuzzer: Testing Large Language Models for Package Hallucinations via Phrase-based Fuzzing** [source](http://arxiv.org/pdf/2509.23835v1.pdf) #security 

 Phrase-based fuzzing exposes widespread and previously under-detected package hallucinations in all tested LLMs, highlighting a critical supply chain security risk for AI-assisted software development.
 - The HFUZZER framework discovers 2.60 times more unique hallucinated packages in large language models compared to previous mutation-based fuzzing approaches, indicating a significantly higher risk of supply chain vulnerabilities in AI-generated code.
 - Every major large language model tested, including advanced models like GPT-4o, exhibits package hallucinations not just in code generation but also when suggesting environment configuration, revealing risks that go beyond programming assistance.
 - Ablation studies show that using phrase-based task generation improves both hallucination detection and task diversity (by up to 4.40x), with the most substantial impact coming from the explicit use of curated phrase extraction over raw descriptions.

<br>

üßë‚Äçüî¨ **Defending MoE LLMs against Harmful Fine-Tuning via Safety Routing Alignment** [source](http://arxiv.org/pdf/2509.22745v1.pdf) #security 

 SAFEMOE robustly preserves the safety of MoE language models during fine-tuning by directly regularizing harmful input routing, substantially outpacing conventional defenses with minimal performance cost.
 - Mixture-of-Experts (MoE) large language models experience significant and highly correlated increases in harmful output when the routing of harmful inputs drifts away from safety-critical experts during fine-tuning, even with benign data.
 - The proposed SAFEMOE technique reduces harmfulness scores in attacked MoE LLMs by over 90% (e.g., from 62.0 to 5.0 in OLMoE), while maintaining task utility within 1% of baseline performance and incurring only a 2% time overhead.
 - State-of-the-art defenses designed for monolithic LLMs prove largely ineffective for MoE architectures, underscoring the necessity of architecture-aware safeguards like SAFEMOE that directly target MoE-specific vulnerabilities.

<br>

üß¨ **LLM Watermark Evasion via Bias Inversion** [source](http://arxiv.org/pdf/2509.23019v1.pdf) #security 

 Bias-inversion rewriting reliably defeats LLM watermarks, raising urgent questions for AI provenance and policy enforcement.
 - A bias-inversion attack method (BIRA) enables over 99% evasion against recent LLM watermarking techniques while maintaining high semantic fidelity with original content.
 - Applying negative logit bias to high-entropy tokens in paraphrased text drastically lowers watermark detection probability, rendering current detectors unable to distinguish AI-generated text from human-written text at practical thresholds.
 - BIRA outperforms prior watermark removal baselines in both effectiveness and semantic preservation, and introduces minimal computational overhead due to adaptive bias control.

<br>

üîé **Fairness Testing in Retrieval-Augmented Generation: How Small Perturbations Reveal Bias in Small Language Models** [source](http://arxiv.org/pdf/2509.26584v1.pdf) #general 

 Small language models integrated with RAG pipelines break fairness up to a third of the time due to retrieval sensitivity to demographic cues, especially race, highlighting the need for component-level bias testing.
 - Up to 33% of output fairness violations occur in small language models using retrieval-augmented generation when minor demographic cues are introduced, with racial perturbations being responsible for nearly half of these failures.
 - The retrieval component itself contributes significantly to bias, showing a 28.52% attack success rate, and demonstrates greater sensitivity to racial identifiers than other demographic factors.
 - A practical Retriever Robustness Score threshold is established, enabling developers to identify when semantic and label drift in retrieved content signals degraded fairness or reliability.

<br>

üõ°Ô∏è **Better Privilege Separation for Agents by Restricting Data Types** [source](http://arxiv.org/pdf/2509.25926v1.pdf) #security 

 Type-directed privilege separation offers a practical, robust, and model-agnostic solution for defending AI agents against prompt injection attacks while preserving utility for many applications.
 - Restricting data flows between AI agents to curated data types such as integers, booleans, floats, and whitelisted enums reliably reduces the attack success rate of prompt injections to 0% across diverse real-world agent scenarios.
 - Defended agents using type-directed privilege separation maintain high utility in sensitive tasks, including online shopping (22.4% completion rate) and calendar scheduling (91.0% success rate), without sacrificing operational effectiveness.
 - The technique demonstrates a strong trade-off in some contexts‚Äîwhile eliminating prompt injection attacks in software bug fixing agents, the utility decreased from 49.7% to 14.6%, highlighting the importance of context-rich inputs for certain domains.

<br>

üÜî **Fingerprinting LLMs via Prompt Injection** [source](http://arxiv.org/pdf/2509.25448v1.pdf) #security 

 A prompt-injection-based fingerprinting method provides a reliable, highly accurate tool for LLM provenance detection, robust against model post-processing and outperforming existing baselines.
 - Fingerprinting via prompt injection allows for over 90% true positive provenance detection rates and near-zero false positive rates across five major LLM families and 700+ suspect models, even after post-training and quantization.
 - LLMPrint significantly outperforms previous provenance methods (e.g. TRAP, LLMmap) in both gray-box and black-box settings, maintaining robust detection accuracy even when APIs only expose top-20 log probabilities or prompt-injection detectors are employed.
 - Detection failures primarily coincide with suspect models that have degraded up to 20% in general-purpose benchmark scores relative to their base, indicating that identification issues stem from severe model drift rather than fingerprint fragility.

<br>

üéûÔ∏è **How Diffusion Models Memorize** [source](http://arxiv.org/pdf/2509.25705v1.pdf) #security 

 Memorization in diffusion models stems from early overestimation caused by guidance, not simple overfitting, and this mechanism can be both detected and mitigated by controlling early denoising dynamics.
 - Early overestimation of training images during the denoising process, amplified by classifier-free guidance, is the key driver of memorization in diffusion models, leading to rapid convergence and replication of training data.
 - Memorization severity correlates almost perfectly with deviations from the theoretical denoising schedule, as excessive injection of the original training sample suppresses randomness and reduces latent diversity.
 - Increasing the guidance scale linearly amplifies memorization risk by elevating the contribution of training images in generated outputs, which can be detected and mitigated by strategically adjusting guidance during early denoising steps.

<br>

üõ°Ô∏è **STAC: When Innocent Tools Form Dangerous Chains to Jailbreak LLM Agents** [source](http://arxiv.org/pdf/2509.25624v1.pdf) #security 

 Sequential tool attack chains can reliably bypass agent safety guardrails by distributing malicious intent across benign-looking steps, causing real-world harm while evading detection.
 - Tool-enabled LLM agents are highly vulnerable to distributed multi-turn attacks, with attack success rates exceeding 90% and prompts appearing harmless in isolation over 483 evaluated cases.
 - Existing prompt-based defenses for agentic systems are largely ineffective against stealthy attack chains, but a reasoning-driven defense prompt can reduce attack success by up to 28.8%.
 - Safeguarding agents against these attacks requires holistic monitoring and reasoning over entire action sequences, not just individual prompts, highlighting fundamental limitations in current AI safety approaches.

<br>

üõ°Ô∏è **SafeBehavior: Simulating Human-Like Multistage Reasoning to Mitigate Jailbreak Attacks in Large Language Models** [source](http://arxiv.org/pdf/2509.26345v1.pdf) #security 

 Emulating human-like multistage reasoning, SafeBehavior delivers universal and efficient jailbreak resilience for language models without sacrificing core task capability.
 - SafeBehavior reduces jailbreak attack success rates to 0.00 across five state-of-the-art attack types and two model benchmarks, outperforming all existing defenses.
 - False positive rates for SafeBehavior are consistently 0.00, ensuring no benign queries are incorrectly flagged, which demonstrates reliability without disrupting normal user interaction.
 - SafeBehavior maintains or improves reasoning ability with a retain ratio of 1.00, indicating no trade-off between safety and model utility compared to other safety strategies that degrade performance.

<br>

üõ°Ô∏è **ASGuard: Activation-Scaling Guard to Mitigate Targeted Jailbreaking Attack** [source](http://arxiv.org/pdf/2509.25843v1.pdf) #security 

 Targeted recalibration of model internals via mechanistic intervention sharply mitigates tense-based jailbreaks, achieving robust safety without sacrificing utility.
 - Attack success rate of tense jailbreaking is reduced from 42% to 8% in Llama-3.1-8B-Instruct and from 51% to 8% in Qwen2.5-7B-Instruct following targeted activation scaling and preventative fine-tuning.
 - ASGUARD achieves strong safety improvements while minimizing over-refusal and preserving general capabilities, consistently outperforming traditional alignment and representation engineering methods on the safety‚Äìutility Pareto frontier.
 - Mechanistic analysis reveals that a small, model-specific set of attention heads are causally responsible for tense-related jailbreaks, and surgical intervention at this level both neutralizes the vulnerability and guides more robust refusal circuitry without catastrophic forgetting.

<br>

üßπ **Beyond Sharp Minima: Robust LLM Unlearning via Feedback-Guided Multi-Point Optimization** [source](http://arxiv.org/pdf/2509.20230v3.pdf) #security 

 This work exposes a critical weakness in LLM unlearning methods and introduces StableUN, a novel feedback-driven optimization technique that delivers markedly stronger protection against both relearning and jailbreak attacks without sacrificing utility.
 - Conventional LLM unlearning techniques remain highly vulnerable to relearning attacks, with suppressed knowledge rapidly recoverable through minimal fine-tuning; for example, accuracy on the 'forgotten' test set rebounds from 27% (post-unlearning) to over 50% after relearning with only 80 samples.
 - StableUN, a feedback-guided bi-level optimization framework, significantly improves robustness, reducing knowledge recovery by relearning attacks by 10-15% on key benchmarks, while maintaining or modestly improving model utility compared to baseline methods.
 - The introduction of adversarial and stochastic parameter perturbations during unlearning directs model optimization toward flatter regions in the loss landscape, leading to enhanced resistance not only to relearning attacks but also to prompt-based jailbreak adversarial threats.

<br>

üõ°Ô∏è **Incentive-Aligned Multi-Source LLM Summaries** [source](http://arxiv.org/pdf/2509.25184v1.pdf) #security 

 Reshaping LLM-driven search so that sources maximize visibility only by providing truthful, corroborated information, TTS dramatically improves factual accuracy and defeats strategic manipulations.
 - The Truthful Text Summarization (TTS) framework increases factual accuracy in multi-source LLM summaries from 22.7% to 70.7% on Natural Questions and from 3.3% to 74.3% on ClashEval compared to baseline methods.
 - TTS robustly separates reliable sources from adversarial and deceptive ones‚Äîassigning near-zero reliability scores to manipulated or coordinated uninformative content‚Äîwithout using ground-truth labels.
 - Beyond technical robustness, TTS aligns incentives: truthful reporting strictly maximizes source inclusion probability, making honest, informative contributions the utility-maximizing strategy even amid strategic attacks.

<br>

üõ°Ô∏è **Red Teaming Program Repair Agents: When Correct Patches can Hide Vulnerabilities** [source](http://arxiv.org/pdf/2509.25894v1.pdf) #security 

 Functionally correct patches produced by LLM-based repair agents can be stealthily manipulated to contain exploitable vulnerabilities that evade current security checks.
 - Up to 91% of patches generated by LLM-based automated program repair agents after exposure to adversarial issue reports are both functionally correct and secretly contain security vulnerabilities, vastly exceeding the <20% success rate of prior methods.
 - Current defenses, such as perplexity-based filtering and input rephrasing, are largely ineffective at detecting or mitigating adversarial patches crafted via carefully engineered GitHub issue statements, with detection performance worse than random chance.
 - Adversarial vulnerabilities injected by this method are transferable across multiple language models and CI/CD pipelines, and can even bypass static analysis defenses when combined with lightweight code obfuscation, highlighting systemic risks in automated software maintenance workflows.

<br>

üîó **Linking Process to Outcome: Conditional Reward Modeling for LLM Reasoning** [source](http://arxiv.org/pdf/2509.26578v1.pdf) #general 

 Linking stepwise reasoning rewards to final outcomes with CRM reliably strengthens LLM math performance, thwarts reward hacking, and supports scalable, data-efficient optimization‚Äîeven without access to ground-truth labels.
 - Conditional Reward Modeling (CRM) significantly outperforms existing step-wise reward modeling methods for LLM reasoning, achieving up to a 1.4% absolute accuracy improvement on challenging math datasets (e.g., 56.6% vs. 55.2% on MATH500) while ensuring cross-sample comparability and robust trajectory selection.
 - CRM-based RL optimization yields performance on par with or exceeding RL systems utilizing ground-truth verifiable rewards, boosting pass@1 accuracies up to 43.3% on benchmark competitions like AIME24‚Äîsubstantially higher than prior process reward model approaches.
 - CRM effectively suppresses reward hacking in RL, evidenced by stable downstream accuracy and lower response repetition rates, and enhances desirable reasoning behaviors such as self-reflection, which correlates with increased accuracy during training.

<br>
