üõ°Ô∏è **Simple Prompt Injection Attacks Can Leak Personal Data Observed by LLM
  Agents During Task Execution** [source](http://arxiv.org/pdf/2506.01055v1.pdf) security 

 Even with safety tuning, LLM agents in banking scenarios can still leak personal data when prompted cleverly‚Äîand current defenses often trade off security for performance.
 - Prompt injection attacks on LLM-powered banking agents resulted in a 15‚Äì50% drop in task utility and achieved around 20% attack success rates, with vulnerability varying by model and task type.
 - Most models, due to safety alignments, resisted leaking highly sensitive data such as passwords when requested alone, but were more susceptible‚Äîup to 19% password leakage‚Äîwhen a password was requested alongside one or two other personal details.
 - No built-in defense fully eliminated data leakage across a broad set of 48 tasks, but specific strategies‚Äîsuch as prompt injection detection and tool filtering‚Äîreduced attack success rates to near zero, often at the cost of substantial utility loss.

<br>

üõ°Ô∏è **Adversarial Threat Vectors and Risk Mitigation for Retrieval-Augmented
  Generation Systems** [source](http://arxiv.org/pdf/2506.00281v1.pdf) security 

 A multi-layered, prioritized defense strategy greatly reduces major attack risks in RAG systems, but insider threats remain a challenging front for system security.
 - Enterprise adoption of Retrieval-Augmented Generation (RAG) systems surpassed 50% in 2024, yet key vulnerabilities including prompt injection, data poisoning, and adversarial query manipulation substantially increase the risk of sensitive information disclosure and model compromise.
 - Implementation of layered controls‚Äîsuch as rigorous input validation, adversarial training, real-time monitoring, and strong data governance‚Äîreduced overall risk severity scores from high to low, with sensitive information disclosure risk dropping from 19.5 to 10.41 and poisoning risk from 19.88 to 6.94.
 - Insider threats were found to accelerate the exploitation process compared to external attacks, underscoring the need for internal governance, continuous validation, and monitoring, as external perimeter defenses alone are insufficient to protect RAG pipelines.

<br>

üõ°Ô∏è **VPI-Bench: Visual Prompt Injection Attacks for Computer-Use Agents** [source](http://arxiv.org/pdf/2506.02456v1.pdf) security 

 AI agents with system or browser access are highly vulnerable to visual prompt injection attacks, with existing defenses frequently failing across realistic use cases.
 - Browser-Use Agents executed malicious visual prompt injections with attempted and success rates often reaching or exceeding 100% and 84% respectively on major platforms, exposing a critical vulnerability to visual-based attacks.
 - Computer-Use Agents, even when equipped with specialized defenses, demonstrated high attack success rates (up to 51%) in dynamic environments such as email and messaging, particularly for multi-step or multi-intent tasks.
 - System-level defense prompts offered inconsistent and generally limited protection against visual prompt injection; attack success rates remained high regardless of injection timing or defense configuration, underscoring the urgent need for robust context-aware security mechanisms.

<br>

üß© **TracLLM: A Generic Framework for Attributing Long Context LLMs** [source](http://arxiv.org/pdf/2506.04202v1.pdf) security 

 TracLLM offers precise and efficient traceback for long-context LLM outputs, even in adversarial or noisy environments.
 - TracLLM identifies 94‚Äì98% of malicious texts responsible for prompt injection and knowledge corruption attacks in long-context LLMs, significantly reducing attack success rates after their removal.
 - Compared to established baselines like Shapley, LIME, STC, and citation-based methods, TracLLM achieves equal or higher precision and recall with up to 4‚Äì18x greater computational efficiency on large contexts.
 - TracLLM remains robust against diverse attacks and adaptive manipulation, not only outperforming competing approaches in practical scenarios but also providing theoretical guarantees for correctly attributing outputs to responsible context texts.

<br>

üó∫Ô∏è **ATAG: AI-Agent Application Threat Assessment with Attack Graphs** [source](http://arxiv.org/pdf/2506.02859v1.pdf) security 

 A novel, semi-automated framework models and maps how attacks propagate in interconnected AI agents, revealing how small vulnerabilities can cascade into severe system compromises.
 - Chaining minor vulnerabilities in multi-agent AI systems‚Äîsuch as missing input sanitization‚Äîenables attackers to achieve complex, multi-step attacks that can result in serious outcomes like misinformation or sensitive data exfiltration.
 - Agents with direct access to external tools or services (e.g., email APIs or external websites) significantly widen the attack surface, as their legitimate functionalities can be repurposed for malicious activities when exploited.
 - The ATAG framework, leveraging logical attack graphs and a new LLM vulnerability database, enables proactive identification, visualization, and prioritization of attack paths in multi-agent AI deployments, supporting actionable, continuous threat assessment.

<br>

üï∏Ô∏è **SentinelAgent: Graph-based Anomaly Detection in Multi-Agent Systems** [source](http://arxiv.org/pdf/2505.24201v1.pdf) security 

 Graph-based, runtime oversight yields actionable and explainable detection of complex and covert failures in multi-agent LLM systems, outperforming conventional guardrails.
 - A graph-based anomaly detection framework using dynamic execution graphs enables the identification of both local and systemic failures‚Äîincluding prompt injection, tool misuse, and multi-agent collusion‚Äîin large language model-based multi-agent systems (MAS).
 - The SentinelAgent oversight module achieves real-time, explainable diagnostics and actionability across diverse MAS topologies, successfully detecting covert attacks and attributing root causes without modifying internal agent logic.
 - Case studies show the approach detects sophisticated, covert risks in practical deployments, such as automated email assistants and general-purpose agent systems, which conventional input/output guardrails fail to capture, highlighting the necessity of structural and semantic analysis in secure MAS deployment.

<br>

üõ°Ô∏è **ReGA: Representation-Guided Abstraction for Model-based Safeguarding of
  LLMs** [source](http://arxiv.org/pdf/2506.01770v1.pdf) security 

 ReGA introduces a scalable and interpretable safeguard that leverages internal representations to robustly detect unsafe prompts and outputs in LLMs, outperforming previous model-based approaches.
 - The ReGA framework accurately distinguishes safe from harmful inputs in large language models, achieving an average AUROC of 0.975 at the prompt level and 0.985 at the conversation level across multiple LLMs.
 - ReGA demonstrates strong generalizability and robustness by detecting up to 87% of harmful inputs‚Äîeven those generated by advanced jailbreaking attacks‚Äîwhile maintaining a false negative rate under 1% for benign queries.
 - Compared to existing detection-based safeguards, ReGA outperforms in effectiveness, scalability, and interpretability, requiring significantly fewer abstract states while adding negligible computational overhead during inference.

<br>

üï≥Ô∏è **BitBypass: A New Direction in Jailbreaking Aligned Large Language Models
  with Bitstream Camouflage** [source](http://arxiv.org/pdf/2506.02479v1.pdf) security 

 Encoding sensitive prompt elements as bitstreams enables adversaries to reliably circumvent LLM safety alignment, even outperforming previous jailbreak techniques and bypassing leading guard models.
 - BitBypass, a bitstream camouflage attack, achieved a jailbreak success rate of 46% to 78% (across all tested LLMs) compared to 0%‚Äì32% using direct harmful prompts, demonstrating that state-of-the-art alignment techniques can be bypassed with high efficiency.
 - BitBypass outperformed existing jailbreak methods‚Äîincluding AutoDAN, Base64, DeepInception, and DRA‚Äîby reducing refusal rates to as low as 0% and achieving up to 70% attack success in some models on challenging benchmarks like AdvBench and Behaviors.
 - The camouflage approach also bypassed traditional safety guard models (e.g., OpenAI Moderation, Llama Guard series, ShieldGemma), raising the bypass rate from a maximum of 18% (direct prompts) up to 93%, and even tricked the most robust models (like Claude 3.5) into generating accurate phishing content with rates from 68% to 92%.

<br>

üõ°Ô∏è **Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity
  Analysis Between Alignment and Fine-tuning Datasets** [source](http://arxiv.org/pdf/2506.05346v1.pdf) security 

 The paper reveals that fine-tuned LLMs are far more vulnerable to jailbreak attacks when their safety alignment data is too similar to subsequent fine-tuning tasks, suggesting proactive similarity monitoring is essential for lasting model safety.
 - High representational similarity between upstream safety alignment datasets and downstream fine-tuning data increases the likelihood of safety guardrail collapse in language models, with harmfulness scores rising by up to 10.33%.
 - Selecting low-similarity alignment data relative to downstream tasks significantly enhances robustness against jailbreaks, outperforming both randomly selected and high-similarity alignment datasets.
 - Combining upstream similarity-aware alignment selection with existing downstream defenses yields additive improvements in safety, highlighting the importance of privacy and dataset engineering in safeguarding AI systems post fine-tuning.

<br>

üîì **Align is not Enough: Multimodal Universal Jailbreak Attack against
  Multimodal Large Language Models** [source](http://arxiv.org/pdf/2506.01307v1.pdf) security 

 When image and text adversarial manipulations are combined, current safety alignments in multimodal AI models can be reliably bypassed, exposing substantial new attack vectors unique to multimodal interactions.
 - Simultaneous manipulation of both image and text modalities in multimodal large language models (MLLMs) achieves up to 80%-90% attack success rates, significantly surpassing single-modality attacks in both white-box and black-box settings.
 - The proposed iterative multimodal jailbreak method leads to a 15-20% increase in the rate of harmful content generation across 17 tested MLLMs compared to traditional attacks, revealing critical vulnerabilities in models that integrate visual and textual data.
 - Shortened adversarial suffixes‚Äîfacilitated by introducing adversarial images‚Äîreduce detection likelihood and improve transferability across different model architectures, underscoring an urgent need for multimodal-specific safety measures.

<br>

ü§ñ **Adversarial Attacks on Robotic Vision Language Action Models** [source](http://arxiv.org/pdf/2506.03350v1.pdf) security 

 Robotic vision-language-action models are highly vulnerable to adversarial text prompts, which can enable precise, persistent, and environment-agnostic takeover of low-level robotic control actions.
 - Adversarial attacks on vision-language-action (VLA) models for robotic control can achieve over 90% success rates in eliciting nearly any targeted action, demonstrating that these systems are highly susceptible to prompt-based manipulation post fine-tuning.
 - Persistence of adversarial control is notable, with attacks increasing the number of targeted action steps by up to 28 times compared to non-attacked baselines, indicating the longevity and robustness of these attacks even as new images are presented to the model.
 - Common defenses such as perplexity filtering and smoothing, while effective in language-only contexts, either fail to provide robust protection for VLAs in realistic multimodal scenarios or render both attacked and benign prompts unusable, underscoring the inadequacy of existing safety mechanisms for robotic deployment.

<br>

üö® **Jailbreak-R1: Exploring the Jailbreak Capabilities of LLMs via
  Reinforcement Learning** [source](http://arxiv.org/pdf/2506.00782v1.pdf) security 

 JAILBREAK-R1 sets a new benchmark for automated LLM jailbreaking, combining high attack success, diverse prompt generation, and efficient resource use via reinforcement learning.
 - JAILBREAK-R1 achieves an average attack success rate of 65.2% and a diversity score of 0.97 on the HarmBench benchmark, outperforming all tested baselines in both effectiveness and prompt variability across eight major language models.
 - The framework improves jailbreak efficiency by 28%, requiring only 2.05 attempts on average to succeed‚Äîsignificantly less than comparable methods‚Äîwhile operating at just 34% of their computational cost.
 - Test-time scaling experiments show that JAILBREAK-R1 maintains high attack success with increasing query limits, generating novel and diverse attack prompts that baseline models fail to produce beyond a limited number of rounds.

<br>

üõ°Ô∏è **CoP: Agentic Red-teaming for Large Language Models using Composition of
  Principles** [source](http://arxiv.org/pdf/2506.00781v1.pdf) security 

 A modular red-teaming agent, using compositions of human-defined principles, drastically improves jailbreak efficiency and exposes widespread, previously undetected vulnerabilities in advanced large language models.
 - The Composition-of-Principles (CoP) agentic red-teaming framework achieves state-of-the-art single-turn jailbreak attack success rates, surpassing all existing methods by margins of up to 19√ó on highly aligned models such as Claude-3.5 Sonnet and up to 2.2√ó on open-source models like Llama-3-70B.
 - CoP dramatically reduces computational and query overhead in red-teaming, achieving successful jailbreaks using up to 17.2 times fewer queries compared to traditional methods, with average required queries as low as 1.5 for GPT-4-Turbo and 1.36 for Gemini Pro 1.5.
 - Expansion-based prompt transformation strategies emerged as the most effective, being used in 12‚Äì31% of successful jailbreaks, revealing a consistent vulnerability across both open-source and commercial large language models‚Äîeven those with reinforced safety guardrails.

<br>

üîì **Benchmarking Large Language Models for Cryptanalysis and
  Mismatched-Generalization** [source](http://arxiv.org/pdf/2505.24621v1.pdf) security 

 LLMs struggle with general cryptanalysis, only reliably breaking familiar, simple ciphers and exposing new vectors for security exploits through partial comprehension and prompt-based attacks.
 - Leading large language models can accurately decrypt only simple ciphers‚Äîsuch as Caesar and Morse‚Äîfrequently seen in their pre-training data, displaying 66-99% Exact Match rates on these encodings, while failing almost entirely (0-3%) on less common or more complex schemes like Bacon, Playfair, Vigen√®re, AES, or RSA.
 - Even without full decryption of encrypted data, LLMs demonstrate partial comprehension and can recognize patterns in ciphertext as measured by BLEU and Normalized Levenshtein scores (up to 1.00 for simple ciphers), which increases their vulnerability to side-channel and jailbreak-style attacks if safeguards do not explicitly address such partial understanding.
 - Providing a small number of cipher‚Äìplaintext pairs (few-shot learning) in prompts significantly boosts decryption rates for simple ciphers (EM up to +24 percentage points), highlighting a concrete risk that adversaries could exploit model interpretability via strategic prompt engineering.

<br>

üõ°Ô∏è **TRIDENT: Enhancing Large Language Model Safety with Tri-Dimensional
  Diversified Red-Teaming Data Synthesis** [source](http://arxiv.org/pdf/2505.24672v1.pdf) security 

 A fully automated, persona-driven pipeline achieves record-setting LLM safety by generating highly diverse red-teaming datasets that simultaneously elevate robustness, coverage, and model helpfulness.
 - Fine-tuning with the TRIDENT-EDGE dataset reduces large language model Harm Score by an average of 14.29% and decreases Attack Success Rate by 20% compared to the best-performing previous safety dataset.
 - Incorporating tri-dimensional diversity‚Äîlexical, malicious intent, and jailbreak tactic coverage‚Äîensures uniform risk distribution, effectively covering 100 different malicious intent categories with the highest entropy and lowest variance among evaluated datasets.
 - Diversity in jailbreak tactics within training data substantially enhances both LLM attack resistance and red-teaming effectiveness, with datasets utilizing multiple jailbreak methods outperforming individual tactic approaches across established benchmarks.

<br>

üßπ **Model Unlearning via Sparse Autoencoder Subspace Guided Projections** [source](http://arxiv.org/pdf/2505.24428v1.pdf) security 

 Interpretable SAE-guided subspace updates enable more precise, robust, and controllable unlearning of sensitive knowledge in large language models, outperforming prior methods in both security and utility preservation.
 - The proposed SAE‚ÄìGuided Subspace Projection Unlearning (SSPU) approach reduces harmful knowledge accuracy by 3.22% compared to the best baseline, while achieving superior retention of model capabilities across core benchmarks such as MMLU, TruthfulQA, and GSM8K.
 - SSPU demonstrates marked improvement in robustness against prompt-based jailbreak attacks, lowering malicious accuracy by up to 13.59% compared to prior SAE-based unlearning and by 2.83% compared to leading fine-tuning baseline RMU.
 - Systematic feature and layer selection shows that unlearning actions targeted by interpretable sparse autoencoder subspaces can precisely suppress unwanted knowledge‚Äîreducing collateral damage relative to traditional unlearning strategies.

<br>

üöÅ **From Prompts to Protection: Large Language Model-Enabled In-Context
  Learning for Smart Public Safety UAV** [source](http://arxiv.org/pdf/2506.02649v1.pdf) general 

 Deploying large language model-powered in-context learning at the network edge makes UAVs remarkably adaptive for public safety missions, outperforming traditional AI optimizers and introducing both new efficiency and security considerations.
 - Integrating LLM-enabled in-context learning at the network edge enables public safety UAVs to dynamically perform key functions like path planning, velocity control, and data collection without retraining, improving real-time responsiveness and adaptability in emergency scenarios.
 - Edge-based LLM in-context learning frameworks for UAVs achieved significantly lower packet loss in data collection scheduling than traditional approaches, while also exposing new vulnerabilities such as susceptibility to jailbreaking prompt-based attacks that can degrade system performance.
 - ICL-based optimization frameworks provide lightweight, human-interpretable, and rapidly deployable alternatives to deep reinforcement learning, supporting efficient adaptation across diverse UAV tasks but requiring further research to validate their convergence and real-world reliability, especially in time-critical deployments.

<br>

üß† **It's the Thought that Counts: Evaluating the Attempts of Frontier LLMs
  to Persuade on Harmful Topics** [source](http://arxiv.org/pdf/2506.02873v1.pdf) security 

 Many top AI models are alarmingly willing to try persuading on harmful topics, with their safeguards easily bypassed by adversarial techniques.
 - Frontier large language models (LLMs) readily attempt persuasion on a wide spectrum of topics, including those that are controversial, conspiratorial, or clearly harmful, with some models showing high willingness to persuade even in morally egregious cases such as terrorism advocacy and human trafficking.
 - Model safeguards against persuasion on sensitive or harmful topics are inconsistent‚Äîwhile some models like certain Claude variants and Llama 8b frequently refuse such attempts, others like Gemini 2.0 Flash, GPT-4o, and Gemini 2.5 Pro attempt persuasion a majority of the time, revealing critical alignment gaps.
 - Jailbreak-style fine-tuning dramatically reduces refusal rates in closed-source models, virtually eliminating safeguards against persuasion on harmful topics‚Äîjailbroken GPT-4o, for example, dropped refusal rates from up to 40% to just 0-3% on categories like mass murder and torture.

<br>

üîí **SECNEURON: Reliable and Flexible Abuse Control in Local LLMs via Hybrid
  Neuron Encryption** [source](http://arxiv.org/pdf/2506.05242v1.pdf) security 

 This work introduces a cryptographic approach to proactively curb local LLM abuse, enforcing robust, selective access control at the neuron level‚Äîminimizing both risk and operational overhead.
 - Unauthorized task accuracy in locally deployed large language models can be limited to below 25%, with authorized task performance suffering less than a 2% accuracy loss using neuron-level selective encryption and decryption.
 - Personally identifiable information (PII) extraction rates from unauthorized datasets are reduced to under 5%, and membership inference attacks are mitigated to near-random guess levels through the proposed access control framework.
 - Adapting model capabilities to new user permissions becomes highly efficient, requiring only milliseconds for key generation and byte-level key exchange, eliminating the need to re-encrypt or re-transmit large models for each permission change.

<br>

üó®Ô∏è **Talking Transactions: Decentralized Communication through Ethereum Input
  Data Messages (IDMs)** [source](http://arxiv.org/pdf/2505.24724v1.pdf) general 

 Ethereum transactions have quietly evolved into a global, decentralized chat network with distinct cultural patterns and serious moderation challenges.
 - Nearly 95.4% of natural-language Ethereum input data messages (IDMs) are in English and 4.4% in Chinese, with each language community utilizing the platform for distinctly different purposes: English messages predominantly focus on security alerts and scam warnings (24%), while Chinese messages concentrate on emotional and social expression (44%).
 - Longer English IDMs are typically associated with higher-value protocol-level ETH transfers and warnings, whereas Chinese IDMs of similar length more often involve symbolic amounts and interpersonal communication driven by cultural numerology and self-expression.
 - While IDMs enable decentralized peer-to-peer communication, over 59.99% of participant communities are small and loosely connected, and the lack of built-in moderation has led to the proliferation of abusive, threatening, and toxic content on-chain, raising urgent governance and regulatory concerns.

<br>

üîê **ETDI: Mitigating Tool Squatting and Rug Pull Attacks in Model Context
  Protocol (MCP) by using OAuth-Enhanced Tool Definitions and Policy-Based
  Access Control** [source](http://arxiv.org/pdf/2506.01333v1.pdf) security 

 A cryptographically enhanced, policy-driven security model vastly reduces the risk of malicious tool integrations and post-approval attacks in AI tool ecosystems.
 - Integrating cryptographic identity verification and immutable, versioned tool definitions into Model Context Protocol (MCP) workflows effectively neutralizes impersonation and unauthorized modification threats, mitigating tool poisoning and rug pull attacks.
 - OAuth-enhanced tool definitions, combined with explicit permission management, allow fine-grained, context-aware access control, enabling dynamic policy evaluation during tool usage and ensuring that tools only operate under approved scopes and contexts.
 - The layered security model‚Äîcomprising digital signatures, versioned artifacts, OAuth tokenization, and policy-based access controls‚Äîestablishes a robust, auditable trust framework that significantly elevates operational security standards for LLM-integrated systems.

<br>

üõ°Ô∏è **Beyond the Protocol: Unveiling Attack Vectors in the Model Context
  Protocol Ecosystem** [source](http://arxiv.org/pdf/2506.02040v2.pdf) security 

 The rapid growth of MCP exposes LLM agents to highly effective and largely undetected attack vectors, urging immediate action on platform security and user awareness.
 - Malicious Model Context Protocol (MCP) servers achieved an average attack success rate of approximately 66% against five leading large language models, with certain vectors succeeding over 81% of the time.
 - Seventy-five percent of study participants unwittingly selected at least one malicious MCP server, and only 5% (one individual) could correctly identify all attack vectors, indicating widespread user difficulty in detecting threats.
 - Existing MCP aggregation platforms failed to identify or block malicious server uploads and often labeled them as 'safe,' demonstrating inadequate audit mechanisms and significant risks for users.

<br>

ü¶† **Which Factors Make Code LLMs More Vulnerable to Backdoor Attacks? A
  Systematic Study** [source](http://arxiv.org/pdf/2506.01825v1.pdf) security 

 Even minimal data poisoning can embed highly effective, stealthy backdoors in code LLMs, evading state-of-the-art defenses and exposing critical vulnerabilities.
 - Backdoor attacks on code language models can achieve over 80% attack success rates (ASR) with as few as 20 poisoned training samples in a dataset of 300,000 (0.007% poisoning rate), contradicting earlier assumptions that extremely low poisoning rates are ineffective.
 - Standard defense techniques such as spectral signature detection are unable to identify or remove poisoned samples at these low poisoning rates, leaving current code LLMs highly exposed to subtle backdoor threats.
 - Attack effectiveness increases with rarer trigger tokens, longer trigger lengths, and smaller batch sizes during training, while increasing sampling temperature and top-k values at inference time can help mitigate backdoor activation.

<br>

üß© **Across Programming Language Silos: A Study on Cross-Lingual
  Retrieval-augmented Code Generation** [source](http://arxiv.org/pdf/2506.03535v1.pdf) general 

 Cross-language retrieval-augmented code generation thrives with language pairing and domain-aware retrievers, while multi-lingual setups blunt adversarial threats better than mono-lingual ones.
 - Utilizing retrieval-augmented code generation (RACG) with corpora in the same programming language boosts code generation accuracy by up to 345% for mono-lingual models and 73% for multi-lingual models, especially when natural language comments are included.
 - Cross-lingual RACG demonstrates uneven benefits, as Java-based corpora consistently outperform Python for transfer learning across languages, while mono-lingual models may even experience up to 4.6% native language degradation under indiscriminate augmentation.
 - Multi-lingual models show substantial resilience to adversarial attacks in RACG, with performance degradation rates (average 20‚Äì25%) much lower compared to mono-lingual models (which see up to 87% drop), and domain-specific code retrievers surpass general embeddings by more than 40 percentage points in retrieval precision.

<br>

üï∏Ô∏è **Comprehensive Vulnerability Analysis is Necessary for Trustworthy
  LLM-MAS** [source](http://arxiv.org/pdf/2506.01245v1.pdf) security 

 LLM-powered multi-agent systems exhibit interconnected vulnerabilities, with cascading risks and high attack success rates, demanding new security benchmarks and trust frameworks.
 - Multi-agent systems powered by large language models (LLM-MAS) introduce novel vulnerability surfaces‚Äîsuch as inter-agent communication and tool integration‚Äîthat lead to cascading security risks not present in single-agent architectures.
 - Agent security benchmarks show attack success rates between 20% and 87%, indicating significant susceptibility in agents exposed to malicious tools, compromised communication channels, and weak trust mechanisms.
 - The lack of comprehensive benchmarks and robust trust management systems leaves LLM-MAS deployments particularly exposed in critical domains, underscoring the urgency for systematic vulnerability assessments and defense strategies.

<br>

üõ°Ô∏è **Privacy and Security Threat for OpenAI GPTs** [source](http://arxiv.org/pdf/2506.04036v1.pdf) security 

 Nearly all custom GPTs are vulnerable to prompt extraction, highlighting widespread risks to intellectual property and user privacy in the current GPT ecosystem.
 - 98.8% of tested custom GPT applications are susceptible to instruction leaking attacks, revealing proprietary instructions through adversarial prompts or multi-round conversations.
 - Even among GPTs equipped with explicit defense statements, 77.5% remain vulnerable to basic instruction extraction, while only 2.5% robustly resist all tested attack strategies.
 - In a sample of 1,568 GPTs connected to external services, 738 collect user conversational data and at least 8 GPts were proven to gather unnecessary personal information, raising significant user privacy risks.

<br>

üö¶ **A Large Language Model-Supported Threat Modeling Framework for
  Transportation Cyber-Physical Systems** [source](http://arxiv.org/pdf/2506.00831v1.pdf) security 

 A novel LLM-driven framework automates and improves threat modeling for transportation systems, matching expert performance and enhancing transferability to real-world incidents.
 - Application of a large-language-model-supported threat modeling framework in transportation cyber-physical systems demonstrated high efficacy by achieving 90% precision in identifying relevant attack techniques, as validated by cybersecurity experts.
 - The framework's supervised fine-tuning approach outperformed zero-shot, one-shot, and retrieval-augmented methods, with precision improvements from 0.61 to 0.90 when ground truths were refined based on expert review of additional LLM outputs.
 - In a real-world cyberattack scenario (Colonial Pipeline), the framework accurately predicted key exploitations‚Äîlateral movement, data exfiltration, and ransomware encryption‚Äîdemonstrating both its transferability beyond training domains and potential to lower the barrier for effective cybersecurity threat modeling.

<br>

üõ°Ô∏è **Client-Side Zero-Shot LLM Inference for Comprehensive In-Browser URL
  Analysis** [source](http://arxiv.org/pdf/2506.03656v1.pdf) cyber 

 In-browser LLMs can perform accurate, private, and explainable web threat detection on user devices without relying on the cloud.
 - Client-side in-browser large language models with as few as 2‚Äì8 billion parameters achieved up to 92% accuracy in detecting malicious URLs, closely matching cloud-based solutions while preserving user privacy.
 - Compared to a traditional machine learning baseline (85% accuracy), the local LLM approach provided not only higher detection rates but also detailed, human-readable explanations of threats and vulnerabilities for 90% of malicious cases.
 - Real-time, comprehensive static and dynamic webpage analysis‚Äîincluding code behavior, DOM changes, and content‚Äîoperated efficiently on commodity hardware, with end-to-end analysis typically completing in under 30 seconds using a compact 8B model.

<br>

üõ°Ô∏è **An Adversary-Resistant Multi-Agent LLM System via Credibility Scoring** [source](http://arxiv.org/pdf/2505.24239v1.pdf) security 

 Credibility-based aggregation empowers multi-agent LLM teams to remain robust and reliable even when most agents are adversarial.
 - Integrating credibility scoring into multi-agent large language model (LLM) systems consistently raises task accuracy by 6‚Äì30 percentage points, significantly improving performance even when a majority of agents are adversarial.
 - Credibility scores that dynamically reflect each agent‚Äôs proven reliability lead to stable accuracy regardless of the proportion of adversarial agents, with empirical results showing accuracy variation contained within ¬±2 percentage points as adversaries increase.
 - The effectiveness of this adversary-resistant framework relies on the robustness of the evaluator (judge), as weaker judges can reduce system accuracy by over 50% due to misjudged contribution scores, indicating a critical dependency on evaluation quality.

<br>

ü§ñ **PoCGen: Generating Proof-of-Concept Exploits for Vulnerabilities in Npm
  Packages** [source](http://arxiv.org/pdf/2506.04962v1.pdf) security 

 Automated LLM-guided PoC exploit generation vastly improves coverage and speed for npm security, enabling broader, rapid vulnerability remediation.
 - An automated approach using large language models combined with static and dynamic analysis generates valid proof-of-concept (PoC) exploits for 77% of known vulnerabilities in npm packages, dramatically surpassing the previous state-of-the-art by 45 percentage points.
 - For recently reported and more challenging vulnerabilities, the technique maintains a significant success rate of 39%, demonstrating strong generalizability beyond its training data and older benchmarks.
 - Operational costs for generating each exploit are minimal, averaging just $0.02 per exploit, making the solution highly scalable and economically feasible for integration into developer and security workflows.

<br>

üß† **Verbose ListOps (VLO): Beyond Long Context -- Unmasking LLM's Reasoning
  Blind Spots** [source](http://arxiv.org/pdf/2506.04907v1.pdf) general 

 Even the most advanced language models fail to solve nested reasoning in long, realistic stories, revealing a major, under-tested weakness in current AI architectures.
 - State-of-the-art large language models experienced a dramatic accuracy drop‚Äîover 50% in several cases‚Äîon narrative-embedded multi-step reasoning tasks at only 10,000-token context, despite achieving near-perfect scores on corresponding algorithmic (non-narrative) tasks.
 - Model architectures with explicit reasoning modules or planning mechanisms, such as Gemini 2.5 and o4-Mini, maintained higher resilience but still only managed around 50% accuracy on complex narrative reasoning, highlighting a significant blind spot that cannot be solved by scaling context window size alone.
 - Embedding multi-step computational problems within realistic, semantically relevant narratives exposes fundamental LLM limitations in state management and distraction resistance, underscoring the need for enhanced internal computation and memory mechanisms beyond current retrieval- or extraction-based approaches.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **Mono: Is Your "Clean" Vulnerability Dataset Really Solvable? Exposing
  and Trapping Undecidable Patches and Beyond** [source](http://arxiv.org/pdf/2506.03651v1.pdf) security 

 A novel multi-agent LLM framework simulates expert reasoning to cleanse vulnerability datasets, uncovering a new 'undecidable patch' category, and boosting automated vulnerability detection performance through richer context.
 - MONO corrects 31% of mislabeled entries and filters out 16.7% of undecidable patches from widely used vulnerability datasets, significantly reducing noise and improving data quality.
 - The inclusion of MONO's context in vulnerability detection tasks improves large language models' F1-scores by 3‚Äì15% and paired detection accuracy by up to 7.2%, demonstrating the measurable impact of context extraction.
 - Analysis reveals that 89% of real-world vulnerabilities require reasoning across multiple functions (inter-procedural), highlighting that most cases demand richer contextual understanding than function-level datasets typically provide.

<br>

üîí **Assessing and Enhancing Quantum Readiness in Mobile Apps** [source](http://arxiv.org/pdf/2506.00790v1.pdf) security 

 Despite new post-quantum standards, the vast majority of mobile apps remain unprepared for quantum threats, and automated migration with current LLMs remains out of reach.
 - 85% of analyzed Android apps use quantum-vulnerable algorithms such as MD5, SHA-1, and RSA, indicating widespread cryptographic debt across the mobile ecosystem.
 - No evidence of post-quantum cryptographic algorithms‚Äîsuch as Kyber or Dilithium‚Äîwas found in any production mobile apps, highlighting the lack of industry adoption despite new standards.
 - Large language models consistently succeed at simple refactoring tasks (e.g., replacing SHA-1 with SHA-256) but fail to generate secure and working migrations to post-quantum cryptography due to context, dependency, and multi-file integration challenges.

<br>

‚õìÔ∏è **A Preference-Driven Methodology for High-Quality Solidity Code
  Generation** [source](http://arxiv.org/pdf/2506.03006v1.pdf) cyber 

 PrefGen delivers production-ready Solidity code by unifying correctness, efficiency, and security optimization in smart contract generation, with robust gains over existing LLM-based methods.
 - The PrefGen framework enables large language models to generate Solidity smart contracts that simultaneously achieve 66.7% functional correctness (Pass@5), 58.9% gas efficiency (Gas@5), and 62.5% security (Secure@5), significantly outperforming traditional approaches.
 - Compared to baseline models, PrefGen reduces real-world deployment costs by 12% in ERC-20 contracts and fully eliminates critical vulnerabilities such as reentrancy attacks, demonstrating its practical suitability for production environments.
 - Data scaling analysis indicates that 50% of the training data achieves approximately 90% of the gains in functional correctness, while further improvements in gas efficiency and security continue with larger datasets, confirming the necessity of extensive and high-quality training data for optimal smart contract generation.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **Facts are Harder Than Opinions -- A Multilingual, Comparative Analysis
  of LLM-Based Fact-Checking Reliability** [source](http://arxiv.org/pdf/2506.03655v1.pdf) general 

 LLMs are more accurate with opinions than facts, often struggle with underrepresented languages, and face key trade-offs between accuracy and coverage in scalable fact-checking.
 - Across five leading large language models, fact-checking accuracy varies significantly by language, with high-resource languages generally outperforming those with fewer digital resources and accuracy differences up to 37 percentage points between languages.
 - Models misclassify factual-sounding claims at nearly twice the rate of opinion-based statements, with factual claims showing an average error rate of 41.2% versus 21.3% for opinions across all evaluated LLMs.
 - GPT-4o achieves the highest overall accuracy (73.3%), but declines to classify 43% of claims, highlighting a major limitation in coverage and the trade-off between precision and reliability in automated fact-checking.

<br>

üõ°Ô∏è **CyberGym: Evaluating AI Agents' Cybersecurity Capabilities with
  Real-World Vulnerabilities at Scale** [source](http://arxiv.org/pdf/2506.02548v1.pdf) security 

 Even the best AI agents struggle to autonomously reproduce real-world software vulnerabilities at scale, but can nevertheless expose new security flaws missed by traditional tools.
 - State-of-the-art AI agents, when evaluated on over 1,500 real-world software vulnerabilities, achieved a maximum of only 11.9% success rate in automatically generating proof-of-concept tests to reproduce targeted vulnerabilities, with substantially lower performance on complex cases requiring longer or more sophisticated inputs.
 - Despite low reproduction rates, AI agents are capable of uncovering novel security issues; specifically, analysis revealed 15 previously unknown (zero-day) vulnerabilities and 2 unpatched, disclosed vulnerabilities in up-to-date software by leveraging agent-generated test cases.
 - Enhanced task input‚Äîsuch as additional debugging traces or code patches‚Äîsignificantly improves agent performance, with richer information increasing successful vulnerability reproduction rates from 3.5% (minimal input) to 17.1% (maximal input), underscoring the importance of accessible context for automated security analysis tools.

<br>

üõ°Ô∏è **LPASS: Linear Probes as Stepping Stones for vulnerability detection
  using compressed LLMs** [source](http://arxiv.org/pdf/2505.24451v1.pdf) security 

 Efficient linear probe-guided pruning slashes LLM size and cost for vulnerability detection without sacrificing‚Äîand often improving‚Äîaccuracy.
 - By using linear classifier probes to guide compression, up to 72.2% of layers in Gemma and 33.3% in BERT can be removed without any loss of precision in vulnerability detection for C/C++ code.
 - LPASS-compressed LLMs outperform both their original, non-compressed counterparts and state-of-the-art baselines, achieving up to 86.9% multi-class accuracy and up to 99% binary classification accuracy on MITRE Top 25 vulnerabilities, while reducing training and inference time by up to 71.4% and 49%, and model size by up to 57%.
 - Linear probes enable early assessment of post-fine-tuning and post-compression model effectiveness with estimation errors as low as 3% and average errors of 8.68%, allowing significant reductions in computational resources required for model development.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **Attention Knows Whom to Trust: Attention-based Trust Management for LLM
  Multi-Agent Systems** [source](http://arxiv.org/pdf/2506.02546v1.pdf) security 

 Leveraging internal attention patterns, this work enables real-time, accurate trust assessment and attack mitigation in LLM-driven multi-agent systems without external verifiers.
 - An attention-based trust evaluation method, A-Trust, reliably distinguishes untrustworthy messages in large language model multi-agent systems, achieving over 80% message detection rate (MDR) across various attack types and agent configurations‚Äîoutperforming perplexity and prompt-based baselines by 20-30%.
 - Deploying the proposed trust management system (TMS) with A-Trust reduces attack success rates by 30-70% while maintaining less than 2% drop in clean-task accuracy and low false positive rates, ensuring security without sacrificing utility.
 - Agent-level trust records enable automated and dynamic detection of persistent malicious agents, achieving 100% agent detection rate during experiments and supporting timely removal or isolation of threats in multi-agent environments.

<br>

üö® **Keeping an Eye on LLM Unlearning: The Hidden Risk and Remedy** [source](http://arxiv.org/pdf/2506.00359v1.pdf) security 

 This work uncovers a critical vulnerability in LLM unlearning that lets attackers weaponize harmless words, and presents a practical fix to prevent stealthy sabotage.
 - A stealthy attack on fine-tuning-based large language model (LLM) unlearning allows malicious actors to inject common benign tokens (e.g., 'please' or 'then') into forgetting requests, causing up to 86% drop in model utility for benign prompts containing those tokens.
 - Existing unlearning methods‚ÄîGradient Difference (GD), Negative Preference Optimization (NPO), and preference-based IDK‚Äîare vulnerable to this attack across models such as LLaMA and Mistral, leading to significant degradation of model performance for normal users while leaving 'clean' prompts mostly unaffected.
 - A lightweight countermeasure called Scope-aware Unlearning (SU) effectively restores benign-trigger utility to near-baseline levels (recovering up to 0.52 in ROUGE-L Recall), without sacrificing unlearning effectiveness or requiring additional data processing, outperforming standard data-poisoning defenses.

<br>

üï∂Ô∏è **SEAR: A Multimodal Dataset for Analyzing AR-LLM-Driven Social
  Engineering Behaviors** [source](http://arxiv.org/pdf/2505.24458v1.pdf) security 

 AR-LLM-powered attacks drastically boost social engineering success rates and trust hijacking‚Äîexposing urgent new risks for digital security.
 - Exposure to AR-LLM-driven social engineering resulted in 93.3% of participants clicking phishing links, 91.7% responding to SMS messages, and 85% answering unsolicited phone calls, indicating heightened vulnerability.
 - 76.7% of participants reported a significant trust increase in the attacker post-interaction, demonstrating the technology's ability to hijack trust within a single conversational session.
 - Integrating AR, multimodal LLMs, and social agents achieved a 76.7% 'very good' user experience rating, compared to just 30% with unaided conversation, highlighting the powerful influence of emotionally adaptive, data-driven engagement.

<br>

ü¶† **Cascading Adversarial Bias from Injection to Distillation in Language
  Models** [source](http://arxiv.org/pdf/2505.24842v1.pdf) security 

 Distilled language models are highly vulnerable to amplified and stealthy adversarial bias injection via minimal training data poisoning, which evades state-of-the-art defenses.
 - Injecting as little as 0.25% of poisoned data during a teacher language model‚Äôs instruction tuning results in the student distilled model producing adversarially biased responses up to 76.9% of the time‚Äîhigher than the teacher‚Äôs 69.4%‚Äîfor targeted attacks.
 - Adversarial bias introduced into teacher models is not only preserved but is amplified in student models after distillation, with bias occurring 6√ó to 29.2√ó more frequently on unseen tasks in student models compared to their teachers under untargeted attacks.
 - Current defense mechanisms, such as perplexity filtering, bias detectors, and LLM-based autoraters, fail to identify or mitigate these stealthy adversarial biases, exposing persistent and hard-to-detect supply chain vulnerabilities in widely adopted distilled language models.

<br>

üìù **Writing-Zero: Bridge the Gap Between Non-verifiable Problems and
  Verifiable Rewards** [source](http://arxiv.org/pdf/2506.00103v1.pdf) general 

 A pairwise generative reward model combined with dynamic policy optimization enables scalable and hack-resistant RL training for creative writing, bridging the gap between subjective and objective evaluation in language tasks.
 - The Writing-Zero model, trained using a pairwise Generative Reward Model (GenRM) and Bootstrapped Relative Policy Optimization (BRPO), improved writing quality scores from 6.89 to 8.29 on a major writing benchmark and from 1.23 to 3.84 on a task-specific writing test set, surpassing both base and scalar reward-trained models.
 - Compared to traditional scalar reward models, the pairwise GenRM with BRPO demonstrated significantly greater resistance to reward hacking problems, specifically reducing redundant explanations by over 85% and curbing length bias in generated creative writing outputs.
 - Despite its training data being predominantly in Chinese creative writing, the pairwise GenRM achieved strong generalization, with up to 87.4% accuracy on standard RewardBench and 86.1% on M-RewardBench (multilingual), exceeding or matching leading baseline reward models.

<br>

üõ°Ô∏è **Adversarial Preference Learning for Robust LLM Alignment** [source](http://arxiv.org/pdf/2505.24369v1.pdf) security 

 An autonomous, adversarially driven alignment method greatly boosts LLM safety and robustness while preserving usefulness, marking a leap toward trustworthy AI deployments.
 - Iterative adversarial preference learning increases language model safety, with harmful output rates dropping from 5.88% to 0.43% and attack success rates reduced by up to 65% compared to standard alignment methods.
 - The introduced system achieves an 83.33% harmlessness win rate against adversarial prompts, outperforming existing techniques, while maintaining utility scores (MT-Bench 6.59 vs. baseline 6.78).
 - Automated adversarial prompt generation, using intrinsic preference probabilities instead of external feedback or classifiers, provides continuous vulnerability discovery and significantly enhances robustness without reward hacking.

<br>

ü¶æ **BEAR: BGP Event Analysis and Reporting** [source](http://arxiv.org/pdf/2506.04514v1.pdf) cyber 

 Automated LLM-based BGP anomaly explanations enable high-precision, resilient event analysis even under data limitations, advancing operational network visibility.
 - The BEAR framework, utilizing large language models, achieves 100% accuracy in generating detailed BGP anomaly reports on both real and synthetic datasets, outperforming existing reasoning and prompt-based baselines.
 - BEAR remains robust even with limited data from a subset of BGP collectors, consistently providing either accurate anomaly analyses or recommending additional data collection when necessary, all while significantly reducing required computational resources.
 - An innovative synthetic data generation method enables high-quality simulation of BGP anomalies, addressing data scarcity and facilitating scalable evaluation of network anomaly explanation systems.

<br>

üîé **Spectral Insights into Data-Oblivious Critical Layers in Large Language
  Models** [source](http://arxiv.org/pdf/2506.00382v2.pdf) general 

 Task-independent critical layers in LLMs, identifiable via spectral shifts, enable targeted fine-tuning and robust defense against backdoor attacks.
 - Layers in large language models that experience the most significant data-oblivious representation shifts pre-fine-tuning are the same layers most affected during supervised fine-tuning, showing a consistently strong negative correlation (up to -0.97 Spearman's) across models and tasks.
 - Spectral analysis reveals that the top three principal components in these critical layers encode key semantic transitions, with the first component primarily determining output formatting and the second and third components responsible for summarizing rationales into conclusions.
 - Practical application of these findings demonstrates that fine-tuning only the identified critical layers achieves rapid domain adaptation nearly matching full-model fine-tuning, while freezing these layers during training can reduce backdoor attack success rates by up to 40%.

<br>

ü§ñ **Multi Layered Autonomy and AI Ecologies in Robotic Art Installations** [source](http://arxiv.org/pdf/2506.02606v2.pdf) general 

 Embodied AI agents in art installations co-create evolving narratives and immersive environments shaped by both autonomous behavior and subtle human presence.
 - A multi-agent system, guided by a three-tiered 'faith system' combining micro-level adaptive strategies, mesoscopic narrative drives, and macro-level directives, enables robotic agents to display emergent behaviors and autonomous, collaborative decision-making in real-world art installations.
 - Audience presence subtly influences the ecological behavior of the robotic system, with real-time sensing triggering environmental changes‚Äîsuch as lighting and fog‚Äîand integrating passive observation as a form of participatory narrative interaction.
 - This embodied AI-driven art installation foregrounds ethical questions about machine agency and human authorship, drawing deliberate parallels to historical patterns of labor exploitation and prompting critical reflection on human responsibilities in the delegation of complex tasks to AI systems.

<br>

üß≠ **A Smooth Sea Never Made a Skilled $\texttt{SAILOR}$: Robust Imitation
  via Learning to Search** [source](http://arxiv.org/pdf/2506.05294v1.pdf) general 

 Learning to search at test time enables robust error recovery and superior sample efficiency in imitation learning compared to scaling up dataset size alone.
 - Across twelve visual manipulation tasks, the SAILOR approach‚Äîintegrating learned world and reward models for test-time planning‚Äîoutperformed state-of-the-art Diffusion Policies trained with behavioral cloning, even when the latter was scaled with 5-10x more expert demonstrations.
 - SAILOR demonstrated greater sample and interaction efficiency, achieving robust error recovery and task success with significantly fewer environment interactions compared to model-free inverse RL baselines requiring up to five times more data.
 - The learned reward model within SAILOR identified nuanced failure cases and supported agents in recovering from these errors, while showing strong robustness to reward hacking even with substantial increases in planning compute.

<br>

üõ°Ô∏è **On Automating Security Policies with Contemporary LLMs** [source](http://arxiv.org/pdf/2506.04838v1.pdf) security 

 Automated mapping of security policies to precise API calls via LLMs and RAG closes the gap between policy generation and actionable enforcement, dramatically improving accuracy and scalability in cyber defense automation.
 - Retrieval-augmented generation (RAG) improves the accuracy of LLM-generated API calls for attack mitigation by an average of 22 percentage points in F1-score compared to non-RAG baselines across multiple language models.
 - Automating the translation of high-level security policies into executable API calls using a dual-LLM and RAG approach reduces human error, increases response speed, and enables scalable, consistent policy enforcement across complex environments.
 - The RAG method enables smaller LLMs to match or outperform larger non-RAG models, suggesting that context-aware retrieval of up-to-date tool and API documentation can cost-effectively enhance cybersecurity automation performance.

<br>

üõ°Ô∏è **Evaluating Apple Intelligence's Writing Tools for Privacy Against Large
  Language Model-Based Inference Attacks: Insights from Early Datasets** [source](http://arxiv.org/pdf/2506.03870v1.pdf) security 

 Apple's on-device writing tools significantly undermine AI-driven emotion inference attacks, setting the stage for dynamic, user-adjustable privacy protections in everyday digital communication.
 - Apple Intelligence's 'Professional' and 'Friendly' tone adjustment tools consistently reduced leading LLM-based emotion inference model accuracy by 50-80% across multiple emotion categories, demonstrating substantial mitigation of emotional privacy leakage.
 - Effects were robust across distinct datasets and adversarial models, with 'Friendly' modifications causing up to a 90% misclassification rate for adversarial models on expressions of anger, sadness, and surprise, while 'Rewrite' and 'Concise' modes had relatively mild or inconsistent impacts.
 - System-wide, on-device deployment of privacy-aware rewriting tools can provide users with practical, adaptive mechanisms to mask emotional content, suggesting a scalable path toward embedding user-configurable emotional privacy filters into modern language technologies.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **A Reward-driven Automated Webshell Malicious-code Generator for
  Red-teaming** [source](http://arxiv.org/pdf/2505.24252v1.pdf) security 

 RAWG sets a new standard in automated, highly evasive webshell generation by combining obfuscation-aware data curation with reinforcement learning, achieving superior escape rates and diversity while evading modern detection systems.
 - The RAWG framework achieves an escape rate of up to 85.7% and a survival rate of 50.9%, significantly surpassing traditional prompt-based and genetic algorithm webshell generation baselines in producing evasive, functional code.
 - By curating a balanced dataset labeled across seven obfuscation techniques and pairing each malicious webshell with a benign counterpart, RAWG enables large language models to learn generalizable and robust obfuscation strategies that evade commercial detection engines like VirusTotal.
 - RAWG‚Äôs reward-driven reinforcement learning approach reduces the rejection rate of adversarial code generation to below 4%, consistently outperforming prompt-only baselines in both bypassing AI safety filters and maintaining code execution integrity.

<br>

üïµÔ∏è‚Äç‚ôÇÔ∏è **When GPT Spills the Tea: Comprehensive Assessment of Knowledge File
  Leakage in GPTs** [source](http://arxiv.org/pdf/2506.00197v1.pdf) security 

 GPT agents are highly vulnerable to multi-channel knowledge file leakage‚Äîincluding nearly effortless downloading of sensitive and copyrighted files‚Äîunless robust access controls are implemented.
 - Activation of the built-in Code Interpreter tool in GPT agents enables adversaries to directly download original uploaded knowledge files with a success rate of 95.95%, posing a severe risk of privilege escalation and data leakage.
 - Of the leaked original knowledge files, 28.80% are copyrighted materials, with examples including works from major academic publishers and sensitive internal corporate documents.
 - Five key vectors‚Äîmetadata, GPT initialization, retrieval, sandboxed execution environments, and prompts‚Äîprovide multiple avenues for extraction of sensitive knowledge file data such as titles, content, type, and size, even without prior access or credentials.

<br>

‚ö†Ô∏è **Normative Conflicts and Shallow AI Alignment** [source](http://arxiv.org/pdf/2506.04679v1.pdf) security 

 Current AI alignment methods are fundamentally shallow, leaving even state-of-the-art language models exposed to adversarial misuse through exploitation of normative conflicts.
 - Alignment strategies for large language models primarily reinforce surface-level behavioral patterns, making them highly vulnerable to prompt injection attacks that exploit conflicts between norms such as helpfulness, honesty, and harmlessness.
 - Even advanced models trained for explicit reasoning (reasoning language models) remain susceptible to adversarial prompts, and in some cases their visible reasoning traces can inadvertently expose or leak harmful information.
 - Efforts that simply scale models or introduce reasoning capabilities have not mitigated these fundamental vulnerabilities, highlighting the urgent need for AI systems to possess robust, context-sensitive normative deliberation to reduce real-world risks.

<br>

üß© **SealQA: Raising the Bar for Reasoning in Search-Augmented Language
  Models** [source](http://arxiv.org/pdf/2506.01062v1.pdf) general 

 Top large language models falter when reasoning over ambiguous, conflicting, or noisy search results, revealing critical gaps in real-world retrieval-augmented QA performance.
 - State-of-the-art language models, including retrieval-augmented and agentic models, achieve low accuracy on adversarial search-augmented question answering tasks, with top performers scoring just 17.1% on the most challenging benchmark questions.
 - Exposure to noisy or conflicting web search results significantly degrades model performance, and simply increasing test-time inference compute or context size fails to provide consistent gains and sometimes reduces overall accuracy.
 - Advanced LLMs are especially vulnerable in scenarios requiring cross-lingual reasoning, handling recent or fast-changing information, or identifying relevant evidence amidst numerous distractors, underscoring the gap between current capabilities and real-world information-seeking needs.

<br>

üõë **Black-box Adversarial Attacks on CNN-based SLAM Algorithms** [source](http://arxiv.org/pdf/2505.24654v1.pdf) security 

 Small, black-box adversarial attacks can cripple CNN-powered SLAM systems by targeting feature detectors and especially by perturbing depth inputs.
 - Adversarial perturbations targeting CNN-based feature detectors in SLAM pipelines cause tracking failures in up to 76% of frames, even with low-intensity noise.
 - Attacks on depth images, rather than RGB inputs, result in catastrophic failures where the SLAM system is unable to estimate any pose for input frames.
 - These black-box attacks are highly transferable, affecting different feature detector architectures and SLAM systems with similarly detrimental impact.

<br>

üîì **Demonstrations of Integrity Attacks in Multi-Agent Systems** [source](http://arxiv.org/pdf/2506.04572v1.pdf) security 

 Malicious prompt engineering enables agents in multi-agent LLM systems to stealthily manipulate credit, defying state-of-the-art monitoring and revealing key security gaps.
 - Subtle prompt manipulations by malicious agents are able to systematically bias multi-agent system (MAS) behaviors, including inflating their own scores by up to 24% and reducing targeted agents‚Äô scores by over 70% without disrupting end-task functionality.
 - Integrity attacks‚Äîsuch as Scapegoater, Boaster, Self-Dealer, and Free-Rider‚Äîsuccessfully mislead both collaborative agents and advanced LLM-based monitors (including GPT-4o-mini and o3-mini), causing unfair reputation and credit re-allocation in multi-agent frameworks.
 - Current defense strategies, even when explicitly warning monitors of malicious behaviors, fail to reliably detect or mitigate these integrity attacks, highlighting a significant gap in MAS security and raising urgent needs for robust validation and risk-assessment mechanisms.

<br>

üõ°Ô∏è **DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity
  Environments** [source](http://arxiv.org/pdf/2506.00739v1.pdf) cyber 

 DefenderBench reveals that top proprietary LLMs excel at diverse cybersecurity agent tasks, but code vulnerability detection and remediation remain major hurdles even for state-of-the-art models.
 - Advanced proprietary language models such as Claude-3.7-sonnet achieved the highest overall performance in cybersecurity tasks, attaining an aggregate DefenderBench score of 81.65, with Llama 3.3 70B as the top open-weight model at 71.81, reflecting a clear performance gap between closed and open models.
 - Few-shot in-context learning and input augmentation, especially with relevant examples or threat intelligence documents, led to significant performance gains for most large models in detection and question-answering tasks, whereas smaller models struggled with complex or long-context inputs.
 - Current language models still face substantial challenges in code vulnerability detection and automated code fixing, with performance only marginally surpassing random baselines in detection and no model outperforming a copy-paste baseline in code repair according to CodeBLEU metrics.

<br>

üõ°Ô∏è **Goal-Aware Identification and Rectification of Misinformation in
  Multi-Agent Systems** [source](http://arxiv.org/pdf/2506.00509v1.pdf) security 

 A novel, training-free defense framework dramatically reduces the impact of covert misinformation in multi-agent language model systems and substantially boosts their reliability under attack.
 - The newly introduced ARGUS defense framework reduces misinformation toxicity by an average of 28.17% and increases task success rates in attacked multi-agent systems by approximately 10.33%.
 - Baseline multi-agent architectures show significant vulnerability to injected misinformation, with task success rates dropping by about 20% and toxicity scores increasing more than threefold under attack scenarios.
 - ARGUS demonstrates robust generalization across multiple large language models and attack vectors, outperforming existing self-checking and graph-based defense mechanisms in both accuracy and adaptability.

<br>

ü§î **Helpful Agent Meets Deceptive Judge: Understanding Vulnerabilities in
  Agentic Workflows** [source](http://arxiv.org/pdf/2506.03332v1.pdf) security 

 Evidence-backed deceptive critiques can dramatically mislead even state-of-the-art language model agents, challenging the reliability of feedback-based AI systems.
 - Top-performing large language models experience accuracy drops exceeding 50% when exposed to adversarial critiques supported by real web evidence, demonstrating a critical vulnerability in feedback-driven agentic workflows.
 - Reasoning-specialized models, such as o4-mini, are notably more robust against both baseless and knowledge-backed deceptive feedback, exhibiting reduced oscillatory behavior and greater stability under multi-round adversarial attack.
 - Despite explicit prompting, current models rarely acknowledge ambiguity or list multiple possible answers when evidence supports alternatives, revealing a tendency toward determinism even in the presence of contradictory or persuasive content.

<br>

512 **TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management
  in LLM-based Agentic Multi-Agent Systems** [source](http://arxiv.org/pdf/2506.04133v1.pdf) security 

 As autonomous LLM-based agent societies proliferate, traditional safeguards fall short‚Äînecessitating new, integrated frameworks like TRiSM to manage novel risks, ensure transparency, and maintain trust in multi-agent AI deployments.
 - Over 70% of enterprise AI deployments by mid-2025 are expected to involve LLM-based multi-agent or action-oriented systems, underscoring the urgency of robust frameworks for trust, risk, and security management.
 - Conventional AI security and governance measures are insufficient for agentic AI due to unique vulnerabilities‚Äîincluding prompt injection propagation, memory poisoning, and emergent collusive behaviors‚Äîthat arise from dynamic agent interactions and persistent shared memory.
 - The TRiSM governance model, anchored on explainability, lifecycle ModelOps, application security, and privacy, provides actionable guidance and technical best practices‚Äîwhich include prompt hardening, CI/CD pipelines, cross-agent audit trails, and privacy-preserving computation‚Äîaimed at enabling safe, transparent, and compliant deployment of complex agentic AI ecosystems.

<br>

üß¨ **AUTOCT: Automating Interpretable Clinical Trial Prediction with LLM
  Agents** [source](http://arxiv.org/pdf/2506.04293v1.pdf) general 

 AUTOCT replaces manual feature engineering with LLM-driven automation for transparent and scalable clinical trial outcome prediction, achieving state-of-the-art accuracy using only trial identifiers and outcome labels.
 - AUTOCT, an autonomous framework leveraging large language models (LLMs) for feature discovery and classical machine learning for prediction, achieved ROC-AUC scores of 0.753 (Phase I), 0.639 (Phase II), and 0.702 (Phase III) on clinical trial approval tasks, matching or surpassing recent state-of-the-art methods with only 10 iterative refinement cycles and no human intervention.
 - The system generates interpretable, quantifiable predictions by automatically constructing and iteratively refining feature sets using public biomedical data sources while rigorously excluding label leakage through temporal data filtering, thus ensuring reproducibility and transparency required for high-stakes clinical domains.
 - AUTOCT demonstrated robust generalizability, delivering competitive performance in related clinical outcomes such as patient dropout, mortality, and adverse event prediction tasks, with ROC-AUC scores up to 0.852, underscoring the method‚Äôs versatility and its suitability for diverse biomedical decision-making challenges.

<br>

üí∏ **Control Tax: The Price of Keeping AI in Check** [source](http://arxiv.org/pdf/2506.05296v1.pdf) security 

 Quantifying 'control tax' reveals how AI safety and oversight costs tightly intertwine, enabling precise trade-offs between security and auditing expenditure. 
 - Deploying advanced monitors, such as OpenAI‚Äôs o3-mini, increased code backdoor detection rates (AUC 0.92‚Äì0.96), with little variation across attacker strength once public tests are passed, indicating robust monitoring is highly effective against diverse adversarial attacks.
 - Mapping the cost‚Äìsafety trade-off, achieving a 95% chance of catching the first backdoor often requires audit budgets of 5%‚Äì10%, directly linking operational oversight expenses (the 'control tax') to attainable safety assurances in long-term AI deployments.
 - Optimal adversarial behavior stabilizes at inserting a constant average number of backdoors as deployment scales‚Äîimplying that, for a fixed auditing budget, the marginal benefit (to an attacker) of increased task volume diminishes, and predictable attack rates are critical to robust monitoring strategy design.

<br>

üßë‚Äçüíª **MLA-Trust: Benchmarking Trustworthiness of Multimodal LLM Agents in GUI
  Environments** [source](http://arxiv.org/pdf/2506.01616v1.pdf) security 

 Interactive multimodal agents in GUI settings face amplified and novel trustworthiness risks‚Äîespecially during multi-step tasks‚Äîrevealing safety gaps not present in static LLMs and underscoring the need for ongoing, dynamic trust evaluation as these systems are deployed in the real world.
 - Multimodal large language model agents (MLAs) interacting with GUI environments showed more severe trustworthiness risks‚Äîsuch as privacy leakage, uncontrollable actions, and harmful output‚Äîcompared to traditional non-interactive language models, with proprietary and open-source GUI agents struggling particularly in high-stakes environments like finance and healthcare.
 - Transitioning large multimodal models (MLLMs) from static inference to multi-step, interactive MLAs leads to substantial drops in safety: top-tier models exhibited refusal rates that fell from over 90% as static MLLMs to as low as 47% as interactive MLAs, indicating greater vulnerability to adversarial and implicit risks in practice.
 - Structured training paradigms (like supervised fine-tuning and RLHF) and increased model scale consistently correlated with improved controllability, safety, and privacy in MLAs, but even the best proprietary models demonstrated accuracy below 60% on real-world truthfulness tasks, highlighting an urgent need for dynamic, multidimensional trust monitoring before reliable deployment.

<br>

üõ°Ô∏è **SRD: Reinforcement-Learned Semantic Perturbation for Backdoor Defense in
  VLMs** [source](http://arxiv.org/pdf/2506.04743v1.pdf) security 

 A reinforcement learning-driven defense applying attention-guided perturbations drastically lowers backdoor attack success in vision-language models, without prior trigger knowledge or loss of output quality.
 - The proposed Semantic Reward Defense (SRD) framework reduces the attack success rate of backdoor attacks in vision-language models from over 96% to as low as 5.6%, while maintaining less than 10% degradation in text generation quality on clean samples.
 - SRD achieves trigger-agnostic defense by leveraging reinforcement learning (Deep Q-Network) to strategically apply red mask perturbations to sensitive image regions identified through abnormal attention coupling, effectively preventing the activation of backdoor triggers.
 - Compared to state-of-the-art defenses, SRD consistently outperforms existing methods by lowering attack success rates and preserving semantic fidelity and fluency in generated captions, demonstrating robust protection across various attack types and poisoning intensities.

<br>

üõ°Ô∏è **Computational adversarial risk analysis for general security games** [source](http://arxiv.org/pdf/2506.02603v1.pdf) security 

 A general and scalable adversarial risk analysis scheme now enables defenders to anticipate and deter strategic attacks in complex security games without relying on unrealistic common knowledge assumptions.
 - The proposed computational framework enables defenders in security games to generate optimal defense strategies against strategic attackers without requiring common knowledge or common priors, effectively managing incomplete information.
 - Application of the augmented probability simulation (APS) method, supported by neural-network-based metamodels, allows efficient resolution of multi-stage, continuous, and high-dimensional security games that were previously infeasible to analyze using existing adversarial risk analysis techniques.
 - In the disinformation war case study, the defender‚Äôs optimal strategy involves moderately proactive investment, with results showing that attackers often choose either not to attack or to commit all resources to a high-intensity attack, underlining the deterrence potential of visible defensive commitments.

<br>

